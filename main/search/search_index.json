{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sveltos - Kubernetes Add-on Controller | Manage and Deploy Add-ons","text":"<p>Star</p> <p></p> Sveltos Kubernetes Add-on Controller - Simplify Add-on Management in Kubernetes","tags":["Kubernetes","add-ons","helm","kustomize","carvel ytt","jsonnet","clusterapi","multi-tenancy"]},{"location":"#what-is-sveltos","title":"What is Sveltos?","text":"<p>Sveltos is a Kubernetes add-on controller that simplifies the deployment and management of Kubernetes add-ons and applications across multiple clusters whether on-prem, in the cloud or a multitenant environment.</p> <p>Note</p> <p>Sveltos is not a replacement for GitOps. It's designed to be an extension for GitOps workflows when managing multiple clusters.</p> <p>Here's how it works:</p> <ul> <li>Sveltos runs in a management cluster. It assists users in programmatically deploying and managing Kubernetes add-ons and applications to any cluster in the fleet, including the management cluster itself.</li> <li>GitOps principles. You can still use GitOps tools to push your configuration to the management cluster. Then, Sveltos takes over, distributing the desired state to your managed clusters.</li> <li>Sveltos supports a variety of add-on formats. This includes Helm charts, raw YAML/JSON, Kustomize, Carvel ytt, and Jsonnet. This flexibility allows you to manage your add-ons using the format that best suits your needs.</li> <li>Sveltos provides a powerful template and event framework. This framework empowers you to better manage your fleet of clusters by customizing deployments and reacting to cluster events.</li> </ul> <p></p>","tags":["Kubernetes","add-ons","helm","kustomize","carvel ytt","jsonnet","clusterapi","multi-tenancy"]},{"location":"#features","title":"Features","text":"<ul> <li>Observability: Sveltos offers different endpoints for notifications. The notifications can be used by other tools to perform additional actions or trigger workflows. The supported types are Slack, Teams, Discord, WebEx, Telegram, SMTP and Kubernetes events.</li> <li>Templating: Patching the rendered resources made easy! Sveltos allows Kubernetes add-ons and applications to be represented as templates. Before deploying to the managed clusters, Sveltos instantiates the templates with information gathered from either the management or the managed clusters. This allows consistent definition across multiple clusters with minimal adjustments and administration overhead.</li> <li>Orchestrated Deployment Order: The Sveltos CDRs (Custom Resource Definition) are deployed in the exact order they appear in the definition file. That ensures a predictable and controlled deployment order.</li> <li>Multitenancy: Sveltos was created with the multitenancy concept in mind. Sveltos <code>ClusterProfile</code> and <code>Profile</code> resources allow platform administrators to facilitate full isolation or tenants sharing a cluster.</li> <li>Events: <code>Sveltos Event Framework</code> allows the deployment of add-ons and applications in response to specific events with the use of the Lua language. That allows dynamic and adaptable deployments based on different needs and use cases.</li> </ul>","tags":["Kubernetes","add-ons","helm","kustomize","carvel ytt","jsonnet","clusterapi","multi-tenancy"]},{"location":"#why-sveltos","title":"Why Sveltos?","text":"<p>Sveltos was built to address the challenges posed by various CI/CD tools. Sveltos was designed to complement or even replace existing GitOps tools, and its integration with Flux CD significantly enhances the GitOps approach at scale.</p> <p>Key features of Sveltos include multitenancy, agent-based drift notification and synchronisation, and resource optimisation. These features ensure secure, reliable, and stable deployments of Kubernetes add-ons and applications, while reducing operational costs in both on-prem and cloud environments.</p>","tags":["Kubernetes","add-ons","helm","kustomize","carvel ytt","jsonnet","clusterapi","multi-tenancy"]},{"location":"#enterprise-offering","title":"Enterprise Offering","text":"<p>Interested in our enterprise offering? Enterprise offering</p>","tags":["Kubernetes","add-ons","helm","kustomize","carvel ytt","jsonnet","clusterapi","multi-tenancy"]},{"location":"#contributing-to-projectsveltos","title":"\ud83d\ude3b Contributing to projectsveltos","text":"<p>We love to hear from you! We believe in the power of community and collaboration!</p> <p>Your ideas and feedback are important to us. Whether you want to report a bug, suggest a new feature, or stay updated with our latest news, we are here for you.</p> <ol> <li>Open a bug/feature enhancement on GitHub </li> <li>Chat with us on the Slack in the #projectsveltos channel </li> <li>If you prefer to reach out directly, just shoot us an email</li> </ol> <p>We are always thrilled to welcome new members to our community, and your contributions are always appreciated. Do not be shy - join us today and let's make Sveltos the best it can be! \u2764\ufe0f</p>","tags":["Kubernetes","add-ons","helm","kustomize","carvel ytt","jsonnet","clusterapi","multi-tenancy"]},{"location":"#support-us","title":"Support us","text":"<p>If you like the project, please give us a  if you haven't done so yet. Your support means a lot to us. Thank you .</p> <p> projectsveltos</p>","tags":["Kubernetes","add-ons","helm","kustomize","carvel ytt","jsonnet","clusterapi","multi-tenancy"]},{"location":"archive/","title":"Recources by Sveltos","text":"","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","multi-tenancy","versions"]},{"location":"archive/#previous-versions","title":"Previous Versions","text":"<ul> <li>Version v1.1.1</li> <li>Version v1.0.1</li> <li>Version v1.0.0</li> <li>Version v0.57.3</li> </ul>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","multi-tenancy","versions"]},{"location":"blogs/","title":"Recources by Sveltos","text":"<p>Star</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"blogs/#sveltos-available-articles","title":"Sveltos Available Articles","text":"<ol> <li>Deploy Kubernetes add-ons</li> <li>Managing Kubernetes Beyond The Firewall - Introducing Pull Mode</li> <li>Centralized Resource Display for Multiple Kubernetes Clusters</li> <li>Sveltos and Kustomize</li> <li>Gateway API: Exposing managed services with Kong and Sveltos</li> <li>Sveltos, Carvel ytt and Flux</li> <li>Kubernetes Cluster Management and Cloud Automation with ClusterAPI, Crossplane and Projectsveltos</li> <li>Event driven add-on deployment</li> <li>Sveltos Health Validation and Dependencies</li> <li>Scaling Event-Driven Applications Made Easy with Sveltos Kubernetes Cross-Cluster Configuration</li> <li>Sveltos Event Framework and Cilium Gateway API</li> <li>Sveltos DryRun Capability</li> <li>Sveltos: Simplifying Kubernetes Add-on Deployment and Constraints</li> <li>Simplify Sveltos and Flux integration</li> <li>Handle long running tasks</li> <li>L4-L7 with Sveltos</li> <li>Using Projectsveltos to Manage Kubernetes Add-ons on Civo Cloud Clusters</li> <li>Flux and Sveltos</li> <li>Multi-tenancy with Sveltos</li> <li>CIS Benchmark Compliance Across Multiple Kubernetes Clusters</li> <li>Sveltos Templating: Cilium Cluster Mesh in One Run</li> <li>Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates</li> <li>Better Together: CAPI, Sveltos, and Cyclops for Automated Development Environments</li> </ol>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"blogs/#sveltos-video-list","title":"Sveltos Video List","text":"<p>More videos can be found here:</p> <ol> <li>Sveltos introduction</li> <li>Sveltos features presentation</li> <li>Sveltos multi-tenancy</li> <li>Sveltos overview</li> <li>Sveltos DryRun mode</li> <li>Sveltos Rollback</li> </ol>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"blogs/#contributing-to-projectsveltos","title":"\ud83d\ude3b Contributing to projectsveltos","text":"<p>\u2764\ufe0f Your contributions are always welcome! If you want to contribute, have questions, noticed any bug or want to get the latest project news, you can connect with us in the following ways:</p> <ol> <li>Open a bug/feature enhancement on github </li> <li>Chat with us on the Slack in the #projectsveltos channel </li> <li>Contact Us</li> </ol>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"blogs/#support-us","title":"Support us","text":"<p>If you like the project, please give us a  if you haven't done so yet. Your support means a lot to us. Thank you .</p> <p> projectsveltos</p> <p></p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"addons/addons/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/addons/#what-is-sveltos","title":"What is Sveltos?","text":"<p>Sveltos is a set of Kubernetes controllers that run in the management cluster. From the management cluster, Sveltos can manage add-ons and applications on a fleet of managed Kubernetes clusters.</p> <p>Sveltos comes with support to automatically discover ClusterAPI powered clusters, but it doesn't stop there. You can easily register any other cluster (on-prem, Cloud) and manage Kubernetes add-ons seamlessly.</p> <p></p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/addons/#how-it-works","title":"How it works?","text":"<p>ClusterProfile and Profile are the CustomerResourceDefinitions used to instruct Sveltos which add-ons to deploy on a set of clusters.</p> <ul> <li> <p>ClusterProfile: It is a cluster-wide resource. It can match any cluster and reference any resource regardless of their namespace.</p> </li> <li> <p>Profile: It is a namespace-scoped resource that is specific to a single namespace. It can only match clusters and reference resources within its own namespace.</p> </li> </ul> <p>By creating a ClusterProfile, Profile instances, we can easily deploy the following points across a set of Kubernetes clusters.</p> <ul> <li>Helm charts</li> <li>Resources assembled with Kustomize</li> <li>Raw Kubernetes YAML/JSON manifests</li> </ul> <p>Define which Kubernetes add-ons to deploy and where:</p> <ol> <li>Select one or more clusters using a Kubernetes label selector;</li> <li>List the Kubernetes add-ons that need to be deployed on the selected clusters</li> </ol> <p>Simple as that!</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/addons/#example-deploy-kyverno-using-a-clusterprofile","title":"Example: Deploy Kyverno using a ClusterProfile","text":"<p>The below example deploys a Kyverno Helm chart in every cluster with the label selector set to <code>env=prod</code>.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/addons/#step-1-register-clusters-with-sveltos","title":"Step 1: Register Clusters with Sveltos","text":"<p>The first step is to register clusters with Sveltos to receive the required addons and deployments. If the clusters are not registered yet, follow the instructions outlined here. This applies to non-CAPI clusters.</p> <pre><code>$ kubectl get sveltosclusters -n projectsveltos --show-labels\n\nNAME        READY   VERSION          LABELS\ncluster12   true    v1.26.9+rke2r1   sveltos-agent=present\ncluster13   true    v1.26.9+rke2r1   sveltos-agent=present\n</code></pre> <p>Note</p> <p>The CAPI clusters are registered in the projectsveltos namespace. If you register the clusters in a different namespace, update the command above.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/addons/#step-2-create-the-clusterprofile","title":"Step 2: Create the ClusterProfile","text":"<p>The second step is to create a <code>ClusterProfile</code> and apply it to the management cluster.</p> <p>Sveltos ClusterProfile Kyverno</p> <pre><code>cat &gt; clusterprofile_kyverno.yaml &lt;&lt;EOF\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\nname: kyverno\nspec:\nclusterSelector:\n    matchLabels:\n    env: prod\nsyncMode: Continuous\nhelmCharts:\n- repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\nEOF\n</code></pre> <pre><code>$ kubectl apply -f \"clusterprofile_kyverno.yaml\"\n\n$ sveltosctl show addons\n\n+--------------------------+---------------+-----------+----------------+---------+-------------------------------+------------------+\n|         CLUSTER          | RESOURCE TYPE | NAMESPACE |      NAME      | VERSION |             TIME              | CLUSTER PROFILES |\n+--------------------------+---------------+-----------+----------------+---------+-------------------------------+------------------+\n| projectsveltos/cluster12 | helm chart    | kyverno   | kyverno-latest | 3.2.5   | 2023-12-16 00:14:17 -0800 PST | kyverno          |\n| projectsveltos/cluster13 | helm chart    | kyverno   | kyverno-latest | 3.2.5   | 2023-12-16 00:14:17 -0800 PST | kyverno          |\n+--------------------------+---------------+-----------+----------------+---------+-------------------------------+------------------+\n</code></pre> <p></p> <p></p> <p>Note</p> <p>If you are not aware of the <code>sveltosctl</code> utility, have a look at the installation documentation found here.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/addons/#more-resources","title":"More Resources","text":"<p>The easiest way to try out Sveltos is by following the Quick Start guide. If you prefer a video demonstration, watch the Sveltos introduction video on YouTube.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#clusterprofiles","title":"ClusterProfiles","text":"<p>ClusterProfile is the CustomerResourceDefinition used to instruct Sveltos which add-ons to deploy on a set of clusters.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#pause-annotation","title":"Pause Annotation","text":"<p>Pausing a ClusterProfile with the <code>profile.projectsveltos.io/paused</code> annotation prevents Sveltos from performing any reconciliation. This effectively freezes the ClusterProfile in its current state, ensuring that no changes are applied to the clusters it manages.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#specclusterselector","title":"Spec.ClusterSelector","text":"<p>clusterSelector field is used to specify which managed clusters should receive the add-ons and applications defined in the configuration.</p> <p>This field employs a Kubernetes label selector, allowing you to target clusters based on specific labels.</p> <pre><code>clusterSelector:\n    matchLabels:\n      env: prod\n</code></pre> <p>By leveraging matchExpressions, you can create more complex and flexible cluster selection criteria.</p> <pre><code>clusterSelector:\n  matchExpressions:\n  - {key: env, operator: In, values: [staging, production]}\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#spechelmcharts","title":"Spec.HelmCharts","text":"<p>helmCharts field consists of a list of helm charts to be deployed to the clusters matching clusterSelector;</p> <pre><code>  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>Helm chart values can be dynamically retrieved from ConfigMaps or Secrets for flexible configuration. Customize Helm behavior with various options, and deploy charts from private container registries. For a complete list of features, refer to the Helm chart section.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#specpolicyrefs","title":"Spec.PolicyRefs","text":"<p>policyRefs field references a list of ConfigMaps/Secrets, each containing Kubernetes resources to be deployed in the clusters matching clusterSelector.</p> <p>This field is a slice of PolicyRef structs. Each PolictRef has the following fields:</p> <ul> <li>Kind: The kind of the referenced resource. The supported kinds are Secret and ConfigMap.</li> <li>Namespace: The namespace of the referenced resource. This field is optional, and if empty, the namespace will be set to the matching cluster's namespace.</li> <li>Name: The name of the referenced resource. This field must be at least one character long.</li> <li>DeploymentType: The deployment type of the referenced resource. This field indicates whether the resource should be deployed to the management cluster (local) or the managed cluster (remote). The default value is Remote.</li> </ul> <pre><code>policyRefs:\n- kind: Secret\n  name: my-secret-1\n  namespace: my-namespace-1\n  deploymentType: Local\n- kind: Remote\n  name: my-configmap-1\n  namespace: my-namespace-1\n  deploymentType: Remote\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#speckustomizationrefs","title":"Spec.KustomizationRefs","text":"<p>kustomizationRefs field is a list of sources containing kustomization files. Resources will be deployed in the clusters matching the clusterSelector specified.</p> <p>This field is a slice of KustomizationRef structs. Each KustomizationRef has the following fields:</p> <ul> <li> <p>Kind: The kind of the referenced resource. The supported kinds are:</p> <ul> <li>flux GitRepository, OCIRepository, Bucket: These kinds represent resources that store Kustomization manifests.</li> <li>ConfigMap, Secret: These kinds represent resources that contain Kustomization manifests or overlays.</li> </ul> </li> <li> <p>Namespace: The namespace of the referenced resource. This field is optional and can be left empty. If it is empty, the namespace will be set to the cluster's namespace.</p> </li> <li>Name: The name of the referenced resource. This field must be at least one character long.</li> <li>Path: The path to the directory containing the kustomization.yaml file, or the set of plain YAMLs for which a kustomization.yaml should be generated. This field is optional and defaults to None, which means the root path of the SourceRef.</li> <li>TargetNamespace: The target namespace for the Kustomization deployment. This field is optional and can be used to override the namespace specified in the kustomization.yaml file.</li> <li>DeploymentType: The deployment type of the referenced resource. This field indicates whether the Kustomization deployment should be deployed to the management cluster (local) or the managed cluster (remote). The default value is Remote.</li> </ul> <p>For a complete list of features, refer to the Kustomize section.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#specsyncmode","title":"Spec.SyncMode","text":"<p>This field can be set to:</p> <ul> <li>OneTime</li> <li>Continuous</li> <li>ContinuousWithDriftDetection</li> <li>DryRun</li> </ul> <p>Let's take a closer look at the OneTime syncMode option. Once you deploy a ClusterProfile with a OneTime configuration, Sveltos will check all of your clusters for a match with the clusterSelector. Any matching clusters will have the resources specified in the ClusterProfile deployed. However, if you make changes to the ClusterProfile later on, those changes will not be automatically deployed to already-matching clusters.</p> <p>Now, if you're looking for real-time deployment and updates, the Continuous syncMode is the way to go. With Continuous, any changes made to the ClusterProfile will be immediately reconciled into matching clusters. This means that you can add new features, update existing ones, and remove them as necessary, all without lifting a finger. Sveltos will deploy, update, or remove resources in matching clusters as needed, making your life as a Kubernetes admin a breeze.</p> <p>ContinuousWithDriftDetection instructs Sveltos to monitor the state of managed clusters and detect a configuration drift for any of the resources deployed because of that ClusterProfile. When Sveltos detects a configuration drift, it automatically re-syncs the cluster state back to the state described in the management cluster. To know more about configuration drift detection, refer to this section.</p> <p>Imagine you're about to make some important changes to your ClusterProfile, but you're not entirely sure what the results will be. You don't want to risk causing any unwanted side effects, right? Well, that's where the DryRun syncMode configuration comes in. By deploying your ClusterProfile with this configuration, you can launch a simulation of all the operations that would normally be executed in a live run. The best part? No actual changes will be made to the matching clusters during this dry run workflow, so you can rest easy knowing that there won't be any surprises. To know more about dry run, refer to this section.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#specstopmatchingbehavior","title":"Spec.StopMatchingBehavior","text":"<p>The stopMatchingBehavior field specifies the behavior when a cluster no longer matches a ClusterProfile. By default, all Kubernetes resources and Helm charts deployed to the cluster will be removed. However, if StopMatchingBehavior is set to LeavePolicies, any policies deployed by the ClusterProfile will remain in the cluster.</p> <p>For instance</p> <p>Example - ClusterProfile Kyverno Deployment</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  stopMatchingBehavior: WithdrawPolicies\n  clusterSelector:\n    matchLabels:\n      env: prod\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>When a cluster matches the ClusterProfile, Kyverno Helm chart will be deployed in such a cluster. If the cluster's labels are subsequently modified and cluster no longer matches the ClusterProfile, the Kyverno Helm chart will be uninstalled. However, if the stopMatchingBehavior property is set to LeavePolicies, Sveltos will retain the Kyverno Helm chart in the cluster.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#specreloader","title":"Spec.Reloader","text":"<p>The reloader property determines whether rolling upgrades should be triggered for Deployment, StatefulSet, or DaemonSet instances managed by Sveltos and associated with this ClusterProfile when changes are made to mounted ConfigMaps or Secrets. When set to true, Sveltos automatically initiates rolling upgrades for affected Deployment, StatefulSet, or DaemonSet instances whenever any mounted ConfigMap or Secret is modified. This ensures that the latest configuration updates are applied to the respective workloads.</p> <p>Please refer to this section for more information.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#specmaxupdate","title":"Spec.MaxUpdate","text":"<p>A ClusterProfile might match more than one cluster. When a change is maded to a ClusterProfile, by default all matching clusters are update concurrently. The maxUpdate field specifies the maximum number of Clusters that can be updated concurrently during an update operation triggered by changes to the ClusterProfile's add-ons or applications. The specified value can be an absolute number (e.g., 5) or a percentage of the desired cluster count (e.g., 10%). The default value is 100%, allowing all matching Clusters to be updated simultaneously. For instance, if set to 30%, when modifications are made to the ClusterProfile's add-ons or applications, only 30% of matching Clusters will be updated concurrently. Updates to the remaining matching Clusters will only commence upon successful completion of updates in the initially targeted Clusters. This approach ensures a controlled and manageable update process, minimizing potential disruptions to the overall cluster environment. Please refer to this section for more information.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#specvalidatehealths","title":"Spec.ValidateHealths","text":"<p>The validateHealths property defines a set of Lua functions that Sveltos executes against the managed cluster to assess the health and status of the add-ons and applications specified in the ClusterProfile. These Lua functions act as validation checks, ensuring that the deployed add-ons and applications are functioning properly and aligned with the desired state. By executing these functions, Sveltos proactively identifies any potential issues or misconfigurations that could arise, maintaining the overall health and stability of the managed cluster.</p> <p>The ValidateHealths property accepts a slice of Lua functions, where each function encapsulates a specific validation check. These functions can access the managed cluster's state to perform comprehensive checks on the add-ons and applications. The results of the validation checks are aggregated and reported back to Sveltos, providing valuable insights into the health and status of the managed cluster's components.</p> <p>Lua's scripting capabilities offer flexibility in defining complex validation logic tailored to specific add-ons or applications.</p> <p>Please refer to this section for more information.</p> <p>Consider a scenario where a new cluster with the label env:prod is created. The following instructions guide Sveltos to:</p> <ul> <li>Deploy Kyverno Helm chart;</li> <li>Validate Deployment Health: Perform health checks on each deployment within the kyverno namespace. Verify that the number of active replicas matches the requested replicas;</li> <li>Successful Deployment: Once the health checks are successfully completed, Sveltos considers the ClusterProfile as successfully deployed.</li> </ul> <p>Example - ClusterProfile Kyverno and Lua</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    validateHealths:\n    - name: deployment-health\n      featureID: Helm\n      group: \"apps\"\n      version: \"v1\"\n      kind: \"Deployment\"\n      namespace: kyverno\n      script: |\n        function evaluate()\n          hs = {}\n          hs.healthy = false\n          hs.message = \"available replicas not matching requested replicas\"\n          if obj.status ~= nil then\n            if obj.status.availableReplicas ~= nil then\n              if obj.status.availableReplicas == obj.spec.replicas then\n                hs.healthy = true\n              end\n            end\n          end\n          return hs\n        end\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#spectemplateresourcerefs","title":"Spec.TemplateResourceRefs","text":"<p>The templateResourceRefs property specifies a collection of resources to be gathered from the management cluster. The values extracted from these resources will be utilized to instantiate templates embedded within referenced PolicyRefs and Helm charts. Refer to template section for more info and examples.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#specdependson","title":"Spec.DependsOn","text":"<p>The dependsOn property specifies a list of other ClusterProfiles that this instance relies on. In any managed cluster that matches to this ClusterProfile, the add-ons and applications defined in this instance will only be deployed after all add-ons and applications in the designated dependency ClusterProfiles have been successfully deployed.</p> <p>For example, clusterprofile-a can depend on another clusterprofile-b. This implies that any Helm charts or raw YAML files associated with ClusterProfile A will not be deployed until all add-ons and applications specified in ClusterProfile B have been successfully provisioned.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: clusterprofile-a\nspec:\n  dependsOn:\n  - clusterprofile_b\n</code></pre> <p>Sveltos automatically resolves and deploys the prerequisite profiles specified in the DependsOn field. Sveltos will analyze the dependency graph, identify the required prerequisite profiles, and ensure they are deployed to the same clusters.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#speccontinueonerror","title":"Spec.ContinueOnError","text":"<p>ContinueOnError configures Sveltos' error handling. When true, errors are logged, but deployment continues. When false (default), Sveltos stops at the first error and retries the failing resource. For instance, if deploying three Helm charts, a failure during the second chart's deployment will halt the process, and Sveltos will retry the second chart. Only if ContinueOnError is true will Sveltos proceed to deploy the third chart before retrying the second chart.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/clusterprofile/#speccontinueonconflict","title":"Spec.ContinueOnConflict","text":"<p>ContinueOnConflict configures Sveltos' conflict resolution behavior. When true, Sveltos logs the conflicts but continues deploying the remaining resources. When false (default), Sveltos halts deployment at the first detected conflict. This can happen when another profile has already deployed the same resource.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/combining_all/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/combining_all/#scenario","title":"Scenario","text":"<p>A ClusterProfile can have a combination of Helm charts, raw YAML/JSON, and Kustomize configurations.</p> <p>Consider a scenario where you want to utilize Kyverno to prevent the deployment of images with the 'latest' tag<sup>1</sup>. To achieve this, you can create a ClusterProfile that:</p> <ul> <li>Deploys the Kyverno Helm chart</li> <li>Deploys a Kyverno policy that enforces image validation, ensuring the image specifies a tag other than 'latest'</li> </ul> <p>Download the Kyverno policy and create a ConfigMap containing the policy within the management cluster.</p> <pre><code>$ wget https://raw.githubusercontent.com/kyverno/policies/main/best-practices/disallow-latest-tag/disallow-latest-tag.yaml\n\n$ kubectl create configmap disallow-latest-tag --from-file disallow-latest-tag.yaml\n</code></pre> <p>To deploy Kyverno and a ClusterPolicy across all managed clusters matching the Sveltos label selector env=fv, utilize the below ClusterProfile.\"</p> <p>Example - ClusterProfile Kyverno Deployment</p> <pre><code>  ---\n  apiVersion: config.projectsveltos.io/v1beta1\n  kind: ClusterProfile\n  metadata:\n    name: kyverno\n  spec:\n    clusterSelector:\n      matchLabels:\n        env: fv\n    helmCharts:\n    - chartName: kyverno/kyverno\n      chartVersion: v3.3.3\n      helmChartAction: Install\n      releaseName: kyverno-latest\n      releaseNamespace: kyverno\n      repositoryName: kyverno\n      repositoryURL: https://kyverno.github.io/kyverno/\n    policyRefs:\n    - kind: ConfigMap\n      name: disallow-latest-tag\n      namespace: default\n</code></pre> <ol> <li> <p>The ':latest' tag is mutable and can lead to unexpected errors if the image changes. A best practice is to use an immutable tag that maps to a specific version of an application Pod.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#flux-sources","title":"Flux Sources","text":"<p>Sveltos can seamlessly integrate with Flux<sup>1</sup> to automatically deploy YAML manifests or Helm charts stored in a Git repository or a Bucket. This powerful combination allows you to manage Kubernetes configurations in a central location and leverage Sveltos to target deployments across clusters.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#example-deploy-nginx-ingress-with-flux-and-sveltos","title":"Example: Deploy Nginx Ingress with Flux and Sveltos","text":"<p>Imagine a repository like this containing an nginx-ingress directory with all the required YAML resources for deployment<sup>2</sup>. In the steps below we demonstrate how to leverage Flux and Sveltos to automatically perform the deployment.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#step-1-configure-flux-in-the-management-cluster","title":"Step 1: Configure Flux in the Management Cluster","text":"<p>Install Flux in the management cluster and configure it to synchronise the Git repository containing the Nginx manifests. More information about the Flux installation can be found here.</p> <p>Deploy a GitRepository resource similar to the below.</p> <p>GitRepository Resource</p> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: flux-system\n  timeout: 60s\n  url: https://github.com/gianlucam76/yaml_flux.git\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#step-2-create-a-sveltos-clusterprofile","title":"Step 2: Create a Sveltos ClusterProfile","text":"<p>Define a Sveltos ClusterProfile referencing the flux-system GitRepository and specify the nginx-ingress directory as the source of the deployment.</p> <p>Sveltos ClusterProfile Nginx Ingress</p> <pre><code>cat &gt; clusterprofile_nginx_ingress.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-nginx-ingress\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  policyRefs:\n  - kind: GitRepository\n    name: flux-system\n    namespace: flux-system\n    path: nginx-ingress\nEOF\n</code></pre> <p>The <code>ClusterProfile</code> targets clusters with the env=fv label and fetches relevant deployment information from the nginx-ingress directory within the flux-system Git repository managed by Flux.</p> <pre><code>$ sveltosctl show addons\n+-----------------------------+----------------------------------------------+-----------+---------------------------------------+---------+-------------------------------+-------------------------------------+\n|           CLUSTER           |                RESOURCE TYPE                 | NAMESPACE |                 NAME                  | VERSION |             TIME              |              PROFILES               |\n+-----------------------------+----------------------------------------------+-----------+---------------------------------------+---------+-------------------------------+-------------------------------------+\n| default/clusterapi-workload | :ConfigMap                                   | default   | nginx-ingress-leader                  | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | rbac.authorization.k8s.io:ClusterRole        |           | nginx-stable-nginx-ingress            | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | rbac.authorization.k8s.io:RoleBinding        | default   | nginx-stable-nginx-ingress            | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | apps:Deployment                              | default   | nginx-stable-nginx-ingress-controller | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | :ServiceAccount                              | default   | nginx-stable-nginx-ingress            | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | :ConfigMap                                   | default   | nginx-stable-nginx-ingress            | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | rbac.authorization.k8s.io:ClusterRoleBinding |           | nginx-stable-nginx-ingress            | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | rbac.authorization.k8s.io:Role               | default   | nginx-stable-nginx-ingress            | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | :Service                                     | default   | nginx-stable-nginx-ingress-controller | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n| default/clusterapi-workload | networking.k8s.io:IngressClass               |           | nginx                                 | N/A     | 2024-03-23 11:43:10 +0100 CET | ClusterProfile/deploy-nginx-ingress |\n+-----------------------------+----------------------------------------------+-----------+---------------------------------------+---------+-------------------------------+-------------------------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#example-deploy-kyverno-policies-with-flux-and-sveltos","title":"Example: Deploy Kyverno policies with Flux and Sveltos","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#step-1-configure-flux-in-the-management-cluster_1","title":"Step 1: Configure Flux in the Management Cluster","text":"<p>Install Flux in the management cluster and configure it to synchronise the Git repository containing the Kyverno manifests.</p> <p>Deploy a GitRepository resource similar to the below.</p> <p>GitRepository Resource</p> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: flux-system\n  timeout: 60s\n  url: https://github.com/gianlucam76/yaml_flux.git\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#step-2-create-a-sveltos-clusterprofile_1","title":"Step 2: Create a Sveltos ClusterProfile","text":"<p>Define a ClusterProfile to deploy the Kyverno helm chart.</p> <p>Sveltos ClusterProfile Kyverno</p> <pre><code>cat &gt; clusterprofile_kyverno.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\nEOF\n</code></pre> <p>Define a Sveltos <code>ClusterProfile</code> referencing the flux-system GitRepository and defining the kyverno directory as the source of the deployment.</p> <p>The mentioned directory contains a list of Kyverno ClusterPolicies.</p> <p>Sveltos ClusterProfile Kyverno Policies</p> <pre><code>cat &gt; clusterprofile_kyverno_policies.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno-policies\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  policyRefs:\n  - kind: GitRepository\n    name: flux-system\n    namespace: flux-system\n    path: kyverno\n  dependsOn:\n  - deploy-kyverno\nEOF\n</code></pre> <p>The above <code>ClusterProfile</code> targets clusters with the env=fv label and fetches relevant deployment information from the kyverno directory within the flux-system Git repository managed by Flux.</p> <p>The Kyverno Helm chart and all the Kyverno policies contained in the Git repository under the kyverno directory are deployed:</p> <pre><code>$ sveltosctl show addons\n+-----------------------------+--------------------------+-----------+---------------------------+---------+-------------------------------+----------------------------------------+\n|           CLUSTER           |      RESOURCE TYPE       | NAMESPACE |           NAME            | VERSION |             TIME              |                PROFILES                |\n+-----------------------------+--------------------------+-----------+---------------------------+---------+-------------------------------+----------------------------------------+\n| default/clusterapi-workload | helm chart               | kyverno   | kyverno-latest            | 3.2.5   | 2024-03-23 11:39:30 +0100 CET | ClusterProfile/deploy-kyverno          |\n| default/clusterapi-workload | kyverno.io:ClusterPolicy |           | restrict-image-registries | N/A     | 2024-03-23 11:40:11 +0100 CET | ClusterProfile/deploy-kyverno-policies |\n| default/clusterapi-workload | kyverno.io:ClusterPolicy |           | disallow-latest-tag       | N/A     | 2024-03-23 11:40:11 +0100 CET | ClusterProfile/deploy-kyverno-policies |\n| default/clusterapi-workload | kyverno.io:ClusterPolicy |           | require-ro-rootfs         | N/A     | 2024-03-23 11:40:11 +0100 CET | ClusterProfile/deploy-kyverno-policies |\n+-----------------------------+--------------------------+-----------+---------------------------+---------+-------------------------------+----------------------------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#example-reference-helm-charts-from-flux-sources","title":"Example: Reference Helm Charts from Flux Sources","text":"<p>Sveltos allows us to deploy Helm charts from various sources, including traditional HTTP repositories, OCI registries, and Flux sources. This section focuses on using Flux sources as a Helm chart repository.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#utilise-flux-gitrepository-as-a-helm-chart-source","title":"Utilise Flux GitRepository as a Helm Chart Source","text":"<p>To deploy Helm charts from a Flux GitRepository, specify the <code>repositoryURL</code> in the <code>ClusterProfile</code> using the following format:</p> <p><pre><code>&lt;flux source kind&gt;://&lt;flux source namespace&gt;/&lt;flux source name&gt;/&lt;path&gt;\n</code></pre> More information about the command arguments defined above.</p> <ul> <li><code>&lt;flux source kind&gt;</code>: The type of Flux source. For Git repositories, this is <code>gitrepository</code> (available options: <code>ocirepository</code> and <code>bucket</code>)</li> <li><code>&lt;flux source namespace&gt;</code>: The Kubernetes namespace where the Flux GitRepository is located</li> <li><code>&lt;flux source name&gt;</code>: The name of the Flux GitRepository</li> <li><code>&lt;path&gt;</code>: The relative path within the Git repository to the directory containing the Helm charts</li> </ul> <p>Note</p> <p>Use the <code>kubectl get gitrepository.source.toolkit.fluxcd.io -A</code> command to get a view of the existing syncronised repositories.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#scenario","title":"Scenario","text":"<p>Let's assume we have a Flux GitRepository named <code>flux-system</code> in the <code>flux-system</code> namespace.</p> <p>The GitRepository resource is configured to synchronise the <code>https://github.com/projectsveltos/helm-charts.git/</code> repository. The Helm charts are located in the <code>charts/projectsveltos</code> directory within the mentioned repository.</p> <p>To deploy the <code>projectsveltos</code> chart using Sveltos, we can create a <code>ClusterProfile</code> with the below <code>helmCharts</code> options.</p> <pre><code>helmCharts:\n  - repositoryURL:    gitrepository://flux-system/flux-system/charts/projectsveltos\n    releaseName:      projectsveltos\n    releaseNamespace: projectsveltos\n    helmChartAction:  Install\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#example-template-with-git-repositorybucket-content","title":"Example: Template with Git Repository/Bucket Content","text":"<p>The content within the Git repository or other sources referenced by a Sveltos <code>ClusterProfile</code> can be templates<sup>3</sup>.To enable templating, annotate the referenced <code>GitRepository</code> instance with \"projectsveltos.io/template: true\".</p> <p>When Sveltos processes the template, it will perform the below.</p> <ul> <li>Read the content of all files inside the specified path</li> <li>Instantiate the templates ultilising the data from resources in the management cluster, similar to how it currently works with referenced Secrets and ConfigMaps</li> </ul> <p>This allows dynamic deployment customisation based on the specific characteristics of the clusters, further enhancing flexibility and automation.</p> <p>Let's try it out! The content in the \"template\" directory of the repository serves as the perfect example.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#template-definition","title":"Template Definition","text":"<p>ConfigMap Definition</p> <pre><code>cat &gt; cm.yaml &lt;&lt;EOF\n# Sveltos will instantiate this template before deploying to matching managed cluster\n# Sveltos will get the ClusterAPI Cluster instance representing the cluster in the\n# managed cluster, and use that resource data to instintiate this ConfigMap before\n# deploying it\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ .Cluster.metadata.name }}\n  namespace: default\ndata:\n  controlPlaneEndpoint: \"{{ .Cluster.spec.controlPlaneEndpoint.host }}:{{ .Cluster.spec.controlPlaneEndpoint.port }}\"\nEOF\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#gitrepository-definition","title":"GitRepository Definition","text":"<p>Note</p> <p>Add the projectsveltos.io/template: ok annotation to the GitRepository resources created further above.</p> <p>GitRepository Resource</p> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: flux-system\n  namespace: flux-system\n  annotations:\n    projectsveltos.io/template: ok\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: flux-system\n  timeout: 60s\n  url: https://github.com/gianlucam76/yaml_flux.git\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#clusterprofile-definition","title":"ClusterProfile Definition","text":"<p>Sveltos ClusterProfile Flux Definition</p> <pre><code>cat &gt; clusterprofile_flux.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: flux-template-example\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  policyRefs:\n  - kind: GitRepository\n    name: flux-system\n    namespace: flux-system\n    path: template\nEOF\n</code></pre> <p>The <code>ClusterProfile</code> will use the information from the \"Cluster\" resource in the management cluster to populate the template and deploy it.</p> <p>An example of a deployed ConfigMap in the managed cluster can be found below.</p> <pre><code>---\napiVersion: v1\ndata:\n  controlPlaneEndpoint: 172.18.0.4:6443\nkind: ConfigMap\nmetadata:\n  ...\n  name: clusterapi-workload\n  namespace: default\n  ...\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/example_flux_sources/#express-path-as-template","title":"Express Path as Template","text":"<p>The path field within a policyRefs object in Sveltos can be defined using a template. This allows to dynamically set the path based on information from the cluster itself.</p> <p>Sveltos ClusterProfile Flux Region West</p> <pre><code>cat &gt; clusterprofile_flux_region_west.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: flux-system\nspec:\n  clusterSelector:\n    matchLabels:\n      region: west\n  syncMode: Continuous\n  policyRefs:\n  - kind: GitRepository\n    name: flux-system\n    namespace: flux-system\n    path: '{{ index .Cluster.metadata.annotations \"environment\" }}/helloWorld'\nEOF\n</code></pre> <p>Sveltos uses the cluster instance in the management cluster to populate the template in the path field. The template expression <code>{{ index .Cluster.metadata.annotations \"environment\" }}</code> retrieves the value of the annotation named environment from the cluster's metadata.</p> <p>For example:</p> <ol> <li>Cluster A: If cluster A has an annotation environment: production, the resulting path will be: production/helloWorld.</li> <li>Cluster B: If cluster B has an annotation environment: pre-production, the resulting path will be: pre-production/helloWorld.</li> </ol> <p>The mentioned approach allows for flexible configuration based on individual cluster environments. Remember to adapt the provided resources to your specific repository structure, cluster configuration, and desired templating logic.</p> <p>A more complex example can be when we want to express the path field as a template using if statements.</p> <p>Sveltos ClusterProfile Flux Template path</p> <pre><code>cat &gt; clusterprofile_flux_template_path.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: flux-system\nspec:\n  clusterSelector:\n    matchLabels:\n      region: west\n  templateResourceRefs:\n  - resource:\n      apiVersion: lib.projectsveltos.io/v1beta1\n      kind: SveltosCluster\n      name: \"{{ .Cluster.metadata.name }}\"\n    identifier: Cluster\n  syncMode: Continuous\n  policyRefs:\n  - kind: GitRepository\n    name: flux-system\n    namespace: flux-system\n    path: |-\n      {{$path := index .Cluster.metadata.labels \"projectsveltos.io/k8s-version\" }}{{- if eq $path \"v1.29.8\"}}system/prod\n      {{- else}}system/test\n      {{- end}}\nEOF\n</code></pre> <p>Effectively, by defining the <code>templateResourceRefs</code> at the beginning of the ClusterProfile we can retrieve the Sveltos managed clusters Kubernetes version. The information is used at the <code>policyRefs</code> definition when we set the path as a template with if statements.</p> <ol> <li> <p>The below ClusterProfile allows us to install the Flux Controllers in the management cluster. However, before applying it, ensure the management cluster has labels that match the specified <code>clusterSelector</code>. <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: flux\nspec:\n  clusterSelector:\n    matchLabels:\n      cluster: mgmt\n  helmCharts:\n  - chartName: flux2/flux2\n    chartVersion: 2.12.4\n    helmChartAction: Install\n    releaseName: flux2\n    releaseNamespace: flux2\n    repositoryName: flux2\n    repositoryURL: https://fluxcd-community.github.io/helm-charts\n</code></pre> \u21a9</p> </li> <li> <p>The YAML was obtained by running <code>helm template nginx-stable nginx-stable/nginx-ingress</code>.\u00a0\u21a9</p> </li> <li> <p>Do you want to dive deeper into Sveltos's templating features? Check out this section.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#helm-chart-deployment","title":"Helm Chart Deployment","text":"<p>The ClusterProfile spec.helmCharts can list a number of Helm charts to get deployed to the managed clusters with a specific label selector.</p> <p>Note</p> <p>Sveltos will deploy the Helm charts in the exact order they are defined (top-down approach).</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#example-single-helm-chart","title":"Example: Single Helm chart","text":"<pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>In the above YAML definition, we install Kyverno on a managed cluster with the label selector set to env=prod.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#example-multiple-helm-charts","title":"Example: Multiple Helm charts","text":"<pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: prometheus-grafana\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  helmCharts:\n  - repositoryURL:    https://prometheus-community.github.io/helm-charts\n    repositoryName:   prometheus-community\n    chartName:        prometheus-community/prometheus\n    chartVersion:     26.0.0\n    releaseName:      prometheus\n    releaseNamespace: prometheus\n    helmChartAction:  Install\n  - repositoryURL:    https://grafana.github.io/helm-charts\n    repositoryName:   grafana\n    chartName:        grafana/grafana\n    chartVersion:     8.6.4\n    releaseName:      grafana\n    releaseNamespace: grafana\n    helmChartAction:  Install\n</code></pre> <p>In the above YAML definition, we initially install the Prometheus community Helm chart and afterwards the Grafana Helm chart. The two defined Helm charts will get deployed on a managed cluster with the label selector set to env=fv.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#example-update-helm-chart-values","title":"Example: Update Helm Chart Values","text":"<pre><code>  ---\n  apiVersion: config.projectsveltos.io/v1beta1\n  kind: ClusterProfile\n  metadata:\n    name: kyverno\n  spec:\n    clusterSelector:\n      matchLabels:\n        env: fv\n    syncMode: Continuous\n    helmCharts:\n    - repositoryURL:    https://kyverno.github.io/kyverno/\n      repositoryName:   kyverno\n      chartName:        kyverno/kyverno\n      chartVersion:     v3.3.3\n      releaseName:      kyverno-latest\n      releaseNamespace: kyverno\n      helmChartAction:  Install\n      values: |\n        admissionController:\n          replicas: 1\n</code></pre> <p>The values field is the way to update different Helm chart values.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#example-update-helm-chart-values-from-referenced-configmapsecret","title":"Example: Update Helm Chart Values From Referenced ConfigMap/Secret","text":"<p>Sveltos allows us to manage Helm chart values using ConfigMaps/Secrets.</p> <p>Note</p> <p>Referenced Secrets must be of type addons.projectsveltos.io/cluster-profile</p> <p>For example, we can create a file named cleanup-controller.yaml with below content.</p> <pre><code>cleanupController:\n  livenessProbe:\n    httpGet:\n      path: /health/liveness\n      port: 9443\n      scheme: HTTPS\n    initialDelaySeconds: 16\n    periodSeconds: 31\n    timeoutSeconds: 5\n    failureThreshold: 2\n    successThreshold: 1\n</code></pre> <p>Create the <code>ConfigMap</code> resource.</p> <pre><code>$ kubectl create configmap cleanup-controller --from-file=cleanup-controller.yaml\n</code></pre> <p>Then, create another file named admission_controller.yaml with the below content.</p> <pre><code>admissionController:\n  readinessProbe:\n    httpGet:\n      path: /health/readiness\n      port: 9443\n      scheme: HTTPS\n    initialDelaySeconds: 6\n    periodSeconds: 11\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n</code></pre> <p>Create a <code>ConfigMap</code> resource.</p> <pre><code>$ kubectl create configmap admission-controller --from-file=admission-controller.yaml\n</code></pre> <p>Within the Sveltos <code>ClusterProfile</code> resource, define the <code>helmCharts</code> section. We can specify the Helm chart details and leverage the valuesFrom to reference the <code>ConfigMaps</code>.</p> <p>This injects the probe configurations from the <code>ConfigMaps</code> into the Helm chart values during deployment.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    values: |\n      admissionController:\n        replicas: 1\n    valuesFrom:\n    - kind: ConfigMap\n      name: cleanup-controller\n      namespace: default\n    - kind: ConfigMap\n      name: admission-controller\n      namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#example-template-based-referencing-for-valuesfrom","title":"Example: Template-based Referencing for ValuesFrom","text":"<p>In the ValuesFrom section, we can express <code>ConfigMap</code> and <code>Secret</code> names as templates. This allows us to generate them dynamically based on the available cluster information, simplifying management and reducing repetition.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#available-cluster-information","title":"Available cluster information","text":"<ul> <li>cluster namespace: <code>.Cluster.metadata.namespace</code></li> <li>cluster name: <code>.Cluster.metadata.name</code></li> <li>cluster type: <code>.Cluster.kind</code></li> </ul> <p>Consider two SveltosCluster instances in the civo namespace.</p> <pre><code>$ kubectl get sveltoscluster -n civo --show-labels\nNAME             READY   VERSION        LABELS\npre-production   true    v1.29.2+k3s1   env=civo,projectsveltos.io/k8s-version=v1.29.2\nproduction       true    v1.28.7+k3s1   env=civo,projectsveltos.io/k8s-version=v1.28.7\n</code></pre> <p>Four <code>ConfigMaps</code> are available within the same namespace.</p> <pre><code>$ kubectl get configmap -n civo\nNAME                                  DATA   AGE\nadmission-controller-pre-production   1      8m31s\nadmission-controller-production       1      7m49s\ncleanup-controller-pre-production     1      8m48s\ncleanup-controller-production         1      8m1s\n</code></pre> <pre><code>---\napiVersion: v1\ndata:\n  cleanup-values: |\n    cleanupController:\n      replicas: 1\nkind: ConfigMap\nmetadata:\n  name: cleanup-controller-pre-production\n  namespace: civo\n</code></pre> <p>The only difference between the <code>ConfigMaps</code> is the <code>admissionController</code> and <code>cleanupController</code> replicas setting: 1 for pre-production and 3 for production.</p> <p>The below points are included in the <code>ClusterProfile</code>.</p> <ol> <li>Matches both SveltosClusters</li> <li>Dynamic ConfigMap Selection:<ul> <li>For the <code>pre-production</code> cluster, the profile should use the <code>admission-controller-pre-production</code> and <code>cleanup-controller-pre-production</code> ConfigMaps.</li> <li>For the <code>production</code> cluster, the profile should use the <code>admission-controller-production</code> and <code>cleanup-controller-production</code> ConfigMaps.</li> </ul> </li> </ol> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: civo\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    values: |\n      backgroundController:\n        replicas: 3\n    valuesFrom:\n    - kind: ConfigMap\n      name: cleanup-controller-{{ .Cluster.metadata.name }}\n      namespace: civo\n    - kind: ConfigMap\n      name: admission-controller-{{ .Cluster.metadata.name }}\n      namespace: civo\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#example-express-helm-values-as-templates","title":"Example: Express Helm Values as Templates","text":"<p>Both the values section and the content stored in referenced <code>ConfigMaps</code> and <code>Secrets</code> can be written using templates. Sveltos will instantiate the templates using resources in the management cluster.</p> <p>Sveltos deploys the Helm chart with the final, resolved values. See the template section for more details.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-calico\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  helmCharts:\n  - repositoryURL:    https://projectcalico.docs.tigera.io/charts\n    repositoryName:   projectcalico\n    chartName:        projectcalico/tigera-operator\n    chartVersion:     v3.24.5\n    releaseName:      calico\n    releaseNamespace: tigera-operator\n    helmChartAction:  Install\n    values: |\n      installation:\n        calicoNetwork:\n          ipPools:\n          {{ range $cidr := .Cluster.spec.clusterNetwork.pods.cidrBlocks }}\n            - cidr: {{ $cidr }}\n              encapsulation: VXLAN\n          {{ end }}\n</code></pre> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-cilium-v1-26\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  helmCharts:\n  - chartName: cilium/cilium\n    chartVersion: 1.12.12\n    helmChartAction: Install\n    releaseName: cilium\n    releaseNamespace: kube-system\n    repositoryName: cilium\n    repositoryURL: https://helm.cilium.io/\n    values: |\n      k8sServiceHost: \"{{ .Cluster.spec.controlPlaneEndpoint.host }}\"\n      k8sServicePort: \"{{ .Cluster.spec.controlPlaneEndpoint.port }}\"\n      hubble:\n        enabled: false\n      nodePort:\n        enabled: true\n      kubeProxyReplacement: strict\n      operator:\n        replicas: 1\n        updateStrategy:\n          rollingUpdate:\n            maxSurge: 0\n            maxUnavailable: 1\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#example-oci-registry","title":"Example: OCI Registry","text":"<pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: vault\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    oci://registry-1.docker.io/bitnamicharts\n    repositoryName:   oci-vault\n    chartName:        vault\n    chartVersion:     0.7.2\n    releaseName:      vault\n    releaseNamespace: vault\n    helmChartAction:  Install\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#example-private-registry","title":"Example: Private Registry","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#docker-hub","title":"Docker Hub","text":"<p>Create a file named secret_content.yaml with the below content. Remember to replace the redacted values with the actual Docker Hub username and password/token.</p> <pre><code>{\"auths\":{\"https://registry-1.docker.io/v1/\":{\"username\":\"REDACTED\",\"password\":\"REDACTED\",\"auth\":\"username:password base64 encoded\"}}}\n</code></pre> <p>Use the kubectl command to create a Secret named regcred in the default namespace. The command references the secret_content.yaml file and sets the type to kubernetes.io/dockerconfigjson.</p> <pre><code>$ kubectl create secret generic regcred  --from-file=.dockerconfigjson=secret_content.yaml --type=kubernetes.io/dockerconfigjson\n</code></pre> <p>Then we can configure the <code>ClusterProfile</code> to use the newly created <code>Secret</code> for authentication with the Docker Hub.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: projectsveltos\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    oci://registry-1.docker.io/gianlucam76\n    repositoryName:   projectsveltos\n    chartName:        projectsveltos\n    chartVersion:     0.46.0\n    releaseName:      projectsveltos\n    releaseNamespace: projectsveltos\n    helmChartAction:  Install\n    registryCredentialsConfig:\n      credentials:\n        name: regcred\n        namespace: default\n</code></pre> <p>In this example, the <code>registryCredentialsConfig</code> section references the regcred Secret stored in the <code>default</code> namespace. This ensures the Helm chart can access the private registry during deployment.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#harbor","title":"Harbor","text":"<p>Another example using Harbor (on Civo cluster) as registry. Create a file named secret_harbor_content.yaml with the below content. Remember to replace the base64 encoded string with the actual Harbor credentials.</p> <pre><code>{\"auths\":{\"https://harbor.XXXX.k8s.civo.com\":{\"auth\":\"YWRtaW46SGFyYm9yMTIzNDU=\"}}}\n</code></pre> <p>Create a Secret named credentials in the <code>default</code> namespace using the secret_harbor_content.yaml file.</p> <pre><code>$ kubectl create secret generic credentials --from-file=.dockerconfigjson=secret_harbor_content.yaml --type=kubernetes.io/dockerconfigjson\n</code></pre> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: projectsveltos\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    oci://harbor.4fc01642-cfc0-4c55-a139-d593c92b232f.k8s.civo.com/library\n    repositoryName:   projectsveltos\n    chartName:        projectsveltos\n    chartVersion:     0.38.1\n    releaseName:      projectsveltos\n    releaseNamespace: projectsveltos\n    helmChartAction:  Install\n    registryCredentialsConfig:\n      insecureSkipTLSVerify: true\n      credentials:\n        name: credentials\n        namespace: default\n</code></pre> <p>The Harbor credentials can be also stored as BasicAuth. Like the example below.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: creds\n  namespace: default\ndata:\n  username: # base64 of user\n  password: # base64 of pass\n</code></pre> <p>The profile's HelmCharts section can reference the secret like the snippet below.</p> <pre><code>   registryCredentialsConfig:\n    credentials:\n     name: creds\n     namespace: default\n    insecureSkipTLSVerify: true\n</code></pre> <p>Note</p> <p>The <code>insecureSkipTLSVerify</code> option should only be used if your private registry does not support TLS verification. It's generally recommended to use a secure TLS connection and set the <code>CASecretRef</code> field in the <code>registryCredentialsConfig</code></p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#upgrade-crds","title":"Upgrade CRDs","text":"<p>Helm doesn't currently offer built-in support for upgrading CRDs. This was a deliberate decision to avoid potential data loss. There's also ongoing discussion within the Helm community about the ideal way to manage CRD lifecycles. Future Helm versions might address this.</p> <p>For custom Helm charts, you can work around this limitation by:</p> <ul> <li>Placing CRDs in templates: Instead of the crds/ directory, include your CRDs within the chart's templates folder. This allows them to be upgraded during the chart update process.</li> <li>Separate Helm chart: As suggested by the official Helm documentation, consider creating a separate Helm chart specifically for your CRDs. This allows independent management of those resources.</li> </ul> <p>However, using third-party Helm charts can be problematic as upgrading their CRDs might not be possible by default. Here's where Sveltos comes in. Sveltos allows you to control CRD upgrades for third-party charts through the <code>upgradeCRDs</code> field within your ClusterProfile configuration. When <code>upgradeCRDs</code> is set to true, Sveltos will initially patch all Custom Resource Definition (CRD) instances located in the Helm chart's crds/ directory. Once these CRDs are updated, Sveltos will proceed with the Helm upgrade process.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n...\n  helmCharts:\n  - repositoryURL:    &lt;REPO URL&gt;\n    repositoryName:   &lt;REPO NAME&gt;\n    chartName:        &lt;CHART NAME&gt;\n    chartVersion:     &lt;CHART VERSION&gt;\n    releaseName:      &lt;...&gt;\n    releaseNamespace: &lt;...&gt;\n    options:\n      upgradeOptions:\n        upgradeCRDs: true\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/helm_charts/#options","title":"Options","text":"<p>Sveltos allows you to configure Helm charts options during deployment.  For a complete list of Helm options, refer to the CRD.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: XXXX\nspec:\n  ...\n  helmCharts:\n  - repositoryURL:    &lt;REPO URL&gt;\n    repositoryName:   &lt;REPO NAME&gt;\n    chartName:        &lt;CHART NAME&gt;\n    chartVersion:     &lt;CHART VERSION&gt;\n    releaseName:      &lt;...&gt;\n    releaseNamespace: &lt;...&gt;\n    options:\n      disableHooks: true\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#introduction-to-kustomize-and-sveltos","title":"Introduction to Kustomize and Sveltos","text":"<p>Sveltos can seamlessly integrate with Flux to automatically deploy Kustomize code in a Git repository or a Bucket. This powerful combination allows you to manage Kubernetes configurations in a central location and leverage Sveltos to target deployments across clusters.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#sveltos-and-flux-sources","title":"Sveltos and Flux Sources","text":"<p>The example demonstrates how Sveltos utilizes a Flux GitRepository<sup>1</sup>. The git repository is located here and comprises multiple kustomize directories. Sveltos executes Kustomize on the <code>helloWorld</code> directory and deploys the Kustomize output to the <code>eng</code> namespace for every Sveltos managed cluster matching the defined clusterSelector.</p> <p>Sveltos ClusterProfile</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: hello-world\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  kustomizationRefs:\n  - namespace: flux2\n    name: flux2\n    kind: GitRepository\n    path: ./helloWorld/\n    targetNamespace: eng\n</code></pre> <p>Flux GitRepository Resource</p> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: flux2\n  namespace: flux2\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  timeout: 60s\n  url: ssh://git@github.com/gianlucam76/kustomize\n</code></pre> <p>Note</p> <p>Deploy both YAML manifest files to the Sveltos management cluster.</p> <pre><code>$ sveltosctl show addons\n+-------------------------------------+-----------------+-----------+----------------+---------+-------------------------------+---------------------------------+\n|               CLUSTER               |  RESOURCE TYPE  | NAMESPACE |      NAME      | VERSION |             TIME              |           PROFILES              |\n+-------------------------------------+-----------------+-----------+----------------+---------+-------------------------------+---------------------------------+\n| default/sveltos-management-workload | apps:Deployment | eng       | the-deployment | N/A     | 2023-05-16 00:48:11 -0700 PDT | ClusterProfile/hello-world      |\n| default/sveltos-management-workload | :Service        | eng       | the-service    | N/A     | 2023-05-16 00:48:11 -0700 PDT | ClusterProfile/hello-world      |\n| default/sveltos-management-workload | :ConfigMap      | eng       | the-map        | N/A     | 2023-05-16 00:48:11 -0700 PDT | ClusterProfile/hello-world      |\n+-------------------------------------+-----------------+-----------+----------------+---------+-------------------------------+---------------------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#substitution-and-templating","title":"Substitution and Templating","text":"<p>The Kustomize build process can generate parameterized YAML manifests. Sveltos can then instantiate the manifests using values provided in two locations.</p> <ol> <li><code>spec.kustomizationRefs.Values</code>: The field defines a list of key-value pairs directly within the ClusterProfile. These values are readily available for Sveltos to substitute into the template.</li> <li><code>spec.kustomizationRefs.ValuesFrom</code>: The field allows referencing external sources like ConfigMaps or Secrets. Their data sections contain key-value pairs that Sveltos can inject during template instantiation.</li> </ol>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#example-sveltos-value-injection","title":"Example: Sveltos Value Injection","text":"<p>Consider a Kustomize build output that includes a template for a deployment manifest.</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  namespace:  test\n  labels:\n    region: {{ default \"west\" .Region }}  # Placeholder for region with default value \"west\"\nspec:\n  ...\n  image: nginx:{{ .Version }}  # Placeholder for image version\n</code></pre> <p>Imagine Sveltos receives a <code>ClusterProfile</code> containing the below key-value pairs.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: hello-world-with-values\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  kustomizationRefs:\n  - deploymentType: Remote\n    kind: GitRepository\n    name: flux2\n    namespace: flux2\n    path: ./template/helloWorld/\n    targetNamespace: eng\n    values:\n      Region: east\n      Version: v1.2.0\n</code></pre> <p>During deployment, Sveltos injects these values into the template, replacing the below placeholders.</p> <ul> <li>{{ default \"west\" .Region }} is replaced with \"east\" (from the ClusterProfile's values).</li> <li>{{ .Version }} is replaced with \"v1.2.0\" (from the ClusterProfile's values).</li> </ul> <p>Taking this approach, we tranform the template into a concrete deployment manifest like the below.</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  namespace:  test\n  labels:\n    region: east # Replaced value\nspec:\n  ...\n  image: nginx:v1.2.0 # Replaced value\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#example-template-based-referencing-for-valuesfrom","title":"Example: Template-based Referencing for ValuesFrom","text":"<p>In the ValuesFrom section, we can express <code>ConfigMap</code> and <code>Secret</code> names as templates and dynamically generate them using cluster information. This allows for easier management and reduces redundancy.</p> <p>Available cluster information:</p> <ul> <li>cluster namespace: use <code>.Cluster.metadata.namespace</code></li> <li>cluster name: <code>.Cluster.metadata.name</code></li> <li>cluster type: <code>.Cluster.kind</code></li> </ul> <p>Consider two SveltosCluster instances in the civo namespace.</p> <pre><code>$ kubectl get sveltoscluster -n civo --show-labels\nNAME             READY   VERSION        LABELS\npre-production   true    v1.29.2+k3s1   env=civo,projectsveltos.io/k8s-version=v1.29.2\nproduction       true    v1.28.7+k3s1   env=civo,projectsveltos.io/k8s-version=v1.28.7\n</code></pre> <p>There are two ConfigMaps within the civo namespace. The ConfigMaps Data sections contain the same keys but different values.</p> <pre><code>$ kubectl get configmap -n civo\nNAME                                  DATA   AGE\nhello-world-pre-production            2      9m40s\nhello-world-production                2      9m45s\n</code></pre> <p>The below Sveltos ClusterProfile includes the following.</p> <ol> <li>Matches both SveltosClusters</li> <li>Dynamic ConfigMap Selection:<ul> <li>For the <code>pre-production</code> cluster, the profile should use the <code>hello-world-pre-production</code> ConfigMap.</li> <li>For the <code>production</code> cluster, the profile should use the <code>hello-world-production</code> ConfigMap.</li> </ul> </li> </ol> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: hello-world-with-values\nspec:\n  clusterSelector:\n    matchLabels:\n      env: civo\n  kustomizationRefs:\n  - deploymentType: Remote\n    kind: GitRepository\n    name: flux-system\n    namespace: flux-system\n    path: ./template/helloWorld/\n    targetNamespace: eng\n    valuesFrom:\n    - kind: ConfigMap\n      name: hello-world-{{ .Cluster.metadata.name }}\n      namespace: civo\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#example-dynamic-values-with-nested-templates","title":"Example: Dynamic Values with Nested Templates","text":"<p>Sveltos offers the capability to define key-value pairs where the value itself can be another template. The nested template can reference resources present in the management cluster. For example, consider the below key-value pair within a ClusterProfile.</p> <pre><code>  Region:  {{ index .Cluster.metadata.labels \"region\" }}\n  Version: v1.2.0\n</code></pre> <p>The value Region is not a static string, but a template referencing the .Cluster.metadata.labels.region property.</p> <p>During deployment, Sveltos retrieves information from the management cluster's Cluster instance (represented here as .Cluster). It then extracts the value associated with the \"region\" label using the index function and assigns it to the Region key-value pair.</p> <p>This mechanism allows us to dynamically populate values based on the management cluster's configuration, ensuring deployments adapt to specific environments.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#components","title":"Components","text":"<p>To specify reusable configuration pieces from Kustomize, we can use the components field within a Sveltos ClusterProfile. This allows us to include external components, defined in separate directories, into the main Kustomization.</p> <p>The components field is a list that points to the directories containing the Kustomize components. Each component should have its own kustomization.yaml file. The paths are relative to the Kustomization we are working</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: component-demo\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  kustomizationRefs:\n  - namespace: flux-system\n    name: flux-system\n    kind: GitRepository\n    path: ./import_components/overlays/community/\n    targetNamespace: eng\n    components:\n    - ../../components/external_db\n    - ../../components/recaptcha\n</code></pre> <p>In this example, the kustomizationRefs points to import_components/overlays/community/kustomization.yaml. This file will then pull in the two components specified: external_db and recaptcha, which are located two directories up in the components folder. This structure enables a modular and reusable approach to configuration.</p> <p>The repo being used here can be found here</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#example-all-in-one","title":"Example: All-in-One","text":"<p>The section outlines how Sveltos manages deployments using Kustomize and key-value pairs.</p> <ol> <li>Kustomize Build: Sveltos initiates a Kustomize build process to prepare the deployment manifest template.</li> <li> <p>Value Collection: Sveltos gathers key-value pairs for deployment customization from two sources:</p> <ul> <li>Directly defined values within the ClusterProfile's spec.kustomizationRefs.values field.</li> <li>ConfigMap/Secret references specified in spec.kustomizationRefs.valuesFrom. Sveltos extracts key-value pairs from the data section of these referenced resources.</li> </ul> </li> <li> <p>Optional: Nested Template Processing (Advanced Usage): For advanced scenarios, a key-value pair's value itself can be a template. Sveltos evaluates these nested templates using data available in the context, such as information from the management cluster. This allows dynamic value construction based on the management cluster's configuration.</p> </li> <li>Template Instantiation: Finally, Sveltos uses the processed key-value pairs to substitute placeholder values within the Kustomize build output. These placeholders are typically denoted by {{ .VariableName }}.</li> </ol> <p>This process ensures deployments are customized with appropriate values based on the ClusterProfile configuration and, optionally, the management cluster's state.</p> <p>Fully working example:</p> <ol> <li>Flux is used to sync git repository https://github.com/gianlucam76/kustomize</li> <li>The Kustomize build of <code>template/helloWorld</code> is a template</li> <li>key-value pairs (<code>Values</code> field) are expressed as template, so Sveltos will instatiate those using the Cluster instance</li> <li>Instantiated key-value pairs are used by Sveltos to instantiate the output of the Kustomize build</li> <li>Resources are finally deployed to the managed cluster</li> </ol> <p>Sveltos ClusterProfile</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: hello-world-with-template\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  kustomizationRefs:\n  - deploymentType: Remote\n    kind: GitRepository\n    name: flux2\n    namespace: flux2\n    path: ./template/helloWorld/\n    targetNamespace: eng\n    values:\n      Region: '{{ index .Cluster.metadata.labels \"region\" }}'\n      Version: v1.2.0\n  reloader: false\n  stopMatchingBehavior: WithdrawPolicies\n  syncMode: Continuous\n</code></pre> <p>Flux GitRepository Resource</p> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: flux2\n  namespace: flux2\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  timeout: 60s\n  url: https://github.com/gianlucam76/kustomize.git\n</code></pre> <pre><code>$ sveltosctl show addons\n+-----------------------------+-----------------+-----------+----------------+---------+--------------------------------+----------------------------+\n|           CLUSTER           |  RESOURCE TYPE  | NAMESPACE |      NAME      | VERSION |              TIME              |          PROFILES          |\n+-----------------------------+-----------------+-----------+----------------+---------+--------------------------------+--------------------------------------+\n| default/clusterapi-workload | apps:Deployment | eng       | the-deployment | N/A     | 2024-05-01 11:43:54 +0200 CEST | ClusterProfile/hello-world-with-template |\n| default/clusterapi-workload | :Service        | eng       | the-service    | N/A     | 2024-05-01 11:43:54 +0200 CEST | ClusterProfile/hello-world-with-template |\n| default/clusterapi-workload | :ConfigMap      | eng       | the-map        | N/A     | 2024-05-01 11:43:54 +0200 CEST | ClusterProfile/hello-world-with-template |\n+-----------------------------+-----------------+-----------+----------------+---------+--------------------------------+--------------------------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#example-express-path-as-template","title":"Example: Express Path as Template","text":"<p>The path field within a kustomizationRef object in Sveltos can be defined using a template. This allows you to dynamically set the path based on information from the cluster itself.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: flux-system\nspec:\n  clusterSelector:\n    matchLabels:\n      region: west\n  syncMode: Continuous\n  kustomizationRefs:\n  - namespace: flux2\n    name: flux2\n    kind: GitRepository\n    path: '{{ index .Cluster.metadata.annotations \"environment\" }}/helloWorld'\n    targetNamespace: eng\n</code></pre> <p>Sveltos uses the cluster instance in the management cluster to populate the template in the path field. The template expression <code>{{ index .Cluster.metadata.annotations \"environment\" }}</code> retrieves the value of the annotation named environment from the cluster's metadata.</p> <p>For instance:</p> <ol> <li>Cluster A: If cluster A has an annotation environment: production, the resulting path will be: production/helloWorld.</li> <li>Cluster B: If cluster B has an annotation environment: pre-production, the resulting path will be: pre-production/helloWorld.</li> </ol> <p>This approach allows for flexible configuration based on individual cluster environments.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#example-kustomize-with-configmaps","title":"Example: Kustomize with ConfigMaps","text":"<p>Directories containing Kustomize resources can be included in a ConfigMap (or a Secret) and use a Sveltos ClusterProfile to reference it.</p> <p>In this example, we are cloning the git repository <code>https://github.com/gianlucam76/kustomize</code> locally, then we create a <code>kustomize.tar.gz</code> with the content of the helloWorldWithOverlays directory.</p> <pre><code>$ git clone git@github.com:gianlucam76/kustomize.git\n\n$ tar -czf kustomize.tar.gz -C kustomize/helloWorldWithOverlays .\n\n$ kubectl create configmap kustomize --from-file=kustomize.tar.gz\n</code></pre> <p>The below ClusterProfile will use the Kustomize SDK to get all the resources needed for deployment. Then will deploy these in the <code>production</code> namespace of the managed clusters with the Sveltos clusterSelector set to env=fv.</p> <p>Sveltos ClusterProfile</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kustomize-with-configmap\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  kustomizationRefs:\n  - namespace: default\n    name: kustomize\n    kind: ConfigMap\n    path: ./overlays/production/\n    targetNamespace: production\n</code></pre> <pre><code>$ sveltosctl show addons\n+-------------------------------------+-----------------+-----------+----------------+---------+-------------------------------+---------------------------------+\n|               CLUSTER               |  RESOURCE TYPE  | NAMESPACE |      NAME      | VERSION |             TIME              |           PROFILES              |\n+-------------------------------------+-----------------+-----------+----------------+---------+-------------------------------+---------------------------------+\n| default/sveltos-management-workload | apps:Deployment | production | production-the-deployment | N/A     | 2023-05-16 00:59:13 -0700 PDT | kustomize-with-configmap |\n| default/sveltos-management-workload | :Service        | production | production-the-service    | N/A     | 2023-05-16 00:59:13 -0700 PDT | kustomize-with-configmap |\n| default/sveltos-management-workload | :ConfigMap      | production | production-the-map        | N/A     | 2023-05-16 00:59:13 -0700 PDT | kustomize-with-configmap |\n+-------------------------------------+-----------------+------------+---------------------------+---------+-------------------------------+--------------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/kustomize/#next-steps","title":"Next Steps","text":"<p>For a better understanding of the Sveltos and Flux integration, check out the Flux Sources examples here.</p> <ol> <li> <p>This ClusterProfile allows you to install Flux in your management cluster. However, before applying it, ensure your management cluster has labels that match the specified clusterSelector. <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: flux\nspec:\n  clusterSelector:\n    matchLabels:\n      cluster: mgmt\n  helmCharts:\n  - chartName: flux2/flux2\n    chartVersion: 2.12.4\n    helmChartAction: Install\n    releaseName: flux2\n    releaseNamespace: flux2\n    repositoryName: flux2\n    repositoryURL: https://fluxcd-community.github.io/helm-charts\n</code></pre> \u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#profiles","title":"Profiles","text":"<p>Profile is the CustomerResourceDefinition used to instruct Sveltos which add-ons to deploy on a set of clusters.</p> <p>Profile is a namespace-scoped resource.  It can only match clusters and reference resources within its own namespace.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#pause-annotation","title":"Pause Annotation","text":"<p>Pausing a ClusterProfile with the <code>profile.projectsveltos.io/paused</code> annotation prevents Sveltos from performing any reconciliation. This effectively freezes the ClusterProfile in its current state, ensuring that no changes are applied to the clusters it manages.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#specclusterselector","title":"Spec.ClusterSelector","text":"<p>clusterSelector field is used to specify which managed clusters should receive the add-ons and applications defined in the configuration.</p> <p>This field employs a Kubernetes label selector, allowing you to target clusters based on specific labels.</p> <pre><code>clusterSelector:\n    matchLabels:\n      env: prod\n</code></pre> <p>By leveraging matchExpressions, you can create more complex and flexible cluster selection criteria.</p> <pre><code>clusterSelector:\n  matchExpressions:\n  - {key: env, operator: In, values: [staging, production]}\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#spechelmcharts","title":"Spec.HelmCharts","text":"<p>helmCharts field consists of a list of helm charts to be deployed to the clusters matching clusterSelector;</p> <pre><code>  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    values: |\n      admissionController:\n        replicas: 1\n</code></pre> <p>Helm chart values can be dynamically retrieved from ConfigMaps or Secrets for flexible configuration. Customize Helm behavior with various options, and deploy charts from private container registries. For a complete list of features, refer to the Helm chart section.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#specpolicyrefs","title":"Spec.PolicyRefs","text":"<p>policyRefs field references a list of ConfigMaps/Secrets, each containing Kubernetes resources to be deployed in the clusters matching clusterSelector.</p> <p>This field is a slice of PolicyRef structs. Each PolictRef has the following fields:</p> <ul> <li>Kind: The kind of the referenced resource. The supported kinds are Secret and ConfigMap.</li> <li>Namespace: The namespace of the resource being referenced. This field is automatically set to the namespace of the Profile instance. In other words, a Profile instance can only reference resources that are within its own namespace.</li> <li>Name: The name of the referenced resource. This field must be at least one character long.</li> <li>DeploymentType: The deployment type of the referenced resource. This field indicates whether the resource should be deployed to the management cluster (local) or the managed cluster (remote). The default value is Remote.</li> </ul> <pre><code>policyRefs:\n- kind: Secret\n  name: my-secret-1\n  namespace: my-namespace-1\n  deploymentType: Local\n- kind: Remote\n  name: my-configmap-1\n  namespace: my-namespace-1\n  deploymentType: Remote\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#speckustomizationrefs","title":"Spec.KustomizationRefs","text":"<p>kustomizationRefs field is a list of sources containing kustomization files. Resources will be deployed in the clusters matching the clusterSelector specified.</p> <p>This field is a slice of KustomizationRef structs. Each KustomizationRef has the following fields:</p> <ul> <li> <p>Kind: The kind of the referenced resource. The supported kinds are:</p> <ul> <li>flux GitRepository, OCIRepository, Bucket: These kinds represent resources that store Kustomization manifests.</li> <li>ConfigMap, Secret: These kinds represent resources that contain Kustomization manifests or overlays.</li> </ul> </li> <li> <p>Namespace: The namespace of the resource being referenced. This field is automatically set to the namespace of the Profile instance. In other words, a Profile instance can only reference resources that are within its own namespace.</p> </li> <li>Name: The name of the referenced resource. This field must be at least one character long.</li> <li>Path: The path to the directory containing the kustomization.yaml file, or the set of plain YAMLs for which a kustomization.yaml should be generated. This field is optional and defaults to None, which means the root path of the SourceRef.</li> <li>TargetNamespace: The target namespace for the Kustomization deployment. This field is optional and can be used to override the namespace specified in the kustomization.yaml file.</li> <li>DeploymentType: The deployment type of the referenced resource. This field indicates whether the Kustomization deployment should be deployed to the management cluster (local) or the managed cluster (remote). The default value is Remote.</li> </ul> <p>For a complete list of features, refer to the Kustomize section.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#specsyncmode","title":"Spec.SyncMode","text":"<p>This field can be set to:</p> <ul> <li>OneTime</li> <li>Continuous</li> <li>ContinuousWithDriftDetection</li> <li>DryRun</li> </ul> <p>Let's take a closer look at the OneTime syncMode option. Once you deploy a Profile with a OneTime configuration, Sveltos will check all of your clusters for a match with the clusterSelector. Any matching clusters will have the resources specified in the Profile deployed. However, if you make changes to the Profile later on, those changes will not be automatically deployed to already-matching clusters.</p> <p>Now, if you're looking for real-time deployment and updates, the Continuous syncMode is the way to go. With Continuous, any changes made to the Profile will be immediately reconciled into matching clusters. This means that you can add new features, update existing ones, and remove them as necessary, all without lifting a finger. Sveltos will deploy, update, or remove resources in matching clusters as needed, making your life as a Kubernetes admin a breeze.</p> <p>ContinuousWithDriftDetection instructs Sveltos to monitor the state of managed clusters and detect a configuration drift for any of the resources deployed because of that Profile. When Sveltos detects a configuration drift, it automatically re-syncs the cluster state back to the state described in the management cluster. To know more about configuration drift detection, refer to this section.</p> <p>Imagine you're about to make some important changes to your Profile, but you're not entirely sure what the results will be. You don't want to risk causing any unwanted side effects, right? Well, that's where the DryRun syncMode configuration comes in. By deploying your Profile with this configuration, you can launch a simulation of all the operations that would normally be executed in a live run. The best part? No actual changes will be made to the matching clusters during this dry run workflow, so you can rest easy knowing that there won't be any surprises. To know more about dry run, refer to this section.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#specstopmatchingbehavior","title":"Spec.StopMatchingBehavior","text":"<p>The stopMatchingBehavior field specifies the behavior when a cluster no longer matches a Profile. By default, all Kubernetes resources and Helm charts deployed to the cluster will be removed. However, if StopMatchingBehavior is set to LeavePolicies, any policies deployed by the Profile will remain in the cluster.</p> <p>For instance</p> <p>Example - Profile Kyverno Deployment</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: Profile\nmetadata:\n  name: kyverno\n  namespace: eng\nspec:\n  stopMatchingBehavior: WithdrawPolicies\n  clusterSelector:\n    matchLabels:\n      env: prod\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>When a cluster matches the Profile, Kyverno Helm chart will be deployed in such a cluster. If the cluster's labels are subsequently modified and cluster no longer matches the Profile, the Kyverno Helm chart will be uninstalled. However, if the stopMatchingBehavior property is set to LeavePolicies, Sveltos will retain the Kyverno Helm chart in the cluster.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#specreloader","title":"Spec.Reloader","text":"<p>The reloader property determines whether rolling upgrades should be triggered for Deployment, StatefulSet, or DaemonSet instances managed by Sveltos and associated with this Profile when changes are made to mounted ConfigMaps or Secrets. When set to true, Sveltos automatically initiates rolling upgrades for affected Deployment, StatefulSet, or DaemonSet instances whenever any mounted ConfigMap or Secret is modified. This ensures that the latest configuration updates are applied to the respective workloads.</p> <p>Please refer to this section for more information.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#specmaxupdate","title":"Spec.MaxUpdate","text":"<p>A Profile might match more than one cluster. When a change is maded to a Profile, by default all matching clusters are update concurrently. The maxUpdate field specifies the maximum number of Clusters that can be updated concurrently during an update operation triggered by changes to the Profile's add-ons or applications. The specified value can be an absolute number (e.g., 5) or a percentage of the desired cluster count (e.g., 10%). The default value is 100%, allowing all matching Clusters to be updated simultaneously. For instance, if set to 30%, when modifications are made to the Profile's add-ons or applications, only 30% of matching Clusters will be updated concurrently. Updates to the remaining matching Clusters will only commence upon successful completion of updates in the initially targeted Clusters. This approach ensures a controlled and manageable update process, minimizing potential disruptions to the overall cluster environment. Please refer to this section for more information.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#specvalidatehealths","title":"Spec.ValidateHealths","text":"<p>The validateHealths property defines a set of Lua functions that Sveltos executes against the managed cluster to assess the health and status of the add-ons and applications specified in the Profile. These Lua functions act as validation checks, ensuring that the deployed add-ons and applications are functioning properly and aligned with the desired state. By executing these functions, Sveltos proactively identifies any potential issues or misconfigurations that could arise, maintaining the overall health and stability of the managed cluster.</p> <p>The ValidateHealths property accepts a slice of Lua functions, where each function encapsulates a specific validation check. These functions can access the managed cluster's state to perform comprehensive checks on the add-ons and applications. The results of the validation checks are aggregated and reported back to Sveltos, providing valuable insights into the health and status of the managed cluster's components.</p> <p>Lua's scripting capabilities offer flexibility in defining complex validation logic tailored to specific add-ons or applications.</p> <p>Please refer to this section for more information.</p> <p>Consider a scenario where a new cluster with the label env:prod is created. The following instructions guide Sveltos to:</p> <ul> <li>Deploy Kyverno Helm chart;</li> <li>Validate Deployment Health: Perform health checks on each deployment within the kyverno namespace. Verify that the number of active replicas matches the requested replicas;</li> <li>Successful Deployment: Once the health checks are successfully completed, Sveltos considers the Profile as successfully deployed.</li> </ul> <p>Example - Profile Kyverno and Lua</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: Profile\nmetadata:\n  name: kyverno\n  namespace: eng\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    validateHealths:\n    - name: deployment-health\n      featureID: Helm\n      group: \"apps\"\n      version: \"v1\"\n      kind: \"Deployment\"\n      namespace: kyverno\n      script: |\n        function evaluate()\n          hs = {}\n          hs.healthy = false\n          hs.message = \"available replicas not matching requested replicas\"\n          if obj.status ~= nil then\n            if obj.status.availableReplicas ~= nil then\n              if obj.status.availableReplicas == obj.spec.replicas then\n                hs.healthy = true\n              end\n            end\n          end\n          return hs\n        end\n</code></pre>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#spectemplateresourcerefs","title":"Spec.TemplateResourceRefs","text":"<p>The templateResourceRefs property specifies a collection of resources to be gathered from the management cluster. The values extracted from these resources will be utilized to instantiate templates embedded within referenced PolicyRefs and Helm charts. Refer to template section for more info and examples.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#specdependson","title":"Spec.DependsOn","text":"<p>The dependsOn property specifies a list of other Profiles that this instance relies on. In any managed cluster that matches to this Profile, the add-ons and applications defined in this instance will only be deployed after all add-ons and applications in the designated dependency Profiles have been successfully deployed.</p> <p>For example, profile-a can depend on another profile-b. This implies that any Helm charts or raw YAML files associated with CProfile A will not be deployed until all add-ons and applications specified in Profile B have been successfully provisioned.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: Profile\nmetadata:\n  name: profile_a\n  namespace: eng\nspec:\n  dependsOn:\n  - profile_b\n</code></pre> <p>Sveltos automatically resolves and deploys the prerequisite profiles specified in the DependsOn field. Sveltos will analyze the dependency graph, identify the required prerequisite profiles, and ensure they are deployed to the same clusters.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#speccontinueonerror","title":"Spec.ContinueOnError","text":"<p>ContinueOnError configures Sveltos' error handling. When true, errors are logged, but deployment continues. When false (default), Sveltos stops at the first error and retries the failing resource. For instance, if deploying three Helm charts, a failure during the second chart's deployment will halt the process, and Sveltos will retry the second chart. Only if ContinueOnError is true will Sveltos proceed to deploy the third chart before retrying the second chart.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/profile/#speccontinueonconflict","title":"Spec.ContinueOnConflict","text":"<p>ContinueOnConflict configures Sveltos' conflict resolution behavior. When true, Sveltos logs the conflicts but continues deploying the remaining resources. When false (default), Sveltos halts deployment at the first detected conflict. This can happen when another profile has already deployed the same resource.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/#clusterprofile-policyrefs-reference","title":"ClusterProfile policyRefs Reference","text":"<p>The ClusterProfile spec.policyRefs is a list of Secrets/ConfigMaps. Both Secrets and ConfigMaps data fields can be a list of key-value pairs. Any key is acceptable, and the value can be multiple objects in YAML or JSON format<sup>1</sup>.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/#example-create-a-secret","title":"Example: Create a Secret","text":"<p>To create a Kubernetes Secret that contains the Calico YAMLs and make it usable with Sveltos, utilise the below commands.</p> <pre><code>$ wget https://raw.githubusercontent.com/projectcalico/calico/master/manifests/calico.yaml\n\n$ kubectl create secret generic calico --from-file=calico.yaml --type=addons.projectsveltos.io/cluster-profile\n</code></pre> <p>The commands will download the calico.yaml manifest file and afterwards create a Kubernetes secret of type <code>generic</code> by defining the file downloaded in the previous command plus defining the needed <code>type=addons.projectsveltos.io/cluster-profile</code>.</p> <p>Note</p> <p>A ClusterProfile can only reference Secrets of type addons.projectsveltos.io/cluster-profile</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/#example-create-a-configmap","title":"Example: Create a ConfigMap","text":"<p>The YAML definition below exemplifies a <code>ConfigMap</code> that holds multiple resources<sup>2</sup>. When a ClusterProfile instance references the <code>ConfigMap</code>, a <code>Namespace</code> and a <code>Deployment</code> instance are automatically deployed in any managed cluster that adheres to the ClusterProfile clusterSelector.</p> <p>Example - Resources Definition</p> <pre><code>cat &gt; nginx_cm.yaml &lt;&lt;EOF\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx\n  namespace: default\ndata:\n  namespace.yaml: |\n    kind: Namespace\n    apiVersion: v1\n    metadata:\n      name: nginx\n  deployment.yaml: |\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: nginx-deployment\n      namespace: nginx\n    spec:\n      replicas: 2 # number of pods to run\n      selector:\n        matchLabels:\n          app: nginx\n      template:\n        metadata:\n          labels:\n            app: nginx\n        spec:\n          containers:\n          - name: nginx\n            image: nginx:latest # public image from Docker Hub\n            ports:\n            - containerPort: 80\nEOF\n</code></pre> <p>Once the required Kubernetes resources are created/deployed, the below example represents a ClusterProfile resource that references the <code>ConfigMap</code> and the <code>Secret</code> created above.</p> <p>Example - ClusterProfile Definition with Reference</p> <pre><code>cat &gt; clusterprofile_deploy_nginx.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  policyRefs:\n  - name: nginx\n    namespace: default\n    kind: ConfigMap\n  - name: calico\n    namespace: default\n    kind: Secret\nEOF\n</code></pre> <p>Note</p> <p>The <code>namespace</code> definition refers to the namespace where the <code>ConfigMap</code>, and the Secret were created in the management cluster. In our example, both resources created in the <code>default</code> namespace.</p> <p>When a ClusterProfile references a <code>ConfigMap</code> or a <code>Secret</code>, the kind and name fields are required, while the namespace field is optional. Specifying a namespace uniquely identifies the resource using the tuple namespace, name, and kind, and that resource will be used for all matching clusters.</p> <p>If you leave the namespace field empty, Sveltos will search for the <code>ConfigMap</code> or the <code>Secret</code> with the provided name within the namespace of each matching cluster.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/#example-understand-namespace-definition","title":"Example: Understand Namespace Definition","text":"<pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  policyRefs:\n  - name: nginx\n    kind: ConfigMap\n</code></pre> <p>Consider the provided ClusterProfile, when we have two workload clusters matching. One in the foo namespace and another in the bar namespace. Sveltos will search for the <code>ConfigMap</code> nginx in the foo namespace for the Cluster in the foo namespace and for a <code>ConfigMap</code> ngix in the bar namespace for the Cluster in the bar namespace.</p> <p>More ClusterProfile examples can be found here.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/#example-template-based-referencing-for-configmaps-and-secrets","title":"Example: Template-based Referencing for ConfigMaps and Secrets","text":"<p>We can express <code>ConfigMap</code> and <code>Secret</code> names as templates. This allows us to generate them dynamically based on the available cluster information, simplifying management and reducing repetition.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/#available-cluster-information","title":"Available cluster information","text":"<ul> <li>cluster namespace: <code>.Cluster.metadata.namespace</code></li> <li>cluster name: <code>.Cluster.metadata.name</code></li> <li>cluster type: <code>.Cluster.kind</code></li> </ul> <p>Consider two SveltosCluster instances in the civo namespace.</p> <pre><code>$ kubectl get sveltoscluster -n civo --show-labels\nNAME             READY   VERSION        LABELS\npre-production   true    v1.29.2+k3s1   env=civo,projectsveltos.io/k8s-version=v1.29.2\nproduction       true    v1.28.7+k3s1   env=civo,projectsveltos.io/k8s-version=v1.28.7\n</code></pre> <p>Two <code>ConfigMaps</code> named nginx-pre-production and nginx-production exist in the same namespace.</p> <pre><code>$ kubectl get configmap -n civo\nNAME                   DATA   AGE\nnginx-pre-production   2      4m59s\nnginx-production       2      4m41s\n</code></pre> <p>The only difference between the <code>ConfigMaps</code> is the replicas setting: 1 for pre-production and 3 for production.</p> <p>The below points are included in the <code>ClusterProfile</code>.</p> <ol> <li>Matches both SveltosClusters</li> <li>Dynamic ConfigMap Selection:<ul> <li>For the <code>pre-production</code> cluster, the profile should use the <code>nginx-pre-production</code> ConfigMap.</li> <li>For the <code>production</code> cluster, the profile should use the <code>nginx-production</code> ConfigMap.</li> </ul> </li> </ol> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-nginx\nspec:\n  clusterSelector:\n    matchLabels:\n      env: civo\n  policyRefs:\n  - name: nginx-{{ .Cluster.metadata.name }}\n    kind: ConfigMap\n</code></pre> <p>The demonstrated approach provides a flexible and centralized way to reference <code>ConfigMaps</code> and <code>Secrets</code> based on the availanle cluster information.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/#template","title":"Template","text":"<p>Define the content for resources in your <code>PolicyRefs</code> using templates. During deployment, Sveltos will automatically populate these templates with relevant information from your cluster and other resources in the management cluster. See the template section template section for details.</p> <p>Remember to adapt the provided resources to your specific repository structure, cluster configuration, and desired templating logic.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"addons/raw_yaml/#subresources","title":"Subresources","text":"<p>Sveltos can update specific subresources of a resource. This is achieved by leveraging the <code>projectsveltos.io/subresources</code> annotation. When the annotation is present on a resource referenced in the <code>PolicyRefs</code> section, Sveltos updates the designated subresources alongside the main resource. Subresources are specified as a comma-separated list within the annotation value.</p> <p>For example, to instruct Sveltos to update the status subresource of a Service, we can create a <code>ConfigMap</code> with the following structure and reference this <code>ConfigMap</code> from a ClusterProfile/Profile.</p> <pre><code>---\napiVersion: v1\ndata:\n  service.yaml: |\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: sveltos-subresource\n      namespace: default\n    spec:\n      selector:\n        app: foo\n      ports:\n      - name: my-port\n        port: 443\n        protocol: TCP\n        targetPort: 1032\n      type: LoadBalancer\n    status:\n      loadBalancer:\n        ingress:\n        - ip: 1.1.1.1\nkind: ConfigMap\nmetadata:\n  annotations:\n    projectsveltos.io/subresources: status\n  name: load-balancer-service\n  namespace: default\n</code></pre> <ol> <li> <p>A ConfigMap is not designed to hold large chunks of data. The data stored in a ConfigMap cannot exceed 1 MiB. If you need to store settings that are larger than this limit, you may want to consider mounting a volume or use a separate database or file service.\u00a0\u21a9</p> </li> <li> <p>Another way to create a Kubernetes ConfigMap resource is with the imperative approach. The below command will create the same ConfigMap resource in the management cluster. <pre><code>$ kubectl create configmap nginx --from-file=namespace.yaml --from-file=deployment.yaml\n</code></pre> \u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"deployment_order/depends_on/","title":"Resource Deployment Order","text":"","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/depends_on/#introduction-to-dependson","title":"Introduction to dependsOn","text":"<p>ClusterProfile instances can leverage other ClusterProfiles to establish a deployment order for add-ons and applications. The dependsOn fields enables the definition of prerequisite ClusterProfiles. Within any managed cluster that matches the current ClusterProfile, the deployment of different add-ons and applications will start once all add-ons and applications in the specified dependency ClusterProfiles have been successfully deployed.</p>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/depends_on/#example-kyverno-clusterprofiles","title":"Example: Kyverno ClusterProfiles","text":"<p>The below examaple displays a ClusterProfile which encapsulates all Kyverno policies for a cluster and declares a ClusterProfile dependency named <code>kyverno</code>, which is responsible for installing the Kyverno Helm chart.</p> <pre><code>---\n  apiVersion: config.projectsveltos.io/v1beta1\n  kind: ClusterProfile\n  metadata:\n    name: kyverno-admission-policies\n  spec:\n    clusterSelector:\n      matchLabels:\n        env: production\n    dependsOn:\n    - kyverno\n    policyRefs:\n    - kind: ConfigMap\n      name: disallow-latest-tag\n      namespace: default\n    - kind: ConfigMap\n      name: restrict-wildcard-verbs\n      namespace: default\n</code></pre> <p><sup>1</sup></p> <pre><code>  ---\n  apiVersion: config.projectsveltos.io/v1beta1\n  kind: ClusterProfile\n  metadata:\n    name: kyverno\n  spec:\n    helmCharts:\n    - chartName: kyverno/kyverno\n      chartVersion: v3.3.3\n      helmChartAction: Install\n      releaseName: kyverno-latest\n      releaseNamespace: kyverno\n      repositoryName: kyverno\n      repositoryURL: https://kyverno.github.io/kyverno/\n</code></pre> <p>Notice that the <code>kyverno</code> ClusterProfile lacks a clusterSelector, so it won't be deployed on its own. The <code>kyverno-admission-policies</code> ClusterProfile, however, has a clusterSelector targeting production clusters and a dependsOn field referencing kyverno. When this profile is created, Sveltos resolves its dependency. Any time a cluster matches the <code>kyverno-admission-policies</code> selector, Sveltos will first deploy the Kyverno Helm chart and then apply the admission policies.</p> <p></p>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/depends_on/#example-kyverno-and-kubevela-clusterprofile","title":"Example: Kyverno and Kubevela ClusterProfile","text":"<p>In the below YAML definitions, the ClusterProfile instance cp-kubevela relies on the ClusterProfile instance cp-kyverno. That means that the cp-kyverno ClusterProfile add-ons will get deployed to clusters matching the label set to <code>env=prod</code> and afterwards, the ClusterProfile <code>cp-kubevela</code> will take place.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: cp-kubevela\nspec:\n  dependsOn:\n  - cp-kyverno\n  clusterSelector:\n    matchLabels:\n      env: production\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL: https://kubevela.github.io/charts\n    repositoryName: kubevela\n    chartName: kubevela/vela-core\n    chartVersion: 1.9.6\n    releaseName: kubevela-core-latest\n    releaseNamespace: vela-system\n    helmChartAction: Install\n</code></pre> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: cp-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: production\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>The above example is equivalent of creating a single ClusterProfile.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: cp-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n  - repositoryURL:    https://kubevela.github.io/charts\n    repositoryName:   kubevela\n    chartName:        kubevela/vela-core\n    chartVersion:     1.9.6\n    releaseName:      kubevela-core-latest\n    releaseNamespace: vela-system\n    helmChartAction:  Install\n</code></pre> <p>Note</p> <p>Separate ClusterProfiles promote better organization and maintainability, especially when different teams or individuals manage different ClusterProfiles.</p>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/depends_on/#recursive-resolution","title":"Recursive Resolution","text":"<p>One of the key strengths of Sveltos is its ability to handle complex dependency chains. Imagine an application <code>whoami</code> that relies on <code>Traefik</code>, which itself depends on <code>cert-manager</code>. With Sveltos, you only need to define the deployment of <code>whoami</code>. Sveltos will automatically resolve the entire dependency tree, ensuring <code>cert-manager</code> and <code>Traefik</code> are deployed in the correct order before <code>whoami</code> is deployed. This simplifies complex deployments by automating the resolution of multi-level dependencies.</p> <p></p>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/depends_on/#dependency-deduplication","title":"Dependency Deduplication","text":"<p>Sveltos efficiently manages shared dependencies by ensuring they are deployed only once per cluster, even when multiple profiles rely on them. This optimizes resource utilization and prevents redundant deployments. Crucially, Sveltos maintains a dependency as long as any profile requiring it is active on the cluster.</p> <p></p> <p>When <code>frontend-app-1</code> is deployed, Sveltos first deploys <code>postgresql</code> and then <code>backend-service-1</code>, resolving the dependency chain. Subsequently, when <code>frontend-app-2</code> is deployed to the same cluster, Sveltos recognizes that <code>postgresql</code> is already present and avoids redeploying it. If <code>frontend-app-1</code> is then removed, <code>backend-service-1</code> is also removed. However, <code>postgresql</code> persists because it remains a dependency of <code>frontend-app-2</code>. Finally, only when <code>frontend-app-2</code> is removed will Sveltos remove <code>backend-service-2</code> and <code>postgresql</code>, as they are no longer required by any active profile on the cluster.</p> <p>\ud83d\udc49 Read more here:</p> <ol> <li> <p>To create the ConfigMaps with Kyverno policies used in this example <pre><code>$ wget https://raw.githubusercontent.com/kyverno/policies/main/best-practices/disallow-latest-tag/disallow-latest-tag.yaml\n\n$ kubectl create configmap disallow-latest-tag --from-file disallow-latest-tag.yaml\n\n$ wget https://raw.githubusercontent.com/kyverno/policies/main/other/res/restrict-wildcard-verbs/restrict-wildcard-verbs.yaml\n\n$ kubectl create configmap restrict-wildcard-verbs --from-file restrict-wildcard-verbs.yaml\n</code></pre> \u21a9</p> </li> </ol>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/depends_on_with_health_checks/","title":"Resource Deployment Order - depends-on","text":"<p>Managing multiple applications across different teams, each of them requiring the presence of the cert-manager, consider utilizing a ClusterProfile to deploy cert-manager centrally.</p> <p>This approach enables other ClusterProfiles, responsible for deploying applications that depend on cert-manager, to leverage the <code>dependsOn</code> field to ensure the cert-manager is present prior to application deployment.</p> <p>To guarantee that cert-manager is not only deployed but also functional, employ the validateHealths flag. The below ClusterProfile will deploy cert-manager in any cluster matching the label selector <code>env=fv</code> and subsequently wait for all deployments in the cert-manager namespace to reach a healthy state (active replicas matching requested replicas) before setting the ClusterProfile as <code>provisioned</code>.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: cert-manager\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://charts.jetstack.io\n    repositoryName:   jetstack\n    chartName:        jetstack/cert-manager\n    chartVersion:     v1.13.2\n    releaseName:      cert-manager\n    releaseNamespace: cert-manager\n    helmChartAction:  Install\n    values: |\n      installCRDs: true\n  validateHealths:\n  - name: deployment-health\n    featureID: Helm\n    group: \"apps\"\n    version: \"v1\"\n    kind: \"Deployment\"\n    namespace: cert-manager\n    script: |\n      function evaluate()\n        local hs = {healthy = false, message = \"available replicas not matching requested replicas\"}\n        if obj.status and obj.status.availableReplicas ~= nil and obj.status.availableReplicas == obj.spec.replicas then\n          hs.healthy = true\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/depends_on_with_health_checks/#common-expression-language-cel-for-health-validation","title":"Common Expression Language (CEL) for Health Validation","text":"<p>Alternatively, you can use Common Expression Language (CEL), which offers a more concise way to define the same health rule. The example below uses a CEL expression to check if the availableReplicas are equal to the requested replicas. The result is the same as the Lua script, providing a healthy and succinct way to validate the state of your deployments.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: cert-manager\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://charts.jetstack.io\n    repositoryName:   jetstack\n    chartName:        jetstack/cert-manager\n    chartVersion:     v1.13.2\n    releaseName:      cert-manager\n    releaseNamespace: cert-manager\n    helmChartAction:  Install\n    values: |\n      installCRDs: true\n  validateHealths:\n  - name: deployment-health\n    featureID: Helm\n    group: \"apps\"\n    version: \"v1\"\n    kind: \"Deployment\"\n    namespace: cert-manager\n    evaluateCEL:\n    - name: deployment_replicas\n      rule: resource.status.availableReplicas == resource.spec.replicas\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/depends_on_with_health_checks/#example-nginx-and-cert-manager","title":"Example: Nginx and Cert Manager","text":"<p>In the below example, the ClusterPofile to deploy the nginx ingress depends on the cert-manager ClusterProfile defined above.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: ingress-nginx\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kubernetes.github.io/ingress-nginx\n    repositoryName:   ingress-nginx\n    chartName:        ingress-nginx/ingress-nginx\n    chartVersion:     \"4.8.4\"\n    releaseName:      ingress-nginx\n    releaseNamespace: ingress-nginx\n    helmChartAction:  Install\n  dependsOn:\n  - cert-manager\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/manifest_order/","title":"Resource Deployment Order - Manifest Order","text":"","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/manifest_order/#introduction-to-deployment-resource-order","title":"Introduction to Deployment Resource Order","text":"<p>When Kubernetes resources are deployed in a cluster, it is sometimes necessary to deploy them in a specific order. For example, a CustomResourceDefinition (CRD) must exist before a custom resources of that type can be created.</p> <p>Sveltos can assist solving this problem by allowing users to specify the order in which Kubernetes resources are deployed.</p>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/manifest_order/#clusterprofile-order","title":"ClusterProfile Order","text":"<ol> <li>ClusterProfile helmCharts field: The <code>helmCharts</code> field allows users to specify a list of Helm charts that need to get deployed. Sveltos will deploy the Helm charts in the order they appear in the list (top-down approach).</li> <li>ClusterProfile policyRefs field: The <code>policyRefs</code> field allows you to reference a list of ConfigMap and Secret resources whose contents need to get deployed. Sveltos will deploy the resources in the order they appear (top-down approach).</li> <li>ClusterProfile kustomizationRefs field: The <code>kustomizationRefs</code> field allows you to reference a list of sources containing kustomization files. Sveltos will deploy the resources in the order they appear in the list (top-down approach)</li> </ol>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/manifest_order/#example-prometheus-and-grafana-definition","title":"Example: Prometheus and Grafana definition","text":"<ul> <li>The below ClusterProfile definition will first deploy the Prometheus Helm chart and then the Grafana Helm chart.</li> </ul> <p>Example - ClusterProfile Monitoring</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: prometheus-grafana\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://prometheus-community.github.io/helm-charts\n    repositoryName:   prometheus-community\n    chartName:        prometheus-community/prometheus\n    chartVersion:     26.0.0\n    releaseName:      prometheus\n    releaseNamespace: prometheus\n    helmChartAction:  Install\n  - repositoryURL:    https://grafana.github.io/helm-charts\n    repositoryName:   grafana\n    chartName:        grafana/grafana\n    chartVersion:     8.6.4\n    releaseName:      grafana\n    releaseNamespace: grafana\n    helmChartAction:  Install\n</code></pre> <p></p>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/manifest_order/#example-postgresql-resource-deployment","title":"Example: PostgreSQL Resource Deployment","text":"<ul> <li>The below ClusterProfile will first deploy the ConfigMap resource named <code>postgresql-deployment</code> and then the ConfigMap resource named <code>postgresql-service</code>.</li> </ul> <p>Example - ClusterProfile Database</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: postgresql\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  policyRefs:\n  - name: postgresql-deployment\n    namespace: default\n    kind: ConfigMap\n  - name: postgresql-service\n    namespace: default\n    kind: ConfigMap\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/order_with_events/","title":"Resource Deployment Order - Order with Events","text":"","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/order_with_events/#scenario-description","title":"Scenario Description","text":"<p>In some cases, it is necessary to deploy Kubernetes resources only after other resources are in a <code>healthy</code> state. For example, a Job that creates a table in a database should not be deployed until the database Deployment is healthy.</p> <p>Sveltos can assist with this problem by allowing you to use events to control the rollout of your application.</p> <p>An event is a notification that is sent when a certain condition is met. For example, you can create an event that it is sent when a database Deployment becomes healthy.</p> <p>You can then use this event to trigger the deployment of the Job that creates the table in the database.</p> <p>By using events, you can ensure that your application is rolled out in a controlled and orderly manner.</p> <p></p> <p></p>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/order_with_events/#postgresql-deployment-and-service","title":"PostgreSQL Deployment and Service","text":"<p><sup>1</sup>With the ConfigMap postgresql-deployment and the postgresql-service containing respectively PostgreSQL Deployment and Service<sup>2</sup>, the below ClusterProfile will instruct Sveltos to create a PostgreSQL Deployment and Service in all clusters matching the label env: fv.</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: postgresql\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  policyRefs:\n  - name: postgresql-deployment\n    namespace: default\n    kind: ConfigMap\n  - name: postgresql-service\n    namespace: default\n    kind: ConfigMap\n</code></pre> <pre><code>$ sveltosctl show addons\n\n+-----------------------------+-----------------+-----------+------------+---------+-------------------------------+------------------+\n|           CLUSTER           |  RESOURCE TYPE  | NAMESPACE |    NAME    | VERSION |             TIME              | CLUSTER PROFILES |\n+-----------------------------+-----------------+-----------+------------+---------+-------------------------------+------------------+\n| default/clusterapi-workload | apps:Deployment | todo      | postgresql | N/A     | 2023-08-20 08:23:11 -0700 PDT | postgresql       |\n| default/clusterapi-workload | :Service        | todo      | postgresql | N/A     | 2023-08-20 08:23:11 -0700 PDT | postgresql       |\n+-----------------------------+-----------------+-----------+------------+---------+-------------------------------+------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/order_with_events/#create-a-table-in-the-database","title":"Create a Table in the database","text":"<p>With the ConfigMap postgresql-job containing a Job that creates a table Todo in the database<sup>3</sup>, the below YAML definition instruct Sveltos to wait for the PostgreSQL Deployment to be healthy and only then deply the Job.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\nname: postgresql-deployment-health\nspec:\ncollectResources: false\nresourceSelectors:\n- group: \"apps\"\n  version: \"v1\"\n  kind: \"Deployment\"\n  namespace: todo\n  evaluate: |\n    function evaluate()\n      hs = {}\n      hs.matching = false\n      hs.message = \"\"\n      if obj.metadata.name == \"postgresql\" then\n        if obj.status ~= nil then\n          if obj.status.availableReplicas ~= nil then\n            if obj.status.availableReplicas == obj.spec.replicas then\n              hs.matching = true\n            end\n          end\n        end\n      end\n      return hs\n    end\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\nname: deploy-insert-table-job\nspec:\nsourceClusterSelector:\n  matchLabels:\n    env: fv\neventSourceName: postgresql-deployment-health\nstopMatchingBehavior: LeavePolicies\npolicyRefs:\n- name: postgresql-job\n  namespace: default\n  kind: ConfigMap\n</code></pre> <p>As soon as the PostgreSQL Deployment is <code>healthy</code>, Sveltos will deploy the Job. The Job will create table Todo.</p> <pre><code>$ sveltosctl show addons\n+-----------------------------+-----------------+-----------+------------+---------+-------------------------------+------------------------------+\n|           CLUSTER           |  RESOURCE TYPE  | NAMESPACE |    NAME    | VERSION |             TIME              |       CLUSTER PROFILES       |\n+-----------------------------+-----------------+-----------+------------+---------+-------------------------------+------------------------------+\n| default/clusterapi-workload | apps:Deployment | todo      | postgresql | N/A     | 2023-08-20 08:23:11 -0700 PDT | postgresql                   |\n| default/clusterapi-workload | :Service        | todo      | postgresql | N/A     | 2023-08-20 08:23:11 -0700 PDT | postgresql                   |\n| default/clusterapi-workload | batch:Job       | todo      | todo-table | N/A     | 2023-08-20 08:30:19 -0700 PDT | sveltos-2gv4dh8dl5fqy2z0amnx |\n+-----------------------------+-----------------+-----------+------------+---------+-------------------------------+------------------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/order_with_events/#deploy-todo-app","title":"Deploy todo App","text":"<p>With the ConfigMap todo-app containing the todo app (ServiceAccount, Deployment, Service)<sup>4</sup>, the below YAML defintion instructs Sveltos to deploy the <code>todo</code> app only after previous Job is complete.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\nname: postgresql-job-completed\nspec:\ncollectResources: false\nresourceSelectors:\n- group: \"batch\"\n  version: \"v1\"\n  kind: \"Job\"\n  namespace: todo\n  evaluate: |\n    function evaluate()\n      hs = {}\n      hs.matching = false\n      hs.message = \"\"\n      if obj.metadata.name == \"todo-table\" then\n        if obj.status ~= nil then\n          if obj.status.succeeded == 1 then\n            hs.matching = true\n          end\n        end\n      end\n      return hs\n    end\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\nname: deploy-todo-app\nspec:\nsourceClusterSelector:\n  matchLabels:\n    env: fv\neventSourceName: postgresql-job-completed\nstopMatchingBehavior: LeavePolicies\npolicyRefs:\n- name: todo-app\n  namespace: default\n  kind: ConfigMap\n</code></pre> <pre><code>$ sveltosctl show addons\n+-----------------------------+---------------------------+-----------+-------------+---------+-------------------------------+------------------------------+\n|           CLUSTER           |       RESOURCE TYPE       | NAMESPACE |    NAME     | VERSION |             TIME              |       CLUSTER PROFILES       |\n+-----------------------------+---------------------------+-----------+-------------+---------+-------------------------------+------------------------------+\n| default/clusterapi-workload | apps:Deployment           | todo      | postgresql  | N/A     | 2023-08-20 08:23:11 -0700 PDT | postgresql                   |\n| default/clusterapi-workload | :Service                  | todo      | postgresql  | N/A     | 2023-08-20 08:23:11 -0700 PDT | postgresql                   |\n| default/clusterapi-workload | batch:Job                 | todo      | todo-table  | N/A     | 2023-08-20 08:30:19 -0700 PDT | sveltos-2gv4dh8dl5fqy2z0amnx |\n| default/clusterapi-workload | :Service                  | todo      | todo-gitops | N/A     | 2023-08-20 08:36:17 -0700 PDT | sveltos-n7201iyuxbsyra94r59f |\n| default/clusterapi-workload | :ServiceAccount           | todo      | todo-gitops | N/A     | 2023-08-20 08:36:17 -0700 PDT | sveltos-n7201iyuxbsyra94r59f |\n| default/clusterapi-workload | apps:Deployment           | todo      | todo-gitops | N/A     | 2023-08-20 08:36:17 -0700 PDT | sveltos-n7201iyuxbsyra94r59f |\n| default/clusterapi-workload | networking.k8s.io:Ingress | todo      | todo        | N/A     | 2023-08-20 08:36:17 -0700 PDT | sveltos-n7201iyuxbsyra94r59f |\n+-----------------------------+---------------------------+-----------+-------------+---------+-------------------------------+------------------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/order_with_events/#write-entry-to-database","title":"Write Entry to Database","text":"<p>With the ConfigMap todo-insert-data containing a Job which will add an entry to databse<sup>5</sup>, the below YAML definition instructs Sveltos to deploy a Job only after the <code>todo</code> app is <code>healthy</code>.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\nname: todo-app-health\nspec:\ncollectResources: false\nresourceSelectors:\n- group: \"apps\"\n  version: \"v1\"\n  kind: \"Deployment\"\n  namespace: todo\n  evaluate: |\n    function evaluate()\n      hs = {}\n      hs.matching = false\n      hs.message = \"\"\n      if obj.metadata.name == \"todo-gitops\" then\n        if obj.status ~= nil then\n          if obj.status.availableReplicas ~= nil then\n            if obj.status.availableReplicas == obj.spec.replicas then\n              hs.matching = true\n            end\n          end\n        end\n      end\n      return hs\n    end\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\nname: insert-data\nspec:\nsourceClusterSelector:\n  matchLabels:\n    env: fv\neventSourceName: todo-app-health\nstopMatchingBehavior: LeavePolicies\npolicyRefs:\n- name: todo-insert-data\n  namespace: default\n  kind: ConfigMap\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/order_with_events/#scenario-resources","title":"Scenario Resources","text":"<ol> <li> <p>The example used in this document is based on and modified from here.\u00a0\u21a9</p> </li> <li> <p>Get PostgreSQL YAML <pre><code>$ wget https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/postgresql_deployment.yaml\n\n$ wget https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/postgresql_service.yaml\n\n$ kubectl create configmap postgresql-deployment --from-file postgresql_deployment.yaml\n\n$ kubectl create configmap postgresql-service --from-file postgresql_service.yaml\n</code></pre> \u21a9</p> </li> <li> <p>Get Job YAML <pre><code>$ wget https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/postgresql_job.yaml\n\n$ kubectl create configmap postgresql-job --from-file postgresql_job.yaml\n</code></pre> \u21a9</p> </li> <li> <p>Get todo-app YAML <pre><code>$ wget https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/todo_app.yaml\n\n$ kubectl create configmap todo-app --from-file todo_app.yaml\n</code></pre> \u21a9</p> </li> <li> <p>Get Job YAML <pre><code>$ wget https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/todo_insert.yaml\n\n$ kubectl create configmap todo-insert-data --from-file todo_insert.yaml\n</code></pre> \u21a9</p> </li> </ol>","tags":["Kubernetes","Sveltos","add-ons","order"]},{"location":"deployment_order/rolling_update_strategy/","title":"ClusterProfile Rolling Update Strategy","text":"","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/rolling_update_strategy/#clusterprofile-rolling-update-strategy","title":"ClusterProfile Rolling Update Strategy","text":"<p>A ClusterProfile might match more than one clusters. When adding or modifying a ClusterProfile, it is helpful to:</p> <ul> <li>Incrementally add a new configuration to a few clusters at a time.</li> <li>Validate the health of the deployment before declaring it successful.</li> </ul> <p>To support this, Sveltos uses two <code>ClusterProfile Spec</code> fields: <code>MaxUpdate</code> and <code>ValidateHealths</code>.</p>","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/rolling_update_strategy/#maxupdate","title":"MaxUpdate","text":"<p><code>MaxUpdate</code> indicates the maximum number of clusters that can be updated concurrently. The value can be an absolute number (e.g., 5) or a percentage of the desired managed clusters (e.g., 10%). The default vlue is set to 100%.</p>","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/rolling_update_strategy/#example","title":"Example","text":"<p>When the field is set to 30%, the list of add-ons/applications in ClusterProfile changes, only 30% of the matching clusters will be updated in parallel. Only when the updates in these clusters succeed, it will proceed with the update of the remaining clusters</p>","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/rolling_update_strategy/#validatehealths","title":"ValidateHealths","text":"<p>The <code>validateHealths</code> field in a ClusterProfile Spec allows you to specify health validation checks that Sveltos should perform before declaring an update successful. These checks are expressed using the Lua language.</p>","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/rolling_update_strategy/#example_1","title":"Example","text":"<p>For instance, when deploying Helm charts, it is possible to instruct Sveltos to check the deployments health (number of active replicas) before declaring the Helm chart deployment successful.</p> <pre><code>validateHealths:\n- name: deployment-health\n  featureID: Helm\n  group: \"apps\"\n  version: \"v1\"\n  kind: \"Deployment\"\n  namespace: kyverno\n  script: |\n    function evaluate()\n      hs = {}\n      hs.healthy = false\n      hs.message = \"available replicas not matching requested replicas\"\n      if obj.status ~= nil then\n        if obj.status.availableReplicas ~= nil then\n          if obj.status.availableReplicas == obj.spec.replicas then\n            hs.healthy = true\n          end\n        end\n      end\n      return hs\n    end\n</code></pre> <p>The above YAML definition instructs Sveltos to fetch all the deployments in the kyverno namespace. For each of those, the Lua script is evaluated.</p> <p>The Lua function must be named <code>evaluate</code>. It is passed as a single argument, which is an instance of the object being validated (<code>obj</code>). The function must return a struct containing a field <code>healthy</code>, which is a boolean indicating whether the resource is healthy or not. The struct can also have an optional field <code>message</code>, which will be reported back by Sveltos if the resource is not healthy.</p>","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/rolling_update_strategy/#rolling-update-strategy-benefits","title":"Rolling Update Strategy Benefits","text":"<p>A rolling update strategy allows you to update your clusters gradually, minimizing downtime and risk. By updating a few clusters at a time, you can identify and resolve any issues before rolling out the update to all of your clusters. Additionally, you can use the ValidateHealths field to ensure that your clusters are healthy before declaring the update successful.</p>","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/rolling_update_strategy/#all-in-one-example-rolling-update-strategy","title":"All in One: Example Rolling Update Strategy","text":"<p>To use the rolling update strategy, simply set the <code>MaxUpdate</code> field in the ClusterProfile Spec to the desired number of clusters to update concurrently. You can also use the <code>ValidateHealths</code> field to specify any health validation checks that you want to perform.</p> <p>The following ClusterProfile Spec would update a maximum of 30% of matching clusters concurrently, and would check that the number of active replicas for all deployments in the kyverno namespace matche the requested replicas before declaring the update successful.</p> <p>Example ClusterProfile - Kyverno - Lua</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  maxUpdate: 30%\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    values: |\n      admissionController:\n        replicas: 1\n  validateHealths:\n  - name: deployment-health\n    featureID: Helm\n    group: \"apps\"\n    version: \"v1\"\n    kind: \"Deployment\"\n    namespace: kyverno\n    script: |\n      function evaluate()\n        hs = {}\n        hs.healthy = false\n        hs.message = \"available replicas not matching requested replicas\"\n        if obj.status ~= nil then\n          if obj.status.availableReplicas ~= nil then\n            if obj.status.availableReplicas == obj.spec.replicas then\n              hs.healthy = true\n            end\n          end\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/rolling_update_strategy/#manual-verification-lua-script","title":"Manual Verification Lua Script","text":"<p>To verify the Lua script without a cluster, you can follow steps pointers.</p> <ol> <li>Clone addon-controller repo</li> <li>Navigate to the <code>controllers/health_policies/deployment_health directory</code>: <code>cd controllers/health_policies/deployment_health</code></li> <li>Create a directory <code>mkdir my_script</code></li> <li>Create a new file named <code>lua_policy.lua</code> in the directory you just created, and add your <code>evaluate</code> function to it.</li> <li>Create a new file named <code>valid_resource.yaml</code> in the same directory, and add a healthy resource to it. This is a resource that your evaluate function should evaluate to healthy.</li> <li>Create a new file named <code>invalid_resource.yaml</code> in the same directory, and add a non-healthy resource to it. This is a resource that your evaluate function should evaluate to false.</li> <li>Run the following command to build and run the unit tests: <code>make ut</code></li> </ol> <p>Tip</p> <p>If the unit tests pass, the Lua script is valid.</p>","tags":["Kubernetes","add-ons","rolling update","clusterapi","multi-tenancy"]},{"location":"deployment_order/tiers/","title":"Resource Deployment Order - Order with Tiers","text":"","tags":["Kubernetes","Sveltos","add-ons","order","tier"]},{"location":"deployment_order/tiers/#introduction-to-sveltos-tiers","title":"Introduction to Sveltos Tiers","text":"<p>The section explains ClusterProfile/Profile in Sveltos, focusing on how tiers enable prioritized deployments for resources targeted by multiple configurations.</p>","tags":["Kubernetes","Sveltos","add-ons","order","tier"]},{"location":"deployment_order/tiers/#efficient-cluster-management-with-clusterprofilesprofiles","title":"Efficient Cluster Management with ClusterProfiles/Profiles","text":"<p>Sveltos streamlines application and add-on deployments across different Kubernetes clusters using ClusterProfiles and Profiles. They target a set of managed clusters, simplifying the configuration for large groups.</p>","tags":["Kubernetes","Sveltos","add-ons","order","tier"]},{"location":"deployment_order/tiers/#addressing-the-challenge-of-subset-modifications","title":"Addressing the Challenge of Subset Modifications","text":"<p>Imagine we have the need to adjust deployments for a subset of clusters within a previously defined group. Traditionally, creating a new Profile targeting the subset and including resources already managed by another profile would lead to conflicts. Sveltos will not allow deployment for the overlapping resources.</p>","tags":["Kubernetes","Sveltos","add-ons","order","tier"]},{"location":"deployment_order/tiers/#introducing-tiers-for-conflict-resolution-and-priority","title":"Introducing Tiers for Conflict Resolution and Priority","text":"<p>Sveltos tiers provide a solution for managing deployment priority when resources are targeted by multiple configurations.</p> <ul> <li>Tier Property: Each ClusterProfile/Profile now has a new property called <code>tier</code></li> <li>Deployment Order Control: The tier value dictates the deployment order for resources targeting the same element within a cluster (e.g., a Helm chart or Kubernetes object)</li> <li>Default Behavior: By default, the first configuration to reach the cluster successfully deploys the resource</li> <li>Tier Overrides: <code>tier</code> overrides the default behavior. In case of conflicts, the configuration with the lowest tier value takes precedence and deploys the resource</li> </ul> <p>Note</p> <p>Lower tier values represent higher priority deployments. The <code>default</code> tier value is set to 100.</p>","tags":["Kubernetes","Sveltos","add-ons","order","tier"]},{"location":"deployment_order/tiers/#example","title":"Example","text":"<p>Let's consider a scenario with managed clusters labeled by environment (<code>env:prod</code>) and regions (<code>region:west</code> and <code>region:east</code>).</p> <p>An initial ClusterProfile named <code>validation-and-monitoring</code> deploys Kyverno (v3.1.1), Prometheus (23.4.0), and Grafana (6.58.9) across all clusters (<code>env:prod</code>).</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: validation-and-monitoring\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  continueOnConflict: true\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n  - repositoryURL:    https://prometheus-community.github.io/helm-charts\n    repositoryName:   prometheus-community\n    chartName:        prometheus-community/prometheus\n    chartVersion:     26.0.0\n    releaseName:      prometheus\n    releaseNamespace: prometheus\n    helmChartAction:  Install\n  - repositoryURL:    https://grafana.github.io/helm-charts\n    repositoryName:   grafana\n    chartName:        grafana/grafana\n    chartVersion:     8.6.4\n    releaseName:      grafana\n    releaseNamespace: grafana\n    helmChartAction:  Install\n</code></pre> <p>Imagine we want to upgrade Kyverno to version 3.1.4 and only in the clusters within the <code>region:west</code> area.</p> <p>Creating a new ClusterProfile targeting <code>region:west</code> for Kyverno (v3.1.4) would result in a conflict because both profiles manage Kyverno. By default, the first one wins (validation-and-monitoring), and the upgrade will not occur.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\nname: kyverno\nspec:\nclusterSelector:\n  matchLabels:\n    region: west\nhelmCharts:\n- repositoryURL:    https://kyverno.github.io/kyverno/\n  repositoryName:   kyverno\n  chartName:        kyverno/kyverno\n  chartVersion:     v3.3.3\n  releaseName:      kyverno-latest\n  releaseNamespace: kyverno\n  helmChartAction:  Install\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order","tier"]},{"location":"deployment_order/tiers/#resolving-conflicts-with-tiers","title":"Resolving Conflicts with Tiers","text":"<p>We can leverage <code>tiers</code> to prioritize the upgrade for west regions. Check the YAML definition below for more details. The revised ClusterProfile targets the <code>region:west</code> and set the<code>tier</code> to the value of 50 (lower than the default 100).</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  tier: 50\n  clusterSelector:\n    matchLabels:\n      region: west\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>The <code>kyverno</code> ClusterProfile (tier:50) overrides the default <code>validation-and-monitoring</code> profile (tier:100) for the Kyverno deployment in west clusters. This ensures Kyverno gets upgraded to version 3.1.4 only in the desired region.</p> <p>The snippet below shows a sample cluster state after applying both ClusterProfiles.</p> <pre><code>+-----------------------------+---------------+------------+----------------+---------+--------------------------------+------------------------------------------+\n|           CLUSTER           | RESOURCE TYPE | NAMESPACE  |      NAME      | VERSION |              TIME              |                 PROFILES                 |\n+-----------------------------+---------------+------------+----------------+---------+--------------------------------+------------------------------------------+\n| default/clusterapi-workload | helm chart    | prometheus | prometheus     | 23.4.0  | 2024-05-14 12:48:20 +0200 CEST | ClusterProfile/validation-and-monitoring |\n| default/clusterapi-workload | helm chart    | grafana    | grafana        | 6.58.9  | 2024-05-14 12:48:31 +0200 CEST | ClusterProfile/validation-and-monitoring |\n| default/clusterapi-workload | helm chart    | kyverno    | kyverno-latest | 3.1.4   | 2024-05-14 13:01:55 +0200 CEST | ClusterProfile/kyverno                   |\n+-----------------------------+---------------+------------+----------------+---------+--------------------------------+------------------------------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","order","tier"]},{"location":"events/addon_event_deployment/","title":"Event Driven Addon Distribution - Project Sveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#introduction-to-sveltos-event-framework","title":"Introduction to Sveltos Event Framework","text":"<p>Sveltos supports an event-driven workflow:</p> <ol> <li>Define what an event is;</li> <li>Select the clusters to watch for such events;</li> <li>Define the event trigger: which add-ons/applications to deploy when the events occur</li> </ol> <p>By default, add-ons/applications are deployed in the same cluster where the events are detected. However, Sveltos also supports cross-clusters:</p> <ol> <li>If an event happens in the cluster foo</li> <li>Deploy the add-ons in the cluster bar</li> </ol> <p>For more information, take a peek at this link.</p> <p></p> <p>Tip</p> <p>Once Sveltos is deployed to the management cluster, it is automatically registered in the <code>mgmt</code> namespace with the name <code>mgmt</code>. Add-ons and applications can be deployed using the Sveltos EventFramework as soon as the appropriate Kubernetes labels are added to the cluster. For more details, see the registration section.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#eventsource","title":"EventSource","text":"<p>An Event is a specific operation in the context of Kubernetes objects. To define an event, use the EventSource Custom Resource Definition (CRD).</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#example-createdelete-service-event","title":"Example: Create/Delete Service Event","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: sveltos-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    labelFilters:\n    - key: sveltos\n      operation: Equal\n      value: fv\n</code></pre> <p>In the above YAML definition, an <code>EventSource</code> instance defines an event as a creation/deletion of a Service with the label set to sveltos: fv.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#example-createdelete-service-event-using-lua","title":"Example: Create/Delete Service Event using Lua","text":"<p>Sveltos supports custom events written in the Lua language.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: sveltos-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    evaluate: |\n      function evaluate()\n        hs = {}\n        hs.matching = false\n        hs.message = \"\"\n        if obj.metadata.labels ~= nil then\n          for key, value in pairs(obj.metadata.labels) do\n            if key == \"sveltos\" then\n              if value == \"fv\" then\n                hs.matching = true\n              end\n            end\n          end\n        end\n        return hs\n      end\n</code></pre> <p>In the above YAML definition, an <code>EventSource</code> instance defines an event as a creation/deletion of a Service with the label set to sveltos: fv using a Lua script.</p> <p>When providing Sveltos with a Lua script, Sveltos expects the below format:</p> <ol> <li>It must contain a function <code>function evaluate()</code>. This is the function that is directly invoked and passed a Kubernetes resource (inside the function <code>obj</code> represents the passed in Kubernetes resource). Any field of the obj can be accessed, for instance obj.metadata.labels to access labels;</li> <li>It must return a Lua table with the below fields:<ul> <li><code>matching</code>: it is a bool indicating whether the resource matches the EventSource instance;</li> <li><code>message</code>: it is a string that can be set and Sveltos will print the value, if set.</li> </ul> </li> </ol> <p>Before applying the Lua Event based definition, it is advisable to validate the script beforehand. Simply clone the sveltos-agent repository. Afterwards, in the pkg/evaluation/events directory, create a directory for your resource. If the directory already exists, create a subdirectory.</p> <p>In the newly created directory or subdirectory, create the below.</p> <ol> <li>A file named <code>eventsource.yaml</code> containing the EventSource instance with Lua script;</li> <li>A file named <code>matching.yaml</code> containing a Kubernetes resource supposed to be a match for the Lua script created in #1 (this is optional);</li> <li>A file named <code>non-matching.yaml</code> containing a Kubernetes resource supposed to not be a match for the Lua script created in #1 (this is optional);</li> <li>Run make test</li> </ol> Lua and <code>AggregatedSelection</code> field <p>The above steps will load the Lua script, pass it the matching (if available) and non-matching (if available) resources and verify result (hs.matching set to true for matching resource, hs.matching set to false for the non matching resource).</p> <p>Resources of different kinds can be examined together. The AggregatedSelection is an optional field and can be used to specify a Lua function that will be used to further select a subset of the resources that have already been selected using the ResourceSelector field. The function will receive the array of resources selected by the <code>ResourceSelectors</code> and can be used as a way to perform more complex filtering or selection operations on the resources, looking at all of them together.</p> <p>The Lua function must return a struct with:</p> <ul> <li><code>resources</code> field: slice of matching resorces;</li> <li><code>message</code> field: (optional) message.</li> </ul> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: sveltos-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"apps\"\n    version: \"v1\"\n    kind: \"Deployment\"\n  - kind: HorizontalPodAutoscaler\n    group: \"autoscaling\"\n    version: v2\n  aggregatedSelection: |\n      function getKey(namespace, name)\n        return namespace .. \":\" .. name\n      end\n\n      function evaluate()\n        local hs = {}\n        hs.message = \"\"\n\n        local deployments = {}\n        local autoscalers = {}\n        local deploymentsWithNoAutoscaler = {}\n\n        for _, resource in ipairs(resources) do\n          local kind = resource.kind\n          if kind == \"Deployment\" then\n            key = getKey(resource.metadata.namespace, resource.metadata.name)\n            deployments[key] = true\n          elseif kind == \"HorizontalPodAutoscaler\" then\n            table.insert(autoscalers, resource)\n          end\n        end\n\n        -- Check for each horizontalPodAutoscaler if there is a matching Deployment\n        for _,hpa in ipairs(autoscalers) do\n            key = getKey(hpa.metadata.namespace, hpa.spec.scaleTargetRef.name)\n            if hpa.spec.scaleTargetRef.kind == \"Deployment\" then\n              if not deployments[key] then\n                table.insert(unusedAutoscalers, hpa)\n              end\n            end\n        end\n\n        if #unusedAutoscalers &gt; 0 then\n          hs.resources = unusedAutoscalers\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#example-createdelete-service-event-using-cel","title":"Example: Create/Delete Service Event using CEL","text":"<p>Sveltos supports custom events written in the CEL language.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: sveltos-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    evaluateCEL:\n    - name: service_with_label_sveltos_fv\n      rule: has(resource.metadata.labels) &amp;&amp; has(resource.metadata.labels.sveltos) &amp;&amp; resource.metadata.labels.sveltos == \"fv\"\n</code></pre> <p>In the above YAML definition, an <code>EventSource</code> instance defines an event as a creation/deletion of a Service with the label set to sveltos: fv but with a CEL definition.</p> <p>Multiple rules can be defined. A resource is a match if it matches at least one rule.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: sveltos-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    evaluateCEL:\n    - name: service_with_label_sveltos_fv # Rule 1: Service has label sveltos: fv\n      rule: has(resource.metadata.labels) &amp;&amp; has(resource.metadata.labels.sveltos) &amp;&amp; resource.metadata.labels.sveltos == \"fv\"\n    - name: default_namespace_service # Rule 2: Service is in the 'default' namespace\n      rule: resource.metadata.namespace == \"default\"\n    - name: service_with_port_8080 # Rule 3: Service exposes port 8080\n      rule: &gt;\n        has(resource.spec.ports) &amp;&amp;\n        resource.spec.ports.exists(p, p.port == 8080)\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#eventtrigger","title":"EventTrigger","text":"<p>EventTrigger is the CRD introduced to define what add-ons to deploy when an event happens.</p> <p>Each <code>EvenTrigger</code> instance:</p> <ol> <li>References an EventSource (which defines what the event is);</li> <li>Has a sourceClusterSelector selecting one or more managed clusters; <sup>1</sup></li> <li>Contains a list of add-ons to deploy</li> </ol> <p></p> <p>The below <code>EventTrigger</code> references the <code>EventSource</code> sveltos-service defined previously. It references a <code>ConfigMap</code> that contains a NetworkPolicy expressed as a template.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: service-network-policy\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: fv\n  eventSourceName: sveltos-service\n  oneForEvent: true\n  policyRefs:\n  - name: network-policy\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: network-policy\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok # this annotation is what tells Sveltos to instantiate this ConfigMap\ndata:\n  networkpolicy.yaml: |\n    kind: NetworkPolicy\n    apiVersion: networking.k8s.io/v1\n    metadata:\n      name: front-{{ .Resource.metadata.name }}\n      namespace: {{ .Resource.metadata.namespace }}\n    spec:\n      podSelector:\n        matchLabels:\n          {{ range $key, $value := .Resource.spec.selector }}\n          {{ $key }}: {{ $value }}\n          {{ end }}\n      ingress:\n        - from:\n          - podSelector:\n              matchLabels:\n                app: internal\n          ports:\n            {{ range $port := .Resource.spec.ports }}\n            - port: {{ $port.port }}\n            {{ end }}\n</code></pre> <p>The Resource in the above <code>ConfigMap</code> YAML definition refers to the specific Kubernetes resource within the managed cluster that generated the event (identified by the Service instance with the label <code>sveltos:fv</code>).</p> <p>Additionally, the template can access information about the cluster where the event originated. To utilize the name of the managed cluster you can use <code>{{ .Cluster.metadata.name }}</code> in your template.</p> <p>Anytime a Service with the label set to sveltos:fv is created in a managed cluster matching the sourceClusterSelector, a NetworkPolicy with an ingress definition is created.</p> <p>For example, if the below Service is created in a managed cluster:</p> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  labels:\n    sveltos: fv\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre> <p>A NetworkPolicy instance is instantiated from the <code>ConfigMap</code> content, using the information from Service (labels and ports) and it is created in the managed cluster.</p> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  annotations:\n    projectsveltos.io/hash: sha256:8e7e0a7848eef3f75aed25d1136631dd58bdb9761709a9c46153bb5d04d69e8b\n  creationTimestamp: \"2023-03-14T16:01:44Z\"\n  generation: 1\n  labels:\n    projectsveltos.io/reference-kind: ConfigMap\n    projectsveltos.io/reference-name: sveltos-evykjze69n3bz3gavzw4\n    projectsveltos.io/reference-namespace: projectsveltos\n  name: front-my-service\n  namespace: default\n  ownerReferences:\n  - apiVersion: config.projectsveltos.io/v1beta1\n    kind: ClusterProfile\n    name: sveltos-8ric1wghsf04cu8i1387\n    uid: ca908a7b-e9a7-457b-a077-81400b59902f\n  resourceVersion: \"2312\"\n  uid: 410e8da6-dddc-4c34-9045-8c3967119ae9\nspec:\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: internal\n    ports:\n    - port: 80\n      protocol: TCP\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/name: MyApp\n  policyTypes:\n  - Ingress\nstatus: {}\n</code></pre> <p></p> <p>In a nutshell, the below flow is executed.</p> <ol> <li>The sveltos-agent in the managed cluster consumes the <code>EventSource</code> instances and detects when an event happens;</li> <li>When an event occurs, it is reported to management cluster (along with resources, since EventSource Spec.CollectResources is set to true) in the form of EventReport;</li> <li> <p>The event-manager pod running in the management cluster, consumes the EventReport and:</p> <ul> <li> <p>creates a new <code>ConfigMap</code> in the projectsveltos namespace, whose content is derived from <code>ConfigMap</code> the <code>EventTrigger</code> instance references, and instantiated using information coming the resource in the managed cluster (Service instance with label sveltos:fv);</p> </li> <li> <p>creates a <code>ClusterProfile</code>.</p> </li> </ul> </li> </ol> <p>Note</p> <p>The field <code>EventSourceName</code> can be expressed as template and dynamically generate them using cluster information. This allows for easier management and reduces redundancy. Any cluster field can be used, for instance: <code>{{ .Cluster.metadata.name }}-{{ index .Cluster.metadata.labels \"region\" }}</code>.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#eventsource-collectresources-setting","title":"EventSource CollectResources Setting","text":"<p>The collectResources field in an EventSource (default: false) determines whether Kubernetes resources matching the EventSource should be collected and transmitted to the management cluster for template instantiation.</p> <ul> <li>When collectResources is true: templates can directly reference the Resource, which is a full representation of the matched Kubernetes resource (e.g., a Service with the label sveltos:fv) from the managed cluster.</li> <li>When collectResources is false: templates can access the MatchingResources, a corev1.ObjectReference providing essential metadata information about the matched resource (e.g., Service with the label sveltos:fv) without the complete resource details.</li> </ul> <p>Based on the example above, the below EventReport instance can be found in the management cluster.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventReport\nmetadata:\n  creationTimestamp: \"2023-03-14T15:55:23Z\"\n  generation: 2\n  labels:\n    eventreport.projectsveltos.io/cluster-name: sveltos-management-workload\n    eventreport.projectsveltos.io/cluster-type: capi\n    projectsveltos.io/eventsource-name: sveltos-service\n  name: capi--sveltos-service--sveltos-management-workload\n  namespace: default\n  resourceVersion: \"7151\"\n  uid: 0b71c54c-7c0e-4478-b48e-0081e2432c58\nspec:\n  clusterName: sveltos-management-workload\n  clusterNamespace: default\n  clusterType: Capi\n  eventSourceName: sveltos-service\n  matchingResources:\n  - apiVersion: v1\n    kind: Service\n    name: my-service\n    namespace: default\n  resources: eyJhcGlWZXJzaW9uIjoidjEiLCJraW5kIjoiU2VydmljZSIsIm1ldGFkYXRhIjp7ImFubm90YXRpb25zIjp7Imt1YmVjdGwua3ViZXJuZXRlcy5pby9sYXN0LWFwcGxpZWQtY29uZmlndXJhdGlvbiI6IntcImFwaVZlcnNpb25cIjpcInYxXCIsXCJraW5kXCI6XCJTZXJ2aWNlXCIsXCJtZXRhZGF0YVwiOntcImFubm90YXRpb25zXCI6e30sXCJsYWJlbHNcIjp7XCJzdmVsdG9zXCI6XCJmdlwifSxcIm5hbWVcIjpcIm15LXNlcnZpY2VcIixcIm5hbWVzcGFjZVwiOlwiZGVmYXVsdFwifSxcInNwZWNcIjp7XCJwb3J0c1wiOlt7XCJwb3J0XCI6ODAsXCJwcm90b2NvbFwiOlwiVENQXCIsXCJ0YXJnZXRQb3J0XCI6OTM3Nn1dLFwic2VsZWN0b3JcIjp7XCJhcHAua3ViZXJuZXRlcy5pby9uYW1lXCI6XCJNeUFwcFwifX19XG4ifSwiY3JlYXRpb25UaW1lc3RhbXAiOiIyMDIzLTAzLTE0VDE2OjAxOjE0WiIsImxhYmVscyI6eyJzdmVsdG9zIjoiZnYifSwibWFuYWdlZEZpZWxkcyI6W3siYXBpVmVyc2lvbiI6InYxIiwiZmllbGRzVHlwZSI6IkZpZWxkc1YxIiwiZmllbGRzVjEiOnsiZjptZXRhZGF0YSI6eyJmOmFubm90YXRpb25zIjp7Ii4iOnt9LCJmOmt1YmVjdGwua3ViZXJuZXRlcy5pby9sYXN0LWFwcGxpZWQtY29uZmlndXJhdGlvbiI6e319LCJmOmxhYmVscyI6eyIuIjp7fSwiZjpzdmVsdG9zIjp7fX19LCJmOnNwZWMiOnsiZjppbnRlcm5hbFRyYWZmaWNQb2xpY3kiOnt9LCJmOnBvcnRzIjp7Ii4iOnt9LCJrOntcInBvcnRcIjo4MCxcInByb3RvY29sXCI6XCJUQ1BcIn0iOnsiLiI6e30sImY6cG9ydCI6e30sImY6cHJvdG9jb2wiOnt9LCJmOnRhcmdldFBvcnQiOnt9fX0sImY6c2VsZWN0b3IiOnt9LCJmOnNlc3Npb25BZmZpbml0eSI6e30sImY6dHlwZSI6e319fSwibWFuYWdlciI6Imt1YmVjdGwtY2xpZW50LXNpZGUtYXBwbHkiLCJvcGVyYXRpb24iOiJVcGRhdGUiLCJ0aW1lIjoiMjAyMy0wMy0xNFQxNjowMToxNFoifV0sIm5hbWUiOiJteS1zZXJ2aWNlIiwibmFtZXNwYWNlIjoiZGVmYXVsdCIsInJlc291cmNlVmVyc2lvbiI6IjIyNTIiLCJ1aWQiOiIzNDg2ODE1Yi1kZjk1LTRhMzAtYjBjMi01MGFlOGEyNmI4ZWIifSwic3BlYyI6eyJjbHVzdGVySVAiOiIxMC4yMjUuMTY2LjExMyIsImNsdXN0ZXJJUHMiOlsiMTAuMjI1LjE2Ni4xMTMiXSwiaW50ZXJuYWxUcmFmZmljUG9saWN5IjoiQ2x1c3RlciIsImlwRmFtaWxpZXMiOlsiSVB2NCJdLCJpcEZhbWlseVBvbGljeSI6IlNpbmdsZVN0YWNrIiwicG9ydHMiOlt7InBvcnQiOjgwLCJwcm90b2NvbCI6IlRDUCIsInRhcmdldFBvcnQiOjkzNzZ9XSwic2VsZWN0b3IiOnsiYXBwLmt1YmVybmV0ZXMuaW8vbmFtZSI6Ik15QXBwIn0sInNlc3Npb25BZmZpbml0eSI6Ik5vbmUiLCJ0eXBlIjoiQ2x1c3RlcklQIn0sInN0YXR1cyI6eyJsb2FkQmFsYW5jZXIiOnt9fX0KLS0t\n</code></pre> <p>The resources is a base64 encoded representation of the Service.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#eventtrigger-oneforevent-setting","title":"EventTrigger OneForEvent setting","text":"<p>The EventTrigger <code>OneForEvent</code> (false by default) field indicates whether to create one ClusterProfile for a Kubernetes resource matching the referenced EventSource, or one for all resources.</p> <p>In the above example, if we create another Service in the managed cluster with the label set to sveltos: fv</p> <pre><code>$ kubectl get services -A --selector=sveltos=fv\nNAMESPACE   NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\ndefault     another-service   ClusterIP   10.225.134.41    &lt;none&gt;        443/TCP   24m\ndefault     my-service        ClusterIP   10.225.166.113   &lt;none&gt;        80/TCP    52m\n</code></pre> <p>two NetworkPolicies will be created, one per Service.</p> <pre><code>$ kubectl get networkpolicy -A\nNAMESPACE   NAME                    POD-SELECTOR                          AGE\ndefault     front-another-service   app.kubernetes.io/name=MyApp-secure   8m40s\ndefault     front-my-service        app.kubernetes.io/name=MyApp          8m40s\n</code></pre> <p>A possible example for OneForEvent false, is when the add-ons to deploy are not template. For instance if Kyverno needs to be deployed in any managed cluster where certain event happened.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: service-network-policy\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: fv\n  eventSourceName: &lt;your eventSource name&gt;\n  oneForEvent: false\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>Currently, it is not possible to change this field once set.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#cleanup","title":"Cleanup","text":"<p>Note</p> <p>Based on the example above, if a Service is deleted, the NetworkPolicy is removed automatically by Sveltos.</p> <pre><code>$ kubectl get services -A --selector=sveltos=fv\nNAMESPACE   NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\ndefault     my-service        ClusterIP   10.225.166.113   &lt;none&gt;        80/TCP    54m\n</code></pre> <pre><code>$ kubectl get networkpolicy -A\nNAMESPACE   NAME                    POD-SELECTOR                          AGE\ndefault     front-my-service        app.kubernetes.io/name=MyApp          10m40s\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/addon_event_deployment/#events-and-multi-tenancy","title":"Events and Multi-tenancy","text":"<p>If the below label is set on the <code>EventSource</code> instance by the tenant admin, Sveltos will make sure tenant admin can define events only looking at resources it has been authorized to by platform admin.</p> <pre><code>projectsveltos.io/admin-name: &lt;admin&gt;\n</code></pre> <p>Sveltos recommends using the below Kyverno ClusterPolicy, which will ensure adding the label defined to each <code>EventSource</code> during creation time.</p> <pre><code>---\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: add-labels\n  annotations:\n    policies.kyverno.io/title: Add Labels\n    policies.kyverno.io/description: &gt;-\n      Adds projectsveltos.io/admin-name label on each EventSource\n      created by tenant admin. It assumes each tenant admin is\n      represented in the management cluster by a ServiceAccount.\nspec:\n  background: false\n  rules:\n  - exclude:\n      any:\n      - clusterRoles:\n        - cluster-admin\n    match:\n      all:\n      - resources:\n          kinds:\n          - EventSource\n          - EventTrigger\n    mutate:\n      patchStrategicMerge:\n        metadata:\n          labels:\n            +(projectsveltos.io/serviceaccount-name): '{{serviceAccountName}}'\n            +(projectsveltos.io/serviceaccount-namespace): '{{serviceAccountNamespace}}'\n    name: add-labels\n  validationFailureAction: enforce\n</code></pre> <ol> <li> <p>EventTrigger can also reference a ClusterSet to select one or more managed clusters.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/generators/","title":"Event Driven Generators - Project Sveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators"]},{"location":"events/generators/#introduction-to-generators","title":"Introduction to Generators","text":"<p>There are times when the EventTrigger has to pass resource data to the addon controller. For that reason, the ConfigMapGenerator and SecretGenerator resources let us capture data from the resource which triggered an event (only the EventTrigger has access to it), package it into a new <code>ConfigMap/Secret</code>, and pass that resource downstream to the addon controller with the use of a predictable reference.</p> <p>This mechanism bridges the gap between the event context and the addon deployment, enabling event-aware configurations even after the EventTrigger hands off to the addon controller.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators"]},{"location":"events/generators/#why-this-matters","title":"Why This Matters","text":"<p>The EventTrigger can access the triggering resource (e.g. a <code>custom resource</code> or a <code>namespace</code>) while the addon controller does not have access to the event or the resource which caused the EventTrigger to fire.</p> <p>If we want to use data from this resource (e.g. <code>labels</code>, <code>annotations</code>, <code>spec</code> fields), we need to make the data available in a Kubernetes resource that the addon controller can read. That is where the <code>ConfigMapGenerator</code> and <code>SecretGenerator</code> come into play. They materialise resource data into real Kubernetes resources that the ClusterProfile can reference.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators"]},{"location":"events/generators/#how-it-works-step-by-step-guide","title":"How It Works: Step-by-Step Guide","text":"<p>Let's assume the EventTrigger detects an event, like the creation of a custom Kubernetes resource. The EventTrigger uses a <code>ConfigMapGenerator/SecretGenerator</code> to create a new <code>ConfigMap/Secret</code>, embedding selected fields from the triggering resource using templates.</p> <p>Example: Fetch Source ConfigMap</p> <p>First, Sveltos evaluates the <code>name</code> and <code>namespace</code> fields to locate the template <code>ConfigMap</code>.</p> <p>Template</p> <pre><code>configMapGenerator:\n  - name: \"{{ .Resource.metadata.name }}-source-config\" # name can also use cluster data\n    namespace: \"{{ .Resource.metadata.namespace }}\" # namespace can also use cluster data\n    nameFormat: \"{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}-{{ .Resource.metadata.name }}-generated\"\n</code></pre> <p>Event Resource</p> <pre><code>metadata:\n  name: workload\n  namespace: apps\n</code></pre> <p>Retrieved ConfigMap</p> <pre><code>name: workload-source-config\nnamespace: apps\n</code></pre> <p>Resolve nameFormat for Output</p> <p>Sveltos computes the final name for the new <code>ConfigMap</code> using both Cluster and resource fields. If the cluster details are like the example below, then the generated name is <code>prod-team-a-workload-generated</code>.</p> <pre><code>metadata:\n  name: team-a\n  namespace: prod\n</code></pre> <p>Instantiate Content</p> <p>The data section of the fetched <code>ConfigMap</code> (workload-source-config) is treated as a Go template. Sveltos renders it using full access to the information below.</p> <ol> <li>.Cluster fields</li> <li>.Resource fields</li> <li>.MatchingResources metadata</li> </ol> <p>Final ConfigMap</p> <p>The final <code>ConfigMap</code> details are listed below.</p> <ul> <li>Name: prod-team-a-workload-generated</li> <li>Namespace: projectsveltos</li> <li>Content: Rendered using live cluster/resource data</li> </ul> <p>The generated\u00a0<code>ConfigMap</code>\u00a0will be referenced by the auto-generated ClusterProfile, typically via the\u00a0spec.templateResourceRefs, so its content is consumed during the add-on or policy deployment.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators"]},{"location":"events/generators/#next-steps","title":"Next Steps","text":"<p>Continue with an example of how to create a Kubernetes <code>Secret</code> on demand.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators"]},{"location":"events/nats/","title":"Respond to CloudEvents published over NATS and JetStream","text":"<p>NATS is a lightweight, high-performance messaging system designed for speed and scalability.  It excels at simple publish/subscribe communication. JetStream builds upon NATS, adding powerful streaming and data management capabilities like message persistence, flow control, and ordered delivery.  Together, NATS and JetStream provide a robust platform for building modern, distributed systems.</p> <p>CloudEvents is a specification for describing event data in a consistent way, regardless of the underlying system or transport protocol.  It simplifies event handling by providing a common format that different systems can understand, enabling interoperability and reducing complexity.  Think of it as a universal language for events.</p> <p>Sveltos can be configured to connect to and respond to CloudEvents published over NATS and JetStream.</p> <p>Video</p> <p>Learn about the Sveltos and NATS.io integration with a practical example by watching this Youtube Video. If you find this valuable, we would be thrilled if you shared it! \ud83d\ude0a</p>","tags":["Kubernetes","managed services","Sveltos","event driven","NATS","JetStream","CloudEvent"]},{"location":"events/nats/#connect-to-nats-and-jetstream","title":"Connect to NATS and JetStream","text":"<p>To configure Sveltos to connect to NATS and/or JetStream within a managed cluster, create a Secret named <code>sveltos-nats</code> in the <code>projectsveltos</code> namespace.  This Secret's data should contain a key also named <code>sveltos-nats</code> with the connection details.</p> <p>For example, to connect to a NATS server exposed as the <code>nats</code> service in the <code>nats</code> namespace on port 4222, with username/password authentication, and for Sveltos to subscribe to the bar and foo subjects, use the following configuration:</p> <pre><code>{\n  \"nats\":\n   {\n     \"configuration\":\n        {\n            \"url\": \"nats://nats.nats.svc.cluster.local:4222\",\n            \"subjects\": [\n                \"test\",\n                \"foo\"\n            ],\n            \"authorization\": {\n                \"user\": {\n                    \"user\": \"admin\",\n                    \"password\": \"my-password\"\n                }\n            }\n        }\n   }\n}\n</code></pre> <p>then create a Secret with it</p> <pre><code>kubectl create secret generic -n projectsveltos sveltos-nats --from-file=sveltos-nats=nats-configuration\n</code></pre> <p>The sveltos-agent automatically detects and reacts to changes in this Secret. Full NATS/JetStream configuration options can be found at the bottom of this page.</p>","tags":["Kubernetes","managed services","Sveltos","event driven","NATS","JetStream","CloudEvent"]},{"location":"events/nats/#define-an-event","title":"Define an Event","text":"<p>When Sveltos receives a CloudEvent on a subscribed subject, it can trigger a specific operation known as an Event.  Define these Events using the EventSource CRD.</p> <p>For example, the following EventSource defines an Event triggered by any message received on the user-login subject with a CloudEvent source of auth.example.com/user-login:</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: user-login\nspec:\n  messagingMatchCriteria:\n  - subject: \"user-login\"\n    cloudEventSource: \"auth.example.com/user-login\"\n</code></pre> <p>Following criteria can be used to narrow down the events that Sveltos will react to:</p> <ul> <li> <p>Subject: This field filters based on the NATS/JetStream subject the CloudEvent is received on.  It allows you to specify a particular subject or use regular expressions for pattern matching.  If left empty, the EventSource will consider CloudEvents from any of the subjects Sveltos is subscribed to.  This is useful for broadly catching events across various subjects.</p> </li> <li> <p>CloudEventSource: This filters CloudEvents based on the source attribute within the CloudEvent itself.  This allows you to distinguish events originating from different systems or components.  Like Subject, it supports regular expressions for flexible matching.</p> </li> <li> <p>CloudEventType:  This filters CloudEvents based on their type attribute.  The type attribute describes the kind of event that occurred (e.g., com.example.order.created).  Using this filter, you can target specific event types.  Regular expressions are also supported here.</p> </li> <li> <p>CloudEventSubject: This field filters CloudEvents based on the subject attribute within the CloudEvent, which is distinct from the NATS/JetStream subject.  This provides another layer of filtering based on the event's content. Regular expressions are supported.</p> </li> </ul>","tags":["Kubernetes","managed services","Sveltos","event driven","NATS","JetStream","CloudEvent"]},{"location":"events/nats/#define-what-to-do-in-response-to-an-event","title":"Define what to do in response to an Event","text":"<p>EventTrigger is the CRD introduced to define what add-ons to deploy when an event happens.</p> <p>Each EvenTrigger instance:</p> <ol> <li>References an EventSource (which defines what the event is);</li> <li>Has a sourceClusterSelector selecting one or more managed clusters; <sup>2</sup></li> <li>Contains a list of add-ons to deploy</li> </ol> <p>The following example demonstrates how to trigger the creation of a Namespace when a user logs in.  It uses an EventTrigger referencing the <code>user-login</code> EventSource (defined previously) and a ConfigMap template.  Sveltos instantiates the ConfigMap template, creating a new Namespace. The Namespace's name is taken from the CloudEvent's subject, and a label is added based on the message within the CloudEvent's data.</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: create-namespace\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: fv\n  eventSourceName: user-login\n  oneForEvent: true\n  cloudEventAction: Create\n  policyRefs:\n  - name: namespace\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: namespace\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  namespace.yaml: |\n    kind: Namespace\n    apiVersion: v1\n    metadata:\n      name: {{ .CloudEvent.subject }} # .CloudEvent is the triggering CloudEvent\n      labels:\n        organization: {{ ( index .CloudEvent.data `organization` ) }}\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven","NATS","JetStream","CloudEvent"]},{"location":"events/nats/#the-role-of-cloudevent-source-and-subject","title":"The Role of cloudEvent Source and Subject","text":"<p>Because CloudEvents describe events rather than actions, Sveltos uses the combination of cloudEvent Source and cloudEvent Subject to uniquely identify the resource associated with the event. This allows Sveltos to manage the lifecycle of resources even though CloudEvents don't have a built-in \"delete\" operation.</p> <p>Let's say we want to automatically create a Namespace when a user logs in and delete that Namespace when they log off. We can achieve this by publishing CloudEvents to NATS subject user-operation.  Critically, to link login and logout events to the same user, the cloudEvent Source and cloudEvent Subject (which should contain a unique user identifier in this example) must be the same in both the login and logout CloudEvents. We then define following EventSource instance to trigger the appropriate actions.</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: user-operation\nspec:\n  messagingMatchCriteria:\n  - subject: \"user-operation\"\n    cloudEventSource: \"auth.example.com\"\n</code></pre> <p>To complete the setup, we define the EventTrigger resource. The <code>EventTrigger.Spec.CloudEventAction</code> field is defined as a template. When CloudEvent type is com.example.auth.login its instantiated value is <code>Create</code>(causing the namespace to be created anytime a user logins). The instantiated value is otherwise <code>Delete</code>causing the namespace to be deleted upon a user logout.</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: manage-namespace\nspec:\n  eventSourceName: user-operation\n  cloudEventAction:  \"{{ if eq .CloudEvent.type 'auth.example.com.logout' }}Delete{{ else }}Create{{ end }}\" # can be espressed as a template and instantiated using CloudEvent\n  policyRefs:\n  - name: namespace\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: namespace\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok # ConfigMap contains a template and needs to be instantiated at run time\ndata:\n  namespace.yaml: |\n    kind: Namespace\n    apiVersion: v1\n    metadata:\n      name: {{ .CloudEvent.subject }} # The CloudEvent is available and can be used to instantiate the template\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven","NATS","JetStream","CloudEvent"]},{"location":"events/nats/#end-to-end-flow","title":"End to End Flow","text":"<p>Deploy NATS to all production clusters:</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: nats\nspec:\n  clusterSelector:\n    matchLabels:\n      env: production\n  helmCharts:\n  - chartName: nats/nats\n    chartVersion: 1.2.9\n    helmChartAction: Install\n    releaseName: nats\n    releaseNamespace: nats\n    repositoryName: nats\n    repositoryURL: https://nats-io.github.io/k8s/helm/charts/\n    values: |-\n      config:\n        merge:\n          authorization:\n            default_permissions:\n              publish: [\"&gt;\"]\n              subscribe:  [\"&gt;\"]\n            users:\n            - user: \"admin\"\n              password: \"my-password\"\n  syncMode: Continuous\n</code></pre> <p>Once NATS is deployed, create a Secret in each production cluster instructing Sveltos to connect to the NATS server<sup>1</sup>:</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-deploy-sveltos-nats-secret\nspec:\n  dependsOn:\n  - nats\n  clusterSelector:\n    matchLabels:\n      env: production\n  policyRefs:\n  - name: deploy-sveltos-nats-secret\n    namespace: default\n    kind: Secret\n---\napiVersion: v1\ndata:\n  sveltos-nats: YXBpVmVyc2lvbjogdjEKZGF0YToKICBzdmVsdG9zLW5hdHM6IGV3b2dJQ0p1WVhSeklqb0tJQ0FnZXdvZ0lDQWdJQ0pqYjI1bWFXZDFjbUYwYVc5dUlqb0tDWHNLQ1NBZ0lDQWlkWEpzSWpvZ0ltNWhkSE02THk5dVlYUnpMbTVoZEhNdWMzWmpMbU5zZFhOMFpYSXViRzlqWVd3Nk5ESXlNaUlzQ2drZ0lDQWdJbk4xWW1wbFkzUnpJam9nV3dvSkNTSjFjMlZ5TFc5d1pYSmhkR2x2YmlJS0NTQWdJQ0JkTEFvSklDQWdJQ0poZFhSb2IzSnBlbUYwYVc5dUlqb2dld29KQ1NKMWMyVnlJam9nZXdvSkNTQWdJQ0FpZFhObGNpSTZJQ0poWkcxcGJpSXNDZ2tKSUNBZ0lDSndZWE56ZDI5eVpDSTZJQ0p0ZVMxd1lYTnpkMjl5WkNJS0NRbDlDZ2tnSUNBZ2ZRb0pmUW9nSUNCOUNuMEsKa2luZDogU2VjcmV0Cm1ldGFkYXRhOgogIG5hbWU6IHN2ZWx0b3MtbmF0cwogIG5hbWVzcGFjZTogcHJvamVjdHN2ZWx0b3MKdHlwZTogT3BhcXVlCg==\nkind: Secret\nmetadata:\n  name: deploy-sveltos-nats-secret\n  namespace: default\ntype: addons.projectsveltos.io/cluster-profile\n</code></pre> <p>Deploy following Sveltos configuration to create a namespace when a user login:</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: user-operation\nspec:\n  messagingMatchCriteria:\n  - subject: \"user-operation\"\n    cloudEventSource: \"auth.example.com\"\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: manage-namespace\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: production\n  eventSourceName: user-operation\n  oneForEvent: true\n  syncMode: ContinuousWithDriftDetection\n  cloudEventAction: '{{if eq .CloudEvent.type \"auth.example.com.logout\"}}Delete{{else}}Create{{end}}'\n  policyRefs:\n  - name: namespace\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: namespace\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  namespace.yaml: |\n    kind: Namespace\n    apiVersion: v1\n    metadata:\n      name: {{ .CloudEvent.subject }}\n</code></pre> <p>Send now a CloudEvent representing user mgianluc login:</p> <pre><code>CLOUDEVENT_JSON=$(cat &lt;&lt; EOF\n{\n  \"specversion\": \"1.0\",\n  \"type\": \"auth.example.com.login\",\n  \"source\": \"auth.example.com\",\n  \"id\": \"10001\",\n  \"subject\": \"mgianluc\",\n  \"datacontenttype\": \"application/json\",\n  \"data\": {\n    \"message\": \"User mgianluc login\"\n  }\n}\nEOF\n)\n</code></pre> <pre><code>KUBECONFIG=&lt;production cluster kubeconfig&gt; kubectl exec -it deployment/nats-box -n nats -- nats pub user-operation $CLOUDEVENT_JSON --user=admin --password=my-password\n</code></pre> <p>Verify namespace is created:</p> <pre><code>sveltosctl show addons\n+-----------------------------+---------------+----------------+--------------+---------+-------------------------------+--------------------------------------------------+\n|           CLUSTER           | RESOURCE TYPE |   NAMESPACE    |     NAME     | VERSION |             TIME              |                     PROFILES                     |\n+-----------------------------+---------------+----------------+--------------+---------+-------------------------------+--------------------------------------------------+\n| default/clusterapi-workload | helm chart    | nats           | nats         | 1.2.9   | 2025-02-04 14:06:14 +0100 CET | ClusterProfile/nats                              |\n| default/clusterapi-workload | :Secret       | projectsveltos | sveltos-nats | N/A     | 2025-02-04 14:06:36 +0100 CET | ClusterProfile/deploy-deploy-sveltos-nats-secret |\n| default/clusterapi-workload | :Namespace    |                | mgianluc     | N/A     | 2025-02-04 14:12:03 +0100 CET | ClusterProfile/sveltos-gbv99bcdsk1aa04jkdzv      |\n+-----------------------------+---------------+----------------+--------------+---------+-------------------------------+--------------------------------------------------+\n</code></pre> <p>We can now send a CloudEvent for the logout operation (note the CloudEvent type):</p> <pre><code>CLOUDEVENT_JSON=$(cat &lt;&lt; EOF\n{\n  \"specversion\": \"1.0\",\n  \"type\": \"auth.example.com.logout\",\n  \"source\": \"auth.example.com\",\n  \"id\": \"10001\",\n  \"subject\": \"mgianluc\",\n  \"datacontenttype\": \"application/json\",\n  \"data\": {\n    \"message\": \"User mgianluc logout\"\n  }\n}\nEOF\n)\n</code></pre> <pre><code>KUBECONFIG=&lt;production cluster kubeconfig&gt; kubectl exec -it deployment/nats-box -n nats -- nats pub user-operation $CLOUDEVENT_JSON --user=admin --password=my-password\n</code></pre> <p>Verify the namespace has been deleted in response to the user logout CloudEvent:</p> <pre><code>sveltosctl show addons\n+-----------------------------+---------------+----------------+--------------+---------+-------------------------------+--------------------------------------------------+\n|           CLUSTER           | RESOURCE TYPE |   NAMESPACE    |     NAME     | VERSION |             TIME              |                     PROFILES                     |\n+-----------------------------+---------------+----------------+--------------+---------+-------------------------------+--------------------------------------------------+\n| default/clusterapi-workload | helm chart    | nats           | nats         | 1.2.9   | 2025-02-04 14:06:14 +0100 CET | ClusterProfile/nats                              |\n| default/clusterapi-workload | :Secret       | projectsveltos | sveltos-nats | N/A     | 2025-02-04 14:06:36 +0100 CET | ClusterProfile/deploy-deploy-sveltos-nats-secret |\n+-----------------------------+---------------+----------------+--------------+---------+-------------------------------+--------------------------------------------------+\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven","NATS","JetStream","CloudEvent"]},{"location":"events/nats/#nats-and-jetstream-configuration-options","title":"NATS and JetStream Configuration Options","text":"<p>Sveltos expects a JSON configuration to connect to NATS and JetStream. This configuration is unmarshalled into a specific schema, and must exist in a Secret called <code>sveltos-nats</code> in the <code>projectsveltos</code> namespace. This Secret's data must contain a key also named <code>sveltos-nats</code> with the connection details.</p> <p>Three different authentication methods are supported:</p> <ul> <li>User/Password: Specify a username and password for basic authentication.</li> <li>Token: Use a token for authentication.</li> <li>mTLS: Use mutual TLS for secure authentication. This requires a PEM-encoded client certificate and private key, along with an optional PEM-encoded root CA certificate.</li> </ul> <p>To connect to NATS or JetStream, Sveltos expects a JSON configuration that conforms to the following format, viewable here as Golang structs or as JSON Schema.</p> <p>Note</p> <p>Fields listed in the Golang structs as <code>[]byte</code> will be automatically converted from strings in the JSON configuration.</p> GolangJSON Schema <pre><code>type messagingUser struct {\n    User     string `json:\"user\"`\n    Password string `json:\"password\"`\n}\n\ntype clientCert struct {\n    CertPem []byte `json:\"certPem\"`\n    KeyPem  []byte `json:\"keyPem\"`\n}\n\ntype messagingAuthorization struct {\n    User       *messagingUser `json:\"user,omitempty\"`\n    Token      *string        `json:\"token,omitempty\"`\n    ClientCert *clientCert    `json:\"clientCert,omitempty\"`\n    RootCA     []byte         `json:\"rootCA,omitempty\"`\n}\n\ntype configuration struct {\n    URL           string                 `json:\"url\"`\n    Subjects      []string               `json:\"subjects\"`\n    Authorization messagingAuthorization `json:\"authorization,omitempty\"`\n}\n\ntype stream struct {\n    Name     string   `json:\"name\"`\n    Subjects []string `json:\"subjects\"`\n}\n\ntype consumerConfiguration struct {\n    URL           string                 `json:\"url\"`\n    Streams       []stream               `json:\"streams\"`\n    Authorization messagingAuthorization `json:\"authorization,omitempty\"`\n}\n\ntype natsConfiguration struct {\n    Configuration configuration `json:\"configuration\"`\n}\n\ntype jetstreamConfiguration struct {\n    ConsumerConfiguration consumerConfiguration `json:\"configuration\"`\n}\n\ntype messagingConfig struct {\n    Nats      *natsConfiguration      `json:\"nats,omitempty\"`\n    Jetstream *jetstreamConfiguration `json:\"jetstream,omitempty\"`\n}\n</code></pre> <pre><code>{\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\": \"Sveltos NATS and JetStream Messaging Configuration\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"nats\": {\n            \"$ref\": \"#/definitions/natsConfiguration\"\n        },\n        \"jetstream\": {\n            \"$ref\": \"#/definitions/jetstreamConfiguration\"\n        }\n    },\n    \"definitions\": {\n        \"natsConfiguration\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"configuration\": {\n                    \"$ref\": \"#/definitions/configuration\"\n                }\n            },\n            \"required\": [\"configuration\"]\n        },\n        \"jetstreamConfiguration\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"configuration\": {\n                    \"$ref\": \"#/definitions/consumerConfiguration\"\n                }\n            },\n            \"required\": [\"configuration\"]\n        },\n        \"configuration\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"url\": {\n                    \"type\": \"string\",\n                    \"format\": \"uri\",\n                    \"description\": \"NATS server URL\"\n                },\n                \"subjects\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\"\n                    },\n                    \"description\": \"List of NATS subjects to subscribe/publish to\"\n                },\n                \"authorization\": {\n                    \"$ref\": \"#/definitions/messagingAuthorization\"\n                }\n            },\n            \"required\": [\"url\", \"subjects\"]\n        },\n        \"consumerConfiguration\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"url\": { \"type\": \"string\" },\n                \"streams\": {\n                    \"type\": \"array\",\n                    \"items\": { \"$ref\": \"#/definitions/stream\" }\n                },\n                \"authorization\": {\n                    \"$ref\": \"#/definitions/messagingAuthorization\"\n                }\n            },\n            \"required\": [\"url\", \"streams\"]\n        },\n        \"stream\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": { \"type\": \"string\" },\n                \"subjects\": {\n                    \"type\": \"array\",\n                    \"items\": { \"type\": \"string\" }\n                }\n            },\n            \"required\": [\"name\", \"subjects\"]\n        },\n        \"messagingAuthorization\": {\n            \"type\": \"object\",\n            \"description\": \"Optional authentication settings\",\n            \"properties\": {\n                \"user\": { \"$ref\": \"#/definitions/messagingUser\" },\n                \"token\": { \"type\": \"string\" },\n                \"clientCert\": { \"$ref\": \"#/definitions/clientCert\" },\n                \"rootCA\": {\n                    \"type\": \"string\",\n                    \"description\": \"PEM-encoded root certificate authority\"\n                }\n            }\n        },\n        \"messagingUser\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"user\": { \"type\": \"string\" },\n                \"password\": { \"type\": \"string\" }\n            },\n            \"required\": [\"user\", \"password\"]\n        },\n        \"clientCert\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"certPem\": {\n                    \"type\": \"string\",\n                    \"description\": \"PEM-encoded certificate\"\n                },\n                \"keyPem\": {\n                    \"type\": \"string\",\n                    \"description\": \"PEM-encoded private key\"\n                }\n            },\n            \"required\": [\"certPem\", \"keyPem\"]\n        }\n    }\n}\n</code></pre> <ol> <li> <p>Secret contains following configuration: <pre><code>{\n  \"nats\":\n   {\n     \"configuration\":\n        {\n            \"url\": \"nats://nats.nats.svc.cluster.local:4222\",\n            \"subjects\": [\n                \"user-operation\"\n            ],\n            \"authorization\": {\n                \"user\": {\n                    \"user\": \"admin\",\n                    \"password\": \"my-password\"\n                }\n            }\n        }\n   }\n}\n</code></pre> Or, for a JetStream connection: <pre><code>{\n  \"nats\":\n   {\n     \"configuration\":\n        {\n            \"url\": \"nats://nats.nats.svc.cluster.local:4222\",\n            \"streams\": [\n              {\n                \"name\": \"USERS\",\n                \"subjects\": [\n                    \"user-operation\"\n                ]\n              }\n            ],\n            \"authorization\": {\n                \"user\": {\n                    \"user\": \"admin\",\n                    \"password\": \"my-password\"\n                }\n            }\n        }\n   }\n}\n</code></pre> \u21a9</p> </li> <li> <p>EventTrigger can also reference a ClusterSet to select one or more managed clusters.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","managed services","Sveltos","event driven","NATS","JetStream","CloudEvent"]},{"location":"events/templating/","title":"Events Templating - Project Sveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#introduction-to-events-and-templating","title":"Introduction to Events and Templating","text":"<p>EventTrigger is a Kubernetes resource that enables dynamic, event-driven configuration management across clusters by automatically creating Sveltos ClusterProfiles in response to specific events.</p> <p>When an event matches all conditions defined in an EventTrigger, it generates a tailored Sveltos ClusterProfile, automating the application of add-ons, policies, or configurations based on real-time changes in an environment.</p> <p>The resulting Sveltos ClusterProfile can include:</p> <ol> <li>TemplateResourceRefs</li> <li>PolicyRefs</li> <li>HelmCharts</li> <li>KustomizationRefs</li> </ol> <p>Note</p> <p>This template functions are available.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#instantiation-flow-templateresourcerefs","title":"Instantiation Flow: TemplateResourceRefs","text":"<p>The EventTrigger TemplateResourceRefs is instantiated using resource data and set to ClusterProfile.Spec.TemplateResourceRefs.</p> <p>The <code>namespace</code> and <code>name</code> fields within each reference can be constants or templates. The templates are dynamically evaluated using data from the triggering cluster or resource, allowing the generated ClusterProfile to be context-aware and tailored to a specific event.</p> <p>Usage Example:</p> <ul> <li><code>{{ .Cluster.metadata.name }}</code></li> <li><code>{{ .Resource.metadata.annotations.env }}</code></li> <li><code>{{ printf \"%s-%s\" .Cluster.metadata.labels.region .Resource.metadata.name }}</code></li> </ul> <p>Note</p> <p>The templates are resolved at runtime. It allows systems to generate tailored Sveltos ClusterProfiles based on the specific context of each event.</p> <p>Example: EventTrigger.spec.templateResourceRefs</p> <p>EventTrigger.spec.templateResourceRefs Details</p> <pre><code>templateResourceRefs:\n  - kind: ConfigMap\n    namespace: \"platform-config\"\n    name: \"{{ .Cluster.metadata.labels.region }}-{{ .Resource.metadata.annotations.env }}\"\n</code></pre> <p>Triggering Cluster Details</p> <pre><code>metadata:\n  name: cluster-west-1\n  labels:\n    region: us-west\n</code></pre> <p>Resource that Generated Event Details</p> <pre><code>metadata:\n  name: nginx-addon\n  annotations:\n    env: prod\n</code></pre> <p>Resulting ClusterProfile.spec.templateResourceRefs Details</p> <pre><code>templateResourceRefs:\n  - kind: ConfigMap\n    namespace: \"platform-config\"\n    name: \"us-west-prod\"\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#instantiation-flow-policyrefs","title":"Instantiation Flow: PolicyRefs","text":"<p>EventTrigger can reference <code>ConfigMaps</code> and <code>Secrets</code> in the policyRefs section.</p> <p>Users can express <code>ConfigMaps/Secret</code> resources as templates. These are dynamically evaluated using the data from the triggering cluster and event resource.</p> <p>Example: Dynamic policyRefs</p> <p>We can express the <code>policyRefs</code> section as a template like the example below.</p> <pre><code>policyRefs:\n  - kind: ConfigMap\n    namespace: \"{{ .Cluster.metadata.labels.region }}\"\n    name: \"{{ .Resource.metadata.name }}-config\"\n</code></pre> <p>The template values will be first instantiated based on the data from the triggering cluster and resource. For example, if the triggering cluster contains the label <code>region=eu-central</code> and a resource is named <code>ingress-addon</code>, the final <code>policyRefs</code> section will look like the below.</p> <pre><code>policyRefs:\n  - kind: ConfigMap\n    namespace: eu-central\n    name: ingress-addon-config\n</code></pre> <p>After instantiation, EventTrigger fetches the corresponding <code>ConfigMap/Secret</code> based on the dynamically instantiated <code>namespace</code> and <code>name</code>.</p> <p>Once fetched, Sveltos handles ConfigMap and Secret resources in two distinct ways, depending on specific annotations:</p> <ol> <li>If the resource does not have the annotation projectsveltos.io/instantiate set, the generated Sveltos ClusterProfile will directly reference the same fetched <code>ConfigMap/Secret</code> resource. If the annotation projectsveltos.io/template is set, the Sveltos addon controller will first instantiate the resource (meaning it will process any templates within it) before deploying it to any matching clusters.</li> <li>If a resource has the projectsveltos.io/instantiate annotation, the EventTrigger component will be responsible for creating a new ConfigMap or Secret. It will use the event resource's data along with information from the managed cluster where the event occurred. The resulting Sveltos ClusterProfile will then reference this newly instantiated <code>ConfigMap/ Secret</code>.</li> </ol> <p>Example: EventTrigger.spec.policyRefs</p> <p>EventTrigger.spec.policyRefs Template Details</p> <pre><code>policyRefs:\n  - kind: ConfigMap\n    namespace: \"{{ .Cluster.metadata.labels.region }}\"\n    name: \"{{ .Resource.metadata.name }}-config\"\n</code></pre> <p>Triggering Cluster Details</p> <pre><code>metadata:\n  labels:\n    region: eu-central\n</code></pre> <p>Resource that Generated Event Details</p> <pre><code>metadata:\n  name: ingress-addon\n</code></pre> <p>Instantiated Namespace and Name Details</p> <pre><code>namespace: eu-central\nname: ingress-addon-config\n</code></pre> <p>Sveltos fetches the ConfigMap at <code>eu-central/ingress-addon-config</code> and processes it based on its annotations.</p> <ul> <li>If <code>projectsveltos.io/instantiate</code> is absent, the Sveltos ClusterProfile spec.profileRefs will reference this one.</li> <li>If <code>projectsveltos.io/instantiate</code> is present, the <code>ConfigMap</code> is instantiated by the EventTrigger using the cluster/event data, and the Sveltos ClusterProfile references this new resource.</li> </ul> <pre><code>    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: network-policy\n      namespace: default\n      annotations:\n        projectsveltos.io/instantiate: ok # this annotation is what tells Sveltos to instantiate this ConfigMap in the event context\n    data:\n      networkpolicy.yaml: |\n        kind: NetworkPolicy\n        apiVersion: networking.k8s.io/v1\n        metadata:\n          name: front-{{ .Resource.metadata.name }}\n          namespace: {{ .Cluster.metadata.namespace }}\n        spec:\n          podSelector:\n            matchLabels:\n              {{ range $key, $value := .Resource.spec.selector }}\n              {{ $key }}: {{ $value }}\n              {{ end }}\n          ingress:\n            - from:\n              - podSelector:\n                  matchLabels:\n                    app: internal\n              ports:\n                {{ range $port := .Resource.spec.ports }}\n                - port: {{ $port.port }}\n                {{ end }}\n</code></pre> <p>The approach enables context-aware policy injection based on dynamically generated resource names, enhancing automation and flexibility in multi-cluster operations.</p> <p></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#instantiation-flow-helm-charts","title":"Instantiation Flow: Helm Charts","text":"<p>In EventTrigger, the <code>spec.helmCharts</code> field defines which Helm charts should be applied when an event occurs. Each Helm chart entry can be defined in two ways.</p> <ol> <li>Constant: A fixed chart definition (e.g., chart name, repo, values, version).</li> <li>Template: A dynamic chart definition using Go templates, which are instantiated using data from the triggering cluster and/or event resource.</li> </ol> <p>When Helm chart fields (such as name, version, or values) are defined as templates, EventTrigger evaluates them at runtime using the cluster and resource metadata that triggered the event. The resulting Helm chart specification is included in the generated Sveltos ClusterProfile, which uses these instantiated Helm chart definitions.</p> <p>Example: EventTrigger.spec.helmCharts</p> <p>EventTrigger.spec.helmCharts Details</p> <pre><code>helmCharts:\n  - releaseName: my-namespace\n    chartName: example\n    repositoryURL: https://charts.example.com\n    version: \"1.0.0\"\n    values: |\n      env: \"{{ .Resource.metadata.annotations.env }}\"\n      region: \"{{ .Cluster.metadata.labels.region }}\"\n</code></pre> <p>Triggering Cluster Details</p> <pre><code>metadata:\n  name: cluster-east\n  labels:\n    region: us-east\n</code></pre> <p>Resource that Generated Event Details</p> <pre><code>metadata:\n  name: nginx-deployment\n  annotations:\n    env: staging\n</code></pre> <p>Resulting ClusterProfile.spec.helmCharts Details</p> <pre><code>helmCharts:\n  - releaseName: my-namespace\n    chartName: example\n    repositoryURL: https://charts.example.com\n    version: \"1.0.0\"\n    values: |\n      env: staging\n      region: us-east\n</code></pre> <p>The templating capability makes Helm chart deployments flexible and context-aware, enabling dynamic customisation for different clusters or triggering resources, all managed declaratively.</p> <p>Additionally, each Helm chart defined in EventTrigger.spec.helmCharts can use the valuesFrom field to reference external <code>ConfigMaps/Secrets</code> that contain Helm values. These references support templated namespace and name fields, just like in <code>policyRefs</code>.</p> <p>Example: Dynamic EventTrigger.spec.helmCharts.valuesFrom</p> <p>The <code>namespace</code> and <code>name</code> of the referenced <code>ConfigMap/Secret</code> in valuesFrom are first instantiated using templates. These templates can access any field from the triggering cluster or resource.</p> <pre><code>valuesFrom:\n  kind: ConfigMap\n  namespace: \"{{ .Cluster.metadata.labels.region }}\"\n  name: \"{{ .Resource.metadata.annotations.env }}-helm-values\"\n</code></pre> <p>After instantiation, the referenced <code>ConfigMap/Secret</code> is fetched.</p> <ul> <li>If the resource does not have the <code>projectsveltos.io/instantiate</code> annotation, the generated <code>ClusterProfile.spec.helmCharts.valuesFrom</code> will reference the same <code>ConfigMap/Secret</code>.</li> <li>If the resource has the <code>projectsveltos.io/instantiate</code> annotation, Sveltos instantiates the content of the <code>ConfigMap/Secret</code> using the cluster and event data. A new <code>ConfigMap/Secret</code> is generated with the instantiated value. The generated <code>ClusterProfile.spec.helmCharts.valuesFrom</code> will reference the newly created <code>ConfigMap/Secret</code>.</li> </ul> <p>The same logic applies to EventTrigger.spec.kustomizationRefs\u2014fields.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#instantiation-flow-clusterprofile-name","title":"Instantiation Flow: ClusterProfile Name","text":"<p>By default, the ClusterProfile instances created by the event framework are assigned random names. While this is acceptable for most use cases, a predictable name is required in scenarios where other resources must be set as dependent on the instantiated ClusterProfile. The random naming convention makes it impossible to reference these instances programmatically.</p> <p>To address this challenge, the EventTrigger spec includes an optional field: <code>InstantiatedProfileNameFormat</code>. This field allows for the definition of a naming template that ensures a predictable name is generated for the ClusterProfile instance. The name is consistently formatted based on a Go template and can leverage data from the cluster and the specific event that triggered the creation.</p> <p>In the example below, the template uses the cluster name and the name of the resource that triggered the event.</p> <p><code>{{ .Cluster.metadata.name }}-{{ .Resource.metadata.name }}-test</code></p> <p>When an event is triggered, Sveltos will automatically apply this template. For example, if the event occurs in a cluster named cluster-alpha and is triggered by a resource named pod-nginx, the resulting ClusterProfile will be named: cluster-alpha-pod-nginx-test.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#template-functions","title":"Template Functions","text":"<p>Sveltos supports the template functions included from the Sprig open source project. The Sprig library provides over 70 template functions for Go\u2019s template language. Some of the functions are listed below. For the full list, have a look at the Spring Github page.</p> <ol> <li>String Functions: trim, wrap, randAlpha, plural, etc.</li> <li>String List Functions: splitList, sortAlpha, etc.</li> <li>Integer Math Functions: add, max, mul, etc.</li> <li>Integer Slice Functions: until, untilStep</li> <li>Float Math Functions: addf, maxf, mulf, etc.</li> <li>Date Functions: now, date, etc.</li> <li>Defaults Functions: default, empty, coalesce, fromJson, toJson, toPrettyJson, toRawJson, ternary</li> <li>Encoding Functions: b64enc, b64dec, etc.</li> <li>Lists and List Functions: list, first, uniq, etc.</li> <li>Dictionaries and Dict Functions: get, set, dict, hasKey, pluck, dig, deepCopy, etc.</li> <li>Type Conversion Functions: atoi, int64, toString, etc.</li> <li>Path and Filepath Functions: base, dir, ext, clean, isAbs, osBase, osDir, osExt, osClean, osIsAbs</li> <li>Flow Control Functions: fail</li> </ol> <p>Sveltos includes a dedicated set of functions for manipulating the resources that trigger events. These functions are designed to make it easy to work with Kubernetes resource data directly within your templates.</p> <ol> <li>getResource: Takes the resource that generated the event and returns a map[string]interface{} allowing to access any field of the resource. Following fields are automatically cleared: managedFields, resourceVersion and uid.</li> <li>copy: Creates a copy of the resource that generated the event.</li> <li>setField: Takes the resource that generated the event, the field name, and a new value. It returns a modified copy of the resource with the specified field updated.</li> <li>removeField: Takes the resource that generated the event and the field name. Returns a modified copy of the resource with the specified field removed.</li> <li>getField: Takes the resource that generated the event and the field name. Returns the field value</li> <li>chainSetField: This function acts as an extension of setField. It allows for chaining multiple field updates.</li> <li>chainRemoveField: Similar to chainSetField, this function allows for chaining multiple field removals.</li> </ol> <p>Note</p> <p>These functions operate on copies of the original resource, ensuring the original data remains untouched.</p> <p>Here are some examples:</p> <pre><code>  # Use getResource to retrieve the triggering resource and store it in a temporary variable.\n  # This variable will be used as the starting point for all subsequent modifications.\n  # Fields managedFields, resourceVersion and uid are automatically cleared\n  {{ $resource := getResource .Resource }}\n\n  # Use chainSetField to modify the 'metadata.name' field.\n  # This function returns the modified resource, allowing it to be chained with the next function.\n  {{ $resource := chainSetField $resource \"metadata.name\" \"new-name\" }}\n\n  # Use chainRemoveField to remove the 'data' field.\n  # The previous changes are preserved as the function operates on the modified resource.\n  {{ $resource := chainRemoveField $resource \"data\" }}\n\n  # Finally, use the toYaml function to output the final, modified resource.\n  # This will be the resource that is applied to the managed cluster.\n  {{ toYaml $resource }}\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#patch-resources","title":"Patch Resources","text":"<p>The EventSource acts as a listener. It continuously monitors the managed cluster for any Service resource. The <code>evaluateCEL</code> rule is used to filter these Services. When a Service is created, updated, or deleted with the sveltos: fv label, it generates an event. The <code>collectResources: true</code> setting ensures that the full YAML of the Service resource is included in the event, which is essential for the copy template function used later.</p> <p>The <code>EventTrigger</code> defines a chain of actions in response to this event:</p> <ul> <li>It creates a ConfigMap containing a copy of the Service that triggered the event.</li> <li>It creates a ClusterProfile that references this new ConfigMap. The ClusterProfile then applies the Service from the ConfigMap, patching it to add two labels, before deploying the modified Service back to the same cluster where the event originated.</li> </ul> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: detect-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    evaluateCEL:\n    - name: service_with_label_sveltos_fv\n      rule: has(resource.metadata.labels) &amp;&amp; has(resource.metadata.labels.sveltos) &amp;&amp; resource.metadata.labels.sveltos == \"fv\"\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: patch-service\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: fv\n  eventSourceName: detect-service\n  oneForEvent: true\n  policyRefs:\n  - name: copy-service\n    namespace: default\n    kind: ConfigMap\n  patches:\n  - target:\n      group: \"\"\n      version: v1\n      kind: Service\n      name: \".*\"\n    patch: |\n            - op: add\n              path: /metadata/labels/mirror.linkerd.io~1exported\n              value: \"true\"\n            - op: add\n              path: /metadata/labels/mirror.linkerd.io~1federated\n              value: member\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: copy-service\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  copy.yaml: |\n    {{ copy .Resource }}\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#benefits","title":"Benefits","text":"<p>The EvenTrigger has access to the resource data and can use them to instantiate <code>namespace/name</code> of the <code>TemplateResourceRefs</code> field and the <code>ConfigMap/Secret</code> of the <code>policyRefs</code> field.</p> <p>Once the EventTrigger is done creating the Sveltos ClusterProfile, the addon controller will take over and deploy it to the matching cluster(s). The addon controller does not have any access to the resource (only the EventTrigger has access to the resource). However, it can fetch any resource present in the management cluster which is defined in the <code>TemplateResourceRefs</code> field.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/templating/#next-steps","title":"Next Steps","text":"<p>Continue with the Event Generators section located here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Events","Templating"]},{"location":"events/examples/api_gateway_contour/","title":"Example API Gateway Contour - Project Sveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/api_gateway_contour/#example-api-gateway-with-contour-create-one-httproute-instance-per-event","title":"Example: API Gateway with Contour: Create one HTTPRoute instance per event","text":"<p>We already covered here how to deploy L4 and L7 routing on multiple Kubernetes clusters securely and programmatically with Sveltos.</p> <p>With the event driven framework, we are taking a step forward: programmatically generate/update HTTPRoutes:</p> <ol> <li>Define a Sveltos Event as creation/deletion of specific Service instances (in our example, the Service instances we are interested in are in the namespace eng and are exposing port 80);</li> <li>Define what add-ons to deploy in response to such events: an HTTPRoute instance defined as a template. Sveltos will instantiate this template using information from Services in the managed clusters that are part of the event defined in #1</li> </ol> <p>Example - EventSource Definition</p> <pre><code>cat &gt; eventsource.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: eng-http-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    namespace: eng\n    evaluate: |\n      function evaluate()\n        hs = {}\n        hs.matching = false\n        if obj.spec.ports ~= nil then\n          for _,p in pairs(obj.spec.ports) do\n            if p.port == 80 then\n              hs.matching = true\n            end\n          end\n        end\n        return hs\n      end\nEOF\n</code></pre> <p>Example - EventTrigger Definition</p> <pre><code>cat &gt; eventtrigger.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: service-network-policy\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: fv\n  eventSourceName: eng-http-service\n  oneForEvent: true\n  policyRefs:\n  ...\n  - name: http-routes\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: http-routes\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  http-route.yaml: |\n    kind: HTTPRoute\n    apiVersion: gateway.networking.k8s.io/v1beta1\n    metadata:\n      name: {{ .Resource.metadata.name }}\n      namespace: {{ .Resource.metadata.namespace }}\n      labels:\n        {{ range $key, $value := .Resource.spec.selector }}\n        {{ $key }}: {{ $value }}\n        {{ end }}\n    spec:\n      parentRefs:\n      - group: gateway.networking.k8s.io\n        kind: Gateway\n        name: contour\n        namespace: projectcontour\n      hostnames:\n      - \"local.projectcontour.io\"\n      rules:\n      - matches:\n        - path:\n            type: PathPrefix\n            value: /{{ .Resource.metadata.name }}\n        backendRefs:\n        - kind: Service\n          name: {{ .Resource.metadata.name }}\n          port: {{ (index .Resource.spec.ports 0).port }}\nEOF\n</code></pre> <p>Anytime a Service exposing port 80 is created in any matching cluster and in the namespace <code>eng</code>, an HTTPRoute instance will get deployed.</p> <p>Full example definitions (with all YAMLs) can be found here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/cluster_inventory/","title":"Example - Cluster Inventory via ClusterProfile API (Revised)","text":"","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cluster_inventory/#overview","title":"Overview","text":"<p>KEP-4322 (Cluster Inventory / ClusterProfile API) from SIG Multi-Cluster proposes a <code>ClusterProfile</code> API to represent a standard inventory of clusters. See the proposal: KEP-4322: Cluster Inventory.</p> <ul> <li>Cluster Inventory: API-driven list of clusters that tools can discover and act on.</li> <li>ClusterProfile: A CRD that represents a single cluster (namespaced, identity, properties, status).</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cluster_inventory/#problem","title":"Problem","text":"<p>Sveltos does not natively support SIG Multi-Cluster ClusterProfile.</p>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cluster_inventory/#solution","title":"Solution","text":"<p>Use Sveltos\u2019 Event Framework to:</p> <ol> <li>Watch <code>ClusterProfile</code> objects (<code>multicluster.x-k8s.io/v1alpha1</code>) via <code>EventSource detect-cluster-inventory-api-cluster-profiles</code>.</li> <li>For each, trigger <code>EventTrigger register-cluster</code> to create a matching <code>SveltosCluster</code>.</li> <li>Watch kubeconfig Secrets labeled with <code>x-k8s.io/cluster-profile</code> via <code>EventSource detect-config-secret</code>.</li> <li>For each, trigger <code>EventTrigger update-sveltoscluster</code> to create the Secret layout expected by <code>SveltosCluster</code>.</li> </ol>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cluster_inventory/#prerequisites","title":"Prerequisites","text":"<ul> <li>Sveltos (including the Event Framework) installed on the management cluster.</li> <li>The management cluster labeled so Sveltos can target it (examples use <code>env: management</code>).</li> <li>ClusterProfile CRDs present (<code>apiVersion: multicluster.x-k8s.io/v1alpha1</code>).</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cluster_inventory/#architecture","title":"Architecture","text":"<p>In the management cluster, Sveltos and its Event Framework run the following components:</p> <ul> <li>EventSource: <code>detect-cluster-inventory-api-cluster-profiles</code>   Watches all <code>ClusterProfile</code> objects.</li> <li>EventTrigger: <code>register-cluster</code>   Instantiates a <code>SveltosCluster</code> resource from each <code>ClusterProfile</code> (same name/namespace, labels copied).</li> <li>EventSource: <code>detect-config-secret</code>   Watches kubeconfig <code>Secret</code> objects that carry the label <code>x-k8s.io/cluster-profile</code>.</li> <li>EventTrigger: <code>update-sveltoscluster</code>   Creates the Secret in the format expected by the corresponding <code>SveltosCluster</code>.</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cluster_inventory/#step-1-detect-clusterprofiles-and-create-sveltosclusters","title":"Step 1: Detect ClusterProfiles and create SveltosClusters","text":"<p>Define <code>EventSource detect-cluster-inventory-api-cluster-profiles</code> to select every <code>ClusterProfile</code>. Then define <code>EventTrigger register-cluster</code> that instantiates a <code>SveltosCluster</code> with matching metadata.</p> <p>EventSource (detect ClusterProfiles)</p> <pre><code>cat &gt; eventsource-clusterprofiles.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: detect-cluster-inventory-api-cluster-profiles\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"multicluster.x-k8s.io\"\n    version: \"v1alpha1\"\n    kind: \"ClusterProfile\"\nEOF\n</code></pre> <p>EventTrigger (create SveltosClusters)</p> <pre><code>cat &gt; eventtrigger-create-sveltosclusters.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: register-cluster\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: management\n  eventSourceName: detect-cluster-inventory-api-cluster-profiles\n  oneForEvent: true\n  policyRefs:\n  - name: sveltoscluster-metadata\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: sveltoscluster-metadata\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  sveltos-cluster.yaml: |\n    apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    metadata:\n      name: {{ .Resource.metadata.name }}\n      namespace: {{ .Resource.metadata.namespace }}\n      {{- with .Resource.metadata.labels }}\n      labels:\n        {{- range $key, $value := . }}\n        {{ $key }}: {{ $value }}\n        {{- end }}\n      {{- end }}\n    spec:\n      kubeconfigKeyName: config\nEOF\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cluster_inventory/#step-2-detect-kubeconfig-secrets-and-create-expected-secret","title":"Step 2: Detect kubeconfig Secrets and create expected Secret","text":"<p>Define <code>EventSource detect-config-secret</code> to detect <code>Secret</code> objects labeled with <code>x-k8s.io/cluster-profile</code>. Then define <code>EventTrigger update-sveltoscluster</code> to generate the Secret consumed by the corresponding <code>SveltosCluster</code>.</p> <p>EventSource (detect kubeconfig Secrets)</p> <pre><code>cat &gt; eventsource-kubeconfig-secrets.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: detect-config-secret\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Secret\"\n    labelFilters:\n    - key: \"x-k8s.io/cluster-profile\"\n      operation: Has\nEOF\n</code></pre> <p>EventTrigger (create kubeconfig Secret)</p> <pre><code>cat &gt; eventtrigger-create-kubeconfig-secret.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: update-sveltoscluster\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: management\n  eventSourceName: detect-config-secret\n  oneForEvent: true\n  policyRefs:\n  - name: sveltoscluster-spec\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: sveltoscluster-spec\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  sveltos-cluster.yaml: |\n    {{ $value := (index .Resource.metadata.labels `x-k8s.io/cluster-profile`) }}\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: {{ $value }}-sveltos-kubeconfig\n      namespace: {{ .Resource.metadata.namespace }}\n    data:\n      {{ range $key, $value := .Resource.data }}\n        {{ $key }}: {{ $value }}\n      {{end}}\nEOF\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cluster_inventory/#result","title":"Result","text":"<ul> <li><code>ClusterProfile</code> resources become discoverable inventory items, materialized as <code>SveltosCluster</code> resources with copied labels.</li> <li>Kubeconfig Secrets labeled for a <code>ClusterProfile</code> are mirrored into the format expected by <code>SveltosCluster</code>.</li> </ul> <p>This bridges KEP-4322\u2019s <code>ClusterProfile</code> API with Sveltos\u2019 native model using the event-driven framework\u2014no changes required in Sveltos core.</p>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","ClusterProfile","KEP-4322"]},{"location":"events/examples/cross_cluster_configuration/","title":"Event Driven Addon Distribution - Project Sveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven","cross cluster configuration"]},{"location":"events/examples/cross_cluster_configuration/#introduction-to-event-driven-addon-distrubution","title":"Introduction to Event Driven Addon Distrubution","text":"<p>Sveltos by default will deploy add-ons in the same way an event is detected. Sveltos can be configured for cross-cluster configuration. That means, it will watch for events in a cluster and deploy add-ons in a set of different clusters.</p> <p>Sveltos also provides two optional fields within the EventTrigger CRD to customize add-on deployment:</p> <ol> <li>destinationClusterSelector</li> </ol> <p>This field is a Kubernetes label selector (clusterSelector). If you set this field, Sveltos' behavior changes. When an event is detected in a cluster, add-ons will be deployed to all clusters that match the specified label selector, rather than just the source cluster.</p> <ol> <li>destinationCluster</li> </ol> <p>This field, a corev1.ObjectReference, allows you to specify a single, specific destination cluster for add-on deployment. This is particularly useful when an event in one cluster (e.g., a management cluster) needs to trigger resource deployment in a designated, different cluster.</p> <p>Templating can be used with destinationCluster to dynamically determine the target cluster based on event data. Sveltos will instantiate the template (using data from both the detected resource and the source cluster) and then set the ClusterProfile.Spec.ClusterRefs accordingly.</p> <p>Example:</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: service-network-policy\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      type: mgmt\n  destinationCluster:\n    name: \"{{ .Resource.metadata.name }}\"\n    namespace: \"{{ .Resource.metadata.namespace }}\"\n    kind: SveltosCluster\n    apiVersion: lib.projectsveltos.io/v1beta1\n</code></pre> <p>In this example, when an event occurs in a cluster matching type: mgmt, the add-ons will be deployed to a SveltosCluster whose name and namespace are dynamically derived from the detected resource. This would result in a ClusterProfile similar to this:</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  ...\nspec:\n  clusterRefs:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: my-service # Dynamically set based on the resource name\n    namespace: default # Dynamically set based on the resource namespace\n</code></pre> <p>These new options provide greater flexibility in designing your Sveltos event-driven add-on deployments, allowing you to precisely target where resources are provisioned.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven","cross cluster configuration"]},{"location":"events/examples/cross_cluster_configuration/#example-cross-cluster-service-discovery","title":"Example: Cross Cluster Service Discovery","text":"<p>To understand the concept mentioned above, let's have a look at a cross-cluster service discovery example.</p> <p>Two clusters with the description below are defined.</p> <ol> <li>GKE cluster (labels env: production) registered with sveltos;</li> <li>A cluster-api cluster (label dep: eng) provisioned by docker.</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven","cross cluster configuration"]},{"location":"events/examples/cross_cluster_configuration/#management-cluster","title":"Management Cluster","text":"<ol> <li>An EventSource instance that matches any Service with a load balancer IP</li> </ol> <p>Example - EventSource Definition</p> <pre><code>cat &gt; eventsource.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: load-balancer-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n  evaluate: |\n    function evaluate()\n      hs = {}\n      hs.matching = false\n      hs.message = \"\"\n      if obj.status.loadBalancer.ingress ~= nil then\n        hs.matching = true\n      end\n      return hs\n    end\nEOF\n</code></pre> <ol> <li>An EventTrigger instance that references the EventSource defined above. It deploys the selector-less Service and corresponding Endpoints in any cluster matching destinationClusterSelector.</li> </ol> <p>Example - EventTrigger Definition</p> <pre><code>cat &gt; eventtrigger.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta11\nkind: EventTrigger\nmetadata:\n  name: service-policy\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: production\n  destinationClusterSelector:\n    matchLabels:\n      dep: eng\n  eventSourceName: load-balancer-service\n  oneForEvent: true\n  policyRefs:\n  - name: service-policy\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: service-policy\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  service.yaml: |\n    kind: Service\n    apiVersion: v1\n    metadata:\n      name: external-{{ .Resource.metadata.name }}\n      namespace: external\n    spec:\n      selector: {}\n      ports:\n        {{ range $port := .Resource.spec.ports }}\n        - port: {{ $port.port }}\n          protocol: {{ $port.protocol }}\n          targetPort: {{ $port.targetPort }}\n        {{ end }}\n  endpoint.yaml: |\n    kind: Endpoints\n    apiVersion: v1\n    metadata:\n      name: external-{{ .Resource.metadata.name }}\n      namespace: external\n    subsets:\n    - addresses:\n      - ip: {{ (index .Resource.status.loadBalancer.ingress 0).ip }}\n      ports:\n        {{ range $port := .Resource.spec.ports }}\n        - port: {{ $port.port }}\n        {{ end }}\nEOF\n</code></pre> <p>As mentioned above, we pass Sveltos a selector-less Service and we then specify our own Endpoints.</p> <p>The Service and Endpoints are defined as templates and will be instantiated by Sveltos using the information taken from the load-balancer service matching the EventSource (Resource in this context represent a resource matching EventSource).</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven","cross cluster configuration"]},{"location":"events/examples/cross_cluster_configuration/#gke-cluster","title":"GKE Cluster","text":"<p>In the GKE cluster we create a deployment and a service of type LoadBalancer.</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment-50001\nspec:\n  selector:\n    matchLabels:\n      app: products\n      department: sales\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: products\n        department: sales\n    spec:\n      containers:\n      - name: hello\n        image: \"us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\"\n        env:\n        - name: \"PORT\"\n          value: \"50001\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-lb-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: products\n    department: sales\n  ports:\n  - protocol: TCP\n    port: 60000\n    targetPort: 50001\n</code></pre> <p>The Service will be assigned to an IP address.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-lb-service\n  namespace: test\n  ...\nspec:\n  ...\nstatus:\n  loadBalancer:\n    ingress:\n    - ip: 34.172.32.172\n</code></pre> <p>Once this is done, it will match the EventSource. Sveltos will deploy the selector-less Service and the Endpoints in the other cluster, the cluster-api provisioned cluster.</p> <p>The Endpoints IP address is set to the one assigned to the loadBalancer Service in the GKE cluster.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: external-my-lb-service\n  namespace: external\n  ...\nspec:\n  ports:\n  - port: 60000\n    protocol: TCP\n    targetPort: 50001\n  type: ClusterIP\nstatus:\n  loadBalancer: {}\n</code></pre> <pre><code>apiVersion: v1\nkind: Endpoints\nmetadata:\n  name: external-my-lb-service\n  namespace: external\n  ...\nsubsets:\n- addresses:\n  - ip: 34.172.32.172\n  ports:\n  - port: 60000\n    protocol: TCP\n</code></pre> <p>So at this point now a pod in the cluster-api provisioned cluster can reach the service in the GKE cluster.</p> <p>Let us create a namespace policy-demo and a busybox pod in the cluster-api provisioned cluster:</p> <pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: busybox\n  name: busybox\n  namespace: policy-demo\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - sleep 360000\n    image: busybox:1.28\n    imagePullPolicy: Always\n    name: busybox\n  nodeSelector:\n    kubernetes.io/os: linux\n</code></pre> <p>Then reach the service in the GKE cluster from the busybox pod in the cluster-api provisioned cluster\"</p> <pre><code>KUBECONFIG=&lt;KIND CLUSTER&gt; kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh\n/ #\n/ # wget -q external-my-lb-service.external:60000 -O -\nHello, world!\nVersion: 2.0.0\nHostname: my-deployment-50001-6664b685bc-db728\n</code></pre> <p></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven","cross cluster configuration"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/","title":"Implementing Multi-Tenancy with Database-as-a-Service - Project Sveltos","text":"<p>In this scenario, a managed cluster is shared among different tenants. Each tenant is assigned a namespace. By simply labeling a namespace with <code>postgres=required</code>, Sveltos will automatically deploy a dedicated PostgreSQL database within the services managed cluster. This database will then be made accessible to the requesting tenants.</p> <p></p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#lab-setup","title":"Lab Setup","text":"<p>A Civo cluster serves as the management cluster. Another Civo cluster, labeled <code>type=services</code>, is dedicated to automatic Postgres DB deployment by Sveltos.</p> <p>Postgres DB will be deployed using Cloudnative-pg.</p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#step-1-install-sveltos-on-management-cluster","title":"Step 1: Install Sveltos on Management Cluster","text":"<p>For this tutorial, we will install Sveltos in the management cluster. Sveltos installation details can be found here.</p> <pre><code>helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.16.1 --set crds.enabled=true\nhelm install projectsveltos projectsveltos/projectsveltos -n projectsveltos --create-namespace\n</code></pre> <p>Add the label <code>type=mgmt</code> to the management cluster:</p> <pre><code>kubectl label sveltoscluster -n mgmt mgmt type=mgmt\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#granting-extra-rbac","title":"Granting Extra RBAC","text":"<p>For this demo, Sveltos needs to be granted extra permission:</p> <pre><code>kubectl patch clusterrole addon-controller-role-extra -p '{\n  \"rules\": [\n    {\n      \"apiGroups\": [\n        \"\"\n      ],\n      \"resources\": [\n        \"configmaps\",\n        \"secrets\"\n      ],\n      \"verbs\": [\n        \"*\"\n      ]\n    }\n  ]\n}'\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#step-2-register-clusters-with-sveltos","title":"Step 2: Register Clusters with Sveltos","text":"<p>Using Civo UI, download the Kubeconfigs, then:</p> <pre><code>kubectl create ns managed-services\nsveltosctl register cluster --namespace=managed-services --cluster=services --kubeconfig=&lt;managed cluster kubeconfig&gt; --labels=type=services\n</code></pre> <p>Verify clusters were successfully registered:</p> <pre><code>kubectl get sveltoscluster -A --show-labels\nNAMESPACE          NAME      READY   VERSION        LABELS\nmgmt               mgmt      true    v1.29.2+k3s1   projectsveltos.io/k8s-version=v1.29.2,sveltos-agent=present,type=mgmt\nmanaged-services   services  true    v1.29.8+k3s1   projectsveltos.io/k8s-version=v1.29.8,sveltos-agent=present,type=services\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#step-3-deploy-cloudnative-pg","title":"Step 3: Deploy cloudnative-pg","text":"<p>Following ClusterProfile will deploy Cloudnative-pg in the managed cluster with label <code>type=services</code></p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/cloudnative-pg.yaml\n</code></pre> <p>Verify resources were deployed</p> <pre><code>sveltosctl show addons\n+--------------------------+---------------+-------------+------+---------+--------------------------------+----------------------------+\n|         CLUSTER          | RESOURCE TYPE |  NAMESPACE  | NAME | VERSION |              TIME              |          PROFILES          |\n+--------------------------+---------------+-------------+------+---------+--------------------------------+----------------------------+\n| managed-services/services| helm chart    | cnpg-system | cnpg | 0.22.1  | 2024-10-25 15:47:54 +0200 CEST | ClusterProfile/deploy-cnpg |\n+--------------------------+---------------+-------------+------+---------+--------------------------------+----------------------------+\n</code></pre> <p></p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#step-4-instruct-sveltos-to-automatically-deploy-postgres-db","title":"Step 4: Instruct Sveltos to automatically deploy Postgres DB","text":"<p>With the following configuration, Sveltos will actively monitor any managed cluster tagged with the label <code>type=app</code>. Specifically, it will look for namespaces within these clusters that are labeled with <code>postgres=required</code>. Upon identifying such a namespace, Sveltos will:</p> <ol> <li>Create a Postgres Cluster instance in the managed cluster with label <code>type:services</code>. DB will be exposed via a LoadBalancer service.</li> <li>Fetch credentials to access the DB.</li> <li>fetch the LoadBalancer service external ip: port</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/auto-deploy-postgres-cluster-per-ns.yaml\nkubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/fetch-postgres-data-per-ns.yaml\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#step-5-onboard-a-new-managed-cluster","title":"Step 5: Onboard a new managed cluster","text":"<p>Here we created a new Civo cluster and registered with Sveltos. This cluster will be shared by different tenants.</p> <pre><code>kubectl create ns apps\nsveltosctl register cluster --namespace=apps --cluster=shared --kubeconfig=&lt;managed cluster kubeconfig&gt; --labels=type=app\n</code></pre> <p>Whenever a new namespace in created in this cluster and assigned the <code>postgres=required</code> label, Sveltos will initiate the following actions:</p> <ol> <li>Deploy a new Postgres database: Sveltos will deploy a new Postgres database instance to the <code>type=services</code> cluster.</li> <li>Gather connection information: Once the deployment is complete, Sveltos will collect crucial connection details for the newly created Postgres instance, including its credentials, external IP address, and port number.</li> </ol> <p>To request a new database, simply create a namespace in your shared cluster and apply the <code>postgres=required</code> label to it.</p> <pre><code>KUBECONFIG=&lt;kubeconfig of your shared cluster&gt; kubectl create namespace coke\nKUBECONFIG=&lt;kubeconfig of your shared cluster&gt; kubectl label namespace coke postgres=required\n</code></pre> <p>Verify Sveltos deployed the Postgres Cluster and fetched the info necessary to connect:</p> <pre><code>kubectl get secret -n apps\nNAME                         TYPE     DATA   AGE\ncoke-app-credentials          Opaque   2      0s\n</code></pre> <p>The Secret Data section contains:</p> <pre><code>data:\n  password: bTloaW9UYUFBdVE1cFBQY1QzWGN6RDF2R3JUYzF5d3NVRTcwUTJQQXVUaTNucEZhRVdEYXpsZ1pmcnAzYWZwdg==\n  user: dG9kbw==\n</code></pre> <pre><code>kubectl get configmap -n apps\nNAME                        DATA   AGE\n...\ncoke-lb-data   2      58s\n</code></pre> <p>The ConfigMap Data section contains:</p> <pre><code>data:\n  external-ip: 212.2.242.242\n  port: \"5432\"\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#step-6-deploy-an-application-that-access-the-postgres-db","title":"Step 6: Deploy an application that access the Postgres DB","text":"<p>Sveltos can now be used to deploy a Job in the <code>coke</code> namespace. This Job will access the Postgres DB in the <code>services</code> cluster.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/job-to-create-table-per-ns.yaml\n</code></pre> <pre><code>watch KUBECONFIG=&lt;kubeconfig of your shared cluster&gt; kubectl get jobs -A\nNAMESPACE   NAME                          COMPLETIONS   DURATION   AGE\ncoke        coke-app-credentials-table    1/1           14s        2m10s\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service-multiple-db-per-cluster/#step-7-add-another-namespace","title":"Step 7: Add another namespace","text":"<p>Here we created yet another namespace in the shared_cluster and label it with <code>postgres=required</code>. As result:</p> <ol> <li>Sveltos deployed a new Postgres DB in the <code>services</code> cluster;</li> <li>Fetched the credentials and external-ip:port info to access the cluster;</li> <li>Deployed a Job in the <code>pepsi</code> cluster that creates a table in the DB.</li> </ol> <pre><code>watch KUBECONFIG=&lt;kubeconfig of your shared cluster&gt; kubectl get jobs -A\nNAMESPACE   NAME                          COMPLETIONS   DURATION   AGE\ncoke        coke-app-credentials-table    1/1           14s        5m10s\npepsi       pepsi-app-credentials-table   1/1           87s        113s\n</code></pre> <p>Note</p> <p>This might take 30 seconds or so, till Cloudnative-pg Cluster comes up and a LoadBalancer IP is assigned.</p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/","title":"DB as a Service - Project Sveltos","text":"<p>This demo will showcase Sveltos' ability to dynamically provision PostgreSQL databases on demand.</p> <p>By simply labeling a managed cluster with  <code>postgres=required</code>, Sveltos will automatically deploy a dedicated PostgreSQL database within the services managed cluster. This database will then be made accessible to the requesting cluster, ensuring seamless integration and data access.</p> <p>Note</p> <p>This tutorial assumes that each managed cluster is in a different namespace.</p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#lab-setup","title":"Lab Setup","text":"<p>A Civo cluster serves as the management cluster. Another Civo cluster, labeled <code>type=services</code>, is dedicated to automatic Postgres DB deployment by Sveltos.</p> <p>Postgres DB will be deployed using Cloudnative-pg.</p> <p></p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#step-1-install-sveltos-on-management-cluster","title":"Step 1: Install Sveltos on Management Cluster","text":"<p>For this tutorial, we will install Sveltos in the management cluster. Sveltos installation details can be found here.</p> <pre><code>helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.16.1 --set crds.enabled=true\nhelm install projectsveltos projectsveltos/projectsveltos -n projectsveltos --create-namespace\n</code></pre> <p>Add the label <code>type=mgmt</code> to the management cluster:</p> <pre><code>kubectl label sveltoscluster -n mgmt mgmt type=mgmt\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#granting-extra-rbac","title":"Granting Extra RBAC","text":"<p>For this demo, Sveltos needs to be granted extra permission:</p> <pre><code>kubectl patch clusterrole addon-controller-role-extra -p '{\n  \"rules\": [\n    {\n      \"apiGroups\": [\n        \"\"\n      ],\n      \"resources\": [\n        \"configmaps\",\n        \"secrets\"\n      ],\n      \"verbs\": [\n        \"*\"\n      ]\n    }\n  ]\n}'\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#step-2-register-clusters-with-sveltos","title":"Step 2: Register Clusters with Sveltos","text":"<p>Using Civo UI, download the Kubeconfigs, then:</p> <pre><code>kubectl create ns managed-services\nsveltosctl register cluster --namespace=managed-services --cluster=cluster --kubeconfig=&lt;managed cluster kubeconfig&gt; --labels=type=services\n</code></pre> <p>Verify clusters were successfully registered:</p> <pre><code>kubectl get sveltoscluster -A --show-labels\nNAMESPACE          NAME      READY   VERSION        LABELS\nmgmt               mgmt      true    v1.29.2+k3s1   projectsveltos.io/k8s-version=v1.29.2,sveltos-agent=present,type=mgmt\nmanaged-services   cluster   true    v1.29.8+k3s1   projectsveltos.io/k8s-version=v1.29.8,sveltos-agent=present,type=services\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#step-3-deploy-cloudnative-pg","title":"Step 3: Deploy cloudnative-pg","text":"<p>Following ClusterProfile will deploy Cloudnative-pg in the managed cluster with label <code>type=services</code></p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/cloudnative-pg.yaml\n</code></pre> <p>Verify resources were deployed</p> <pre><code>sveltosctl show addons\n+--------------------------+---------------+-------------+------+---------+--------------------------------+----------------------------+\n|         CLUSTER          | RESOURCE TYPE |  NAMESPACE  | NAME | VERSION |              TIME              |          PROFILES          |\n+--------------------------+---------------+-------------+------+---------+--------------------------------+----------------------------+\n| managed-services/cluster | helm chart    | cnpg-system | cnpg | 0.22.1  | 2024-10-25 15:47:54 +0200 CEST | ClusterProfile/deploy-cnpg |\n+--------------------------+---------------+-------------+------+---------+--------------------------------+----------------------------+\n</code></pre> <p></p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#step-4-instruct-sveltos-to-automatically-deploy-postgres-db","title":"Step 4: Instruct Sveltos to automatically deploy Postgres DB","text":"<p>Following configuration will instruct Sveltos to watch for managed cluster with labels <code>postgres=required</code>. Anytime such a cluster is detect, Sveltos will:</p> <ol> <li>Create a Postgres Cluster instance in the managed cluster with label <code>type:services</code>. DB will be exposed via a LoadBalancer service.</li> <li>Fetch credentials to access the DB.</li> <li>fetch the LoadBalancer service external ip: port</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/auto-deploy-postgres-cluster.yaml\nkubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/fetch-postgres-data.yaml\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#step-5-onboard-a-new-managed-cluster","title":"Step 5: Onboard a new managed cluster","text":"<p>Whenever a new managed cluster is registered with Sveltos and labeled with 'postgres=required', Sveltos will initiate the deployment of a new Postgres database on the 'type=services' cluster. Once deployed, Sveltos will gather the essential connection information, including credentials, external IP address, and port number, for this newly created Postgres instance.</p> <p>Here we created a new Civo cluster and registered with Sveltos:</p> <pre><code>kubectl create ns coke\nsveltosctl register cluster --namespace=coke --cluster=my-app --kubeconfig=&lt;managed cluster kubeconfig&gt; --labels=postgres=required\n</code></pre> <p>Verify Sveltos deployed the Postgres Cluster and fetched the info necessary to connect:</p> <pre><code>kubectl get secret -n coke\nNAME                         TYPE     DATA   AGE\npg-credentials          Opaque   2      0s\n</code></pre> <p>The Secret Data section contains:</p> <pre><code>data:\n  password: bTloaW9UYUFBdVE1cFBQY1QzWGN6RDF2R3JUYzF5d3NVRTcwUTJQQXVUaTNucEZhRVdEYXpsZ1pmcnAzYWZwdg==\n  user: dG9kbw==\n</code></pre> <pre><code>kubectl get configmap -n coke\nNAME                        DATA   AGE\n...\npg-loadbalancer-data   2      58s\n</code></pre> <p>The ConfigMap Data section contains:</p> <pre><code>data:\n  external-ip: 212.2.242.242\n  port: \"5432\"\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#step-6-deploy-an-application-that-access-the-postgres-db","title":"Step 6: Deploy an application that access the Postgres DB","text":"<p>Sveltos can now be used to deploy a Job in the <code>coke</code> cluster. This Job will access the Postgres DB in the <code>services</code> cluster.</p> <p>This Job is expressed as a template and will be deployed by Sveltos in any cluster with label <code>type=app</code>.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/job-to-create-table.yaml\n</code></pre> <pre><code>kubectl label sveltoscluster -n coke my-app type=app\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#step-7-add-another-managed-cluster","title":"Step 7: Add another managed cluster","text":"<p>Here we created yet another Civo cluster and registered with Sveltos<sup>1</sup>. As result:</p> <ol> <li>Sveltos deployed a new Postgres DB in the <code>services</code> cluster;</li> <li>Fetched the credentials and external-ip:port info to access the cluster;</li> <li>Deployed a Job in the <code>pepsi</code> cluster that creates a table in the DB.</li> </ol>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/db-as-a-service/#multi-tenancy-scenario","title":"Multi-tenancy scenario","text":"<p>For multi-tenant clusters with a database per tenant, see this tutorial.</p> <ol> <li> <p><pre><code>kubectl create ns pepsi\nsveltosctl register cluster --namespace=pepsi --cluster=cluster --kubeconfig=&lt;managed cluster kubeconfig&gt; --labels=postgres=required,type=app\n</code></pre> \u21a9</p> </li> </ol>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/dynamic_nginx_dep_based_ns_annotation/","title":"Example Dynamic Nginx Deploymemt based on Namespace Annotation","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/dynamic_nginx_dep_based_ns_annotation/#scenario-description","title":"Scenario Description","text":"<p>In a fast-changing and often complex Kubernetes environment, it is important for Platform teams and engineers to automate application deployments using unique identifiers. For instance, actions or deployments can be triggered only when a Kubernetes resource has a specific annotation.</p> <p>This example demonstrates how namespace annotations can be used as key identifiers. By using these annotations, Sveltos can automatically deploy different versions of the Nginx server to different namespaces based on the annotations set.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/dynamic_nginx_dep_based_ns_annotation/#namespace-eventsource","title":"Namespace EventSource","text":"<p>Example - EventSource Definition</p> <pre><code>cat &gt; eventsource.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: find-all-namespaces\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Namespace\"\n</code></pre> <p>The <code>EventSource</code> described above will trigger the following <code>EventTrigger</code> action for each namespace in the Kubernetes cluster.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/dynamic_nginx_dep_based_ns_annotation/#eventtrigger-defintion","title":"EventTrigger Defintion","text":"<p>Example - EventTrigger Definition</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: deploy-nginx\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: fv\n  eventSourceName: find-all-namespaces\n  oneForEvent: true\n  policyRefs:\n  - name: deploy-nginx\n    namespace: default\n    kind: ConfigMap\n</code></pre> <p>Example - ConfigMap deploy-nginx</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: deploy-nginx\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok # this annotation is what tells Sveltos to instantiate this ConfigMap\ndata:\n  networkpolicy.yaml: |\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: nginx-deployment\n      namespace: {{ .Resource.metadata.name }}\n      labels:\n        app: nginx\n    spec:\n      replicas: 1\n      selector:\n        matchLabels:\n          app: nginx\n      template:\n        metadata:\n          labels:\n            app: nginx\n        spec:\n          containers:\n          - name: nginx\n            {{- if (index .Resource.metadata `annotations`) -}}\n            {{- if (index .Resource.metadata.annotations `nginx`) }}\n            image: nginx:{{ .Resource.metadata.annotations.nginx }}\n            {{- else }} # nginx key does NOT exist within annotations\n            image: nginx:1.14.2\n            {{- end }}\n            {{- else }} # annotations not present\n            image: nginx:1.14.2\n            {{- end }}\n            ports:\n            - containerPort: 80\n</code></pre> <p>When the <code>EventTrigger</code> resource is activated, Sveltos is instructed to deploy the <code>ConfigMap</code> named <code>deploy-nginx</code> only to managed clusters labeled with env:fv.</p> <p>The <code>ConfigMap</code> is expressed as a Sveltos Template, which allows simple conditions to determine which Nginx version should be deployed based on the namespace annotation.</p> <pre><code>{{- if (index .Resource.metadata `annotations`) -}}\n{{- if (index .Resource.metadata.annotations `nginx`) }}\nimage: nginx:{{ .Resource.metadata.annotations.nginx }}\n{{- else }} # nginx key does NOT exist within annotations\nimage: nginx:1.14.2\n{{- end }}\n{{- else }} # annotations not present\nimage: nginx:1.14.2\n{{- end }}\n</code></pre> <p>If the annotation <code>nginx</code> is present in a namespace, Nginx with the specified version (image: nginx:) will be installed. If the nginx annotation is missing or no annotations are set, Nginx version 1.14.2 will be installed by default.","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/dynamic_nginx_dep_based_ns_annotation/#next-steps","title":"Next Steps","text":"<p>To explore more about the powerful features of Sveltos Templates, have a look here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/flux_sources/","title":"Example - Flux Sources with Sveltos EventFramework","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/flux_sources/#sveltos-and-flux-sources","title":"Sveltos and Flux Sources","text":"<p>Most common use cases utilise the power of Sveltos together with Flux to deliver seamless GitOps workflows for deployments across a fleet of clusters. In this example, we combine the power of the Sveltos Event Framework, enabling operators to easily synchronise and deploy Kubernetes manifests or Helm charts stored in a source control system.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/flux_sources/#install-and-configure-flux","title":"Install and Configure Flux","text":"<p>If you have not done so already, refer to the Flux section to learn how to install Flux, set up Flux sources, and see how Sveltos makes use of them.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/flux_sources/#flux-source","title":"Flux Source","text":"<p>After installing Flux on the management cluster, we can create a <code>GitRepository</code> resource and define the repository we want to synchronise.</p> <p>Flux GitRepository</p> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: test\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: github-creds\n  timeout: 60s\n  url: https://&lt;GitHub domain&gt;/&lt;group name&gt;/&lt;repository name&gt;.git \n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/flux_sources/#eventframework","title":"EventFramework","text":"<p>In this example, we want to automatically deploy Kubernetes manifests and/or Helm charts from a specific Flux source whenever new clusters are registered with Sveltos and have the label env: test.</p> <p>EventSource</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: cluster-registration\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"lib.projectsveltos.io\"\n    version: \"v1beta1\"\n    kind: \"SveltosCluster\"\n    labelFilters:\n    - key: env\n      operation: Equal\n      value: test\n</code></pre> <p>EventTrigger</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: deploy-app\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      cluster: mgmt\n  destinationCluster:\n    name: \"{{ .Resource.metadata.name }}\"\n    namespace: \"{{ .Resource.metadata.namespace }}\"\n    kind: SveltosCluster\n    apiVersion: lib.projectsveltos.io/v1beta1\n  eventSourceName: cluster-registration\n  oneForEvent: true\n  policyRefs:\n  - kind: GitRepository\n    name: test\n    namespace: flux-system\n    path: \"{{ .Resource.metadata.name }}\"\n</code></pre> <p>So, what happens with the <code>EventTrigger</code> manifest?</p> <p>First, we match the Sveltos management cluster as the source cluster. Next, we dynamically define the destination cluster using information from the Sveltos management cluster. Finally, we specify the <code>Path</code> we want to deploy to the matching destination cluster, also in a dynamic way.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/flux_sources/#what-does-this-look-like-in-practice","title":"What does this look like in practice?","text":"<p>Every time a new cluster is registered with Sveltos and has the label env: test, an event is triggered. Sveltos then dynamically deploys the relevant Kubernetes manifests and/or Helm charts to that cluster, using the Flux source defined in the initial step. The Path used is the name of the matching cluster.</p> <p>This means the corresponding folder must exist within the specified Flux source. If it does not, Sveltos will not deploy anything to the cluster.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/loadbalancer/","title":"Example Loadbalancer - Project Sveltos","text":"<p>This guide demonstrates how to automate load balancer configuration for Kubernetes services using Sveltos <code>EventTrigger</code> and <code>EventSource</code>.</p>"},{"location":"events/examples/loadbalancer/#architecutre-overview","title":"Architecutre Overview","text":"<p> There are two clusters involved: a \"MGMT Cluster\" and a \"Managed Cluster\" (cluster-a). Cluster-a hosts two services: <code>svc-a</code> and <code>svc-b</code>. <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    lb.buttah.cloud/class: internet\n  name: svc-a\n  namespace: test\nspec:\n  type: LoadBalancer\n  ports:\n  - name: test\n    port: 1234\n    protocol: TCP\n    targetPort: test\n  selector:\n    app.kubernetes.io/name: test\n</code></pre> The \"MGMT Cluster\" runs a CNI that implements a load balancer for services of type LoadBalancer. This implementation supports the <code>lb.buttah.cloud/class</code> label with values <code>intern</code> or <code>internet</code>.</p>"},{"location":"events/examples/loadbalancer/#eventsource","title":"EventSource","text":"<p>To implement the load balancer solution, an <code>EventSource</code> is required to listen for new services. This example filters only for <code>Services</code> with the label <code>lb.buttah.cloud/class</code>. <pre><code># deploy on MGMT Cluster\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: loadbalancer-class-handler\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    evaluate: |\n      function evaluate()\n        hs = {}\n        hs.matching = false\n        if obj.metadata.labels[\"lb.buttah.cloud/class\"] ~= nil then\n          hs.matching = true\n          return hs\n        end\n        return hs\n      end\n</code></pre></p>"},{"location":"events/examples/loadbalancer/#eventtrigger","title":"EventTrigger","text":"<p>After deploying an <code>EventSource</code>, an <code>EventTrigger</code> is needed. The <code>EventTrigger</code> listens to a specific <code>EventSource</code> and can produce new resources based on the event. This is achieved through <code>EventTrigger.spec.policyRefs</code>.</p> <p>In this case, two new resources are created based on the content of two <code>ConfigMaps</code>: <code>loadbalancer-class-handler-svc</code> and <code>loadbalancer-class-handler-cp</code>. The <code>ConfigMap</code> <code>loadbalancer-class-handler-svc</code> will create a new <code>Service</code> by copying the <code>Service.spec</code> from the resource that triggered the <code>EventTrigger</code> (this example of svc-a from cluster-a). This can be done using the variable <code>{{ .Resource }}</code>.</p> <p>To deploy the new resource to the \"MGMT Cluster\", set <code>EventTrigger.spec.policyRefs[].deploymentType</code> to Local. The new resource should be deployed in the associated namespace of the \"Managed Cluster\" using the variable <code>{{ .Cluster }}</code>.</p> <pre><code># deploy on MGMT Cluster\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: loadbalancer-class-handler\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: prod\n  eventSourceName: loadbalancer-class-handler\n  oneForEvent: true\n  policyRefs:\n  - kind: ConfigMap\n    name: loadbalancer-class-handler-svc\n    namespace: projectsveltos\n    deploymentType: Local\n  - kind: ConfigMap\n    name: loadbalancer-class-handler-cp\n    namespace: projectsveltos\n    deploymentType: Local\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: loadbalancer-class-handler-svc\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  service.yaml: |\n    kind: Service\n    apiVersion: v1\n    metadata:\n      name: \"lb-{{ cat .Resource.metadata.name .Resource.metadata.namespace .Cluster.metadata.name | sha1sum }}\"\n      namespace: \"{{ .Cluster.metadata.namespace }}\"\n      labels:\n        lb.buttah.cloud/class: \"{{ get .Resource.metadata.labels `lb.buttah.cloud/class` }}\"\n        lb.buttah.cloud/cluster: \"{{ .Cluster.metadata.name }}\"\n      annotations:\n        lb.buttah.cloud/name: \"{{ .Resource.metadata.name }}\"\n        lb.buttah.cloud/namespace: \"{{ .Resource.metadata.namespace }}\"\n    spec:\n      ports:\n        {{- range $port := .Resource.spec.ports }}\n        - name: \"{{ $port.name }}\"\n          port: {{ $port.port }}\n          protocol: \"{{ $port.protocol }}\"\n          targetPort: {{ $port.nodePort }}\n        {{- end }}\n      selector:\n        cluster.x-k8s.io/cluster-name: \"{{ .Cluster.metadata.name }}\"\n      type: LoadBalancer\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: loadbalancer-class-handler-cp\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  cp.yaml: |\n    apiVersion: config.projectsveltos.io/v1beta1\n    kind: ClusterProfile\n    metadata:\n      name: \"lbs-{{ cat .Resource.metadata.name .Resource.metadata.namespace .Cluster.metadata.name | sha1sum }}\"\n      annotations:\n        lb.buttah.cloud/name: \"{{ .Resource.metadata.name }}\"\n        lb.buttah.cloud/namespace: \"{{ .Resource.metadata.namespace }}\"\n    spec:\n      clusterRefs:\n      - apiVersion: lib.projectsveltos.io/v1beta1\n        kind: SveltosCluster\n        name: \"{{ .Cluster.metadata.name }}\"\n        namespace: \"{{ .Cluster.metadata.namespace }}\"\n      templateResourceRefs:\n      - identifier: UpstreamLB\n        resource:\n          apiVersion: v1\n          kind: Service\n          name: \"lb-{{ cat .Resource.metadata.name .Resource.metadata.namespace .Cluster.metadata.name | sha1sum }}\"\n          namespace: \"{{ .Cluster.metadata.namespace }}\"\n      policyRefs:\n      - kind: ConfigMap\n        name: loadbalancer-class-handler-status\n        namespace: projectsveltos\n        deploymentType: Remote\n</code></pre> <p>Here is an example <code>ClusterProfile</code> and <code>ConfigMaps</code> which are generated for svc-a from the obove <code>EventTrigger</code> in order to deploy the Service on the \"MGMT Cluster\". These <code>ClusterProfile</code> are then normally executed by the addon-manager.</p> <pre><code>#  generated Ressource on MGMT Cluster from EventTrigger\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: sveltos-2z2p8p7olrro79biygp8\nspec:\n  clusterRefs:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: a\n    namespace: cluster-a\n  policyRefs:\n  - deploymentType: Local\n    kind: ConfigMap\n    name: sveltos-8dem6v44g95u8lh5oi55\n    namespace: projectsveltos\n  - deploymentType: Local\n    kind: ConfigMap\n    name: sveltos-uad8ick2n9mrde65usds\n    namespace: projectsveltos\nstatus:\n  matchingClusters:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: a\n    namespace: cluster-a\n---\napiVersion: v1\ndata:\n  service.yaml: |\n    kind: Service\n    apiVersion: v1\n    metadata:\n      name: \"lb-894cbba1a1a9a95d0bdb13e08dbbeb6db3f2e672\"\n      namespace: \"cluster-a\"\n      labels:\n        lb.buttah.cloud/class: \"internet\"\n        lb.buttah.cloud/cluster: \"a\"\n      annotations:\n        lb.buttah.cloud/name: \"svc-a\"\n        lb.buttah.cloud/namespace: \"default\"\n    spec:\n      ports:\n        - name: \"test\"\n          port: 1234\n          protocol: \"TCP\"\n          targetPort: 1111\n      selector:\n          cluster.x-k8s.io/cluster-name: \"a\"\n      type: LoadBalancer\nkind: ConfigMap\nmetadata:\n  name: sveltos-8dem6v44g95u8lh5oi55\n---\napiVersion: v1\ndata:\n  cp.yaml: |\n    apiVersion: config.projectsveltos.io/v1beta1\n    kind: ClusterProfile\n    metadata:\n      name: \"lbs-894cbba1a1a9a95d0bdb13e08dbbeb6db3f2e672\"\n      annotations:\n        lb.buttah.cloud/name: \"svc-a\"\n        lb.buttah.cloud/namespace: \"default\"\n    spec:\n      clusterRefs:\n      - apiVersion: lib.projectsveltos.io/v1beta1\n        kind: SveltosCluster\n        name: \"a\"\n        namespace: \"cluster-a\"\n      templateResourceRefs:\n      - identifier: UpstreamLB\n        resource:\n          apiVersion: v1\n          kind: Service\n          name: \"lb-894cbba1a1a9a95d0bdb13e08dbbeb6db3f2e672\"\n          namespace: \"cluster-a\"\n      policyRefs:\n      - kind: ConfigMap\n        name: loadbalancer-class-handler-status\n        namespace: projectsveltos\n        deploymentType: Remote\nkind: ConfigMap\nmetadata:\n  name: sveltos-uad8ick2n9mrde65usds\n</code></pre> <p>After the addon-manager executes both <code>ClusterProfiles</code>, the following resources are deployed on the \"MGMT Cluster.\" The load balancer implementation on the \"MGMT Cluster\" will assign an IP to the service, in this case, 1.1.1.1. Another <code>ClusterProfile</code> is responsible for reporting the IP back to the \"Managed Clusters\" (cluster-a). To patch the IP back, the <code>ClusterProfile</code> uses a ConfigMap with the annotation <code>projectsveltos.io/subresources</code> set to <code>status</code>, indicating to the addon-manager to patch this on the subresource status on the Kubernetes API on the \"Managed Clusters\" (cluster-a). The field <code>EventTrigger.spec.templateResourceRefs</code> is used to add a depended object which Information can be used in the <code>ConfigMap</code> in this example you can access the deployed the <code>Service</code> on the MGMT Cluster status using variable <code>UpstreamLB</code>. <pre><code># Service Deployed on MGMT Cluster from addon-manager\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    lb.buttah.cloud/class: internet\n  name: lb-894cbba1a1a9a95d0bdb13e08dbbeb6db3f2e672\n  namespace: cluster-a\nspec:\n  type: LoadBalancer\n  ports:\n  - name: test\n      port: 1234\n      protocol: TCP\n      targetPort: test\n  selector:\n      cluster.x-k8s.io/cluster-name: \"cluster-a\"\nstatus:\n  loadBalancer:\n    ingress:\n    - ip: 1.1.1.1\n---\n# ClusterProfile deployed on MGMT CLuster from addon-manager\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: \"lbs-894cbba1a1a9a95d0bdb13e08dbbeb6db3f2e672\"\n  annotations:\n    lb.buttah.cloud/name: \"svc-a\"\n    lb.buttah.cloud/namespace: \"default\"\nspec:\n  clusterRefs:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: \"cluster-a\"\n    namespace: \"cluster-a\"\n  templateResourceRefs:\n  - identifier: UpstreamLB\n    resource:\n      apiVersion: v1\n      kind: Service\n      name: \"lb-894cbba1a1a9a95d0bdb13e08dbbeb6db3f2e672\"\n      namespace: \"cluster-a\"\n  policyRefs:\n  - kind: ConfigMap\n    name: loadbalancer-class-handler-status\n    namespace: projectsveltos\n    deploymentType: Remote\n---\n# ConfigMap with Patch definition to be deployed on cluster-a\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: loadbalancer-class-handler-status\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/template: ok\n    projectsveltos.io/subressources: \"status\"\ndata:\n  service.yaml: |\n    kind: Service\n    apiVersion: v1\n    metadata:\n      name: {{ get (getResource \"UpstreamLB\").metadata.annotations `lb.buttah.cloud/name` }}\n      namespace: {{ get (getResource \"UpstreamLB\").metadata.annotations `lb.buttah.cloud/namespace` }}\n    status:\n      loadBalancer:\n        ingress:\n          {{- range $ingress := (getResource \"UpstreamLB\").status.loadBalancer.ingress }}\n          - ip: \"{{ $ingress.ip }}\"\n          {{- end }}\n</code></pre></p>"},{"location":"events/examples/loadbalancer/#data-path","title":"Data Path","text":"<p>At the end the <code>Service</code> svc-a on the MGMT Cluster will announce the IP 1.1.1.1 to the outside world. Thus a Client can access it. The backend Service for svc-a on the MGMT Cluster is set to the nodes for cluster-a. Thus the traffic gets forwarded to these nodes using the node-port defined in the svc-a on cluster-a.</p>"},{"location":"events/examples/loadbalancer/#full-code-to-deploy","title":"Full Code to deploy","text":"<pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: loadbalancer-class-handler\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    evaluate: |\n      function evaluate()\n        hs = {}\n        hs.matching = false\n        if obj.metadata.labels[\"lb.buttah.cloud/class\"] ~= nil  then\n          hs.matching = true\n          return hs\n        end\n        return hs\n      end\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: loadbalancer-class-handler\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: prod\n  eventSourceName: loadbalancer-class-handler\n  oneForEvent: true\n  policyRefs:\n  - kind: ConfigMap\n    name: loadbalancer-class-handler-svc\n    namespace: projectsveltos\n    deploymentType: Local\n  - kind: ConfigMap\n    name: loadbalancer-class-handler-cp\n    namespace: projectsveltos\n    deploymentType: Local\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: loadbalancer-class-handler-svc\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  service.yaml: |\n    kind: Service\n    apiVersion: v1\n    metadata:\n      name: \"lb-{{ cat .Resource.metadata.name .Resource.metadata.namespace .Cluster.metadata.name | sha1sum }}\"\n      namespace: \"{{ .Cluster.metadata.namespace }}\"\n      labels:\n        lb.buttah.cloud/class: \"{{ get .Resource.metadata.labels `lb.buttah.cloud/class` }}\"\n        lb.buttah.cloud/cluster: \"{{ .Cluster.metadata.name }}\"\n      annotations:\n        lb.buttah.cloud/name: \"{{ .Resource.metadata.name }}\"\n        lb.buttah.cloud/namespace: \"{{ .Resource.metadata.namespace }}\"\n    spec:\n      ports:\n        {{- range $port := .Resource.spec.ports }}\n        - name: \"{{ $port.name }}\"\n          port: {{ $port.port }}\n          protocol: \"{{ $port.protocol }}\"\n          targetPort: {{ $port.nodePort }}\n        {{- end }}\n      selector:\n        cluster.x-k8s.io/cluster-name: \"{{ .Cluster.metadata.name }}\"\n      type: LoadBalancer\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: loadbalancer-class-handler-cp\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  cp.yaml: |\n    apiVersion: config.projectsveltos.io/v1beta1\n    kind: ClusterProfile\n    metadata:\n      name: \"lbs-{{ cat .Resource.metadata.name .Resource.metadata.namespace .Cluster.metadata.name | sha1sum }}\"\n      annotations:\n        lb.buttah.cloud/name: \"{{ .Resource.metadata.name }}\"\n        lb.buttah.cloud/namespace: \"{{ .Resource.metadata.namespace }}\"\n    spec:\n      clusterRefs:\n      - apiVersion: lib.projectsveltos.io/v1beta1\n        kind: SveltosCluster\n        name: \"{{ .Cluster.metadata.name }}\"\n        namespace: \"{{ .Cluster.metadata.namespace }}\"\n      templateResourceRefs:\n      - identifier: UpstreamLB\n        resource:\n          apiVersion: v1\n          kind: Service\n          name: \"lb-{{ cat .Resource.metadata.name .Resource.metadata.namespace .Cluster.metadata.name | sha1sum }}\"\n          namespace: \"{{ .Cluster.metadata.namespace }}\"\n      policyRefs:\n      - kind: ConfigMap\n        name: loadbalancer-class-handler-status\n        namespace: projectsveltos\n        deploymentType: Remote\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: loadbalancer-class-handler-status\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/template: ok\n    projectsveltos.io/subressources: \"status\"\ndata:\n  service.yaml: |\n    kind: Service\n    apiVersion: v1\n    metadata:\n      name: {{ get (getResource \"UpstreamLB\").metadata.annotations `lb.buttah.cloud/name` }}\n      namespace: {{ get (getResource \"UpstreamLB\").metadata.annotations `lb.buttah.cloud/namespace` }}\n    status:\n      loadBalancer:\n        ingress:\n          {{- range $ingress := (getResource \"UpstreamLB\").status.loadBalancer.ingress }}\n          - ip: \"{{ $ingress.ip }}\"\n          {{- end }}\n</code></pre>"},{"location":"events/examples/managed-services/","title":"Example Service Event Trigger - Project Sveltos","text":"<p>This demo will showcase Sveltos' capabilities by:</p> <ol> <li>Provisioning multiple Postgres instances: All instances will be deployed on a managed Kubernetes cluster, exposed via a LoadBalancer service, and secured with unique credentials;</li> <li>Retrieving instance details: Sveltos will extract Postgres credentials and LoadBalancer endpoint information;</li> <li>Creating database objects: Two separate Jobs will be deployed on distinct managed Kubernetes clusters (<code>pre-production</code> and <code>production</code>). These Jobs will connect to different Postgres instances and execute SQL commands to create tables.</li> </ol> <p></p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/managed-services/#managed-clusters","title":"Managed Clusters","text":"<ul> <li>managed-services-cluster: This cluster is a managed service cluster used to deploy Postgres databases. It has labels <code>type: managed-services</code></li> <li>pre-production: This cluster is dedicated to pre-production deployments. Applications running here will access the pre-production Postgres instance. Labels include <code>type: pre-production</code></li> <li>production: This cluster is for production deployments. Applications running here will access the production Postgres instance. Labels include: <code>type: production</code></li> </ul> <p>The type label differentiates between managed service deployment, pre-production, and production environments. Applications needing to access Postgres databases should be deployed in the appropriate cluster based on their environment (pre-production or production).</p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/managed-services/#granting-extra-rbac","title":"Granting Extra RBAC","text":"<p>For this demo, Sveltos needs to be granted extra permission.</p> <pre><code>kubectl edit clusterroles  addon-controller-role-extra\n</code></pre> <p>and add following permissions</p> <pre><code>- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - namespaces\n  - secrets\n  verbs:\n  - \"*\"\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/managed-services/#deploy-postgres","title":"Deploy Postgres","text":"<p>Sveltos will be used to deploy two Postgres instances on the <code>managed-services-cluster</code></p> <pre><code>wget https://raw.githubusercontent.com/projectsveltos/demos/main/managed-services/production-postgres.yaml\nkubectl create configmap production-postgres --from-file=production-postgres.yaml\nwget https://raw.githubusercontent.com/projectsveltos/demos/main/managed-services/pre-production-postgres.yaml\nkubectl create configmap pre-production-postgres --from-file=pre-production-postgres.yaml\nkubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/managed-services/deploy-postgres-clusterprofile.yaml\n</code></pre> <p>Verify two Postgres instances (one in the <code>pre-production</code> namespace and the other in the <code>production</code> namespace) are deployed using sveltosctl:</p> <pre><code>sveltosctl show addons\n+------------------------------------------+-----------------+-------------------------+-------------------------+---------+--------------------------------+-----------------------------------------------+\n|                 CLUSTER                  |  RESOURCE TYPE  |        NAMESPACE        |          NAME           | VERSION |              TIME              |                   PROFILES                    |\n+------------------------------------------+-----------------+-------------------------+-------------------------+---------+--------------------------------+-----------------------------------------------+\n| managed-services/managed-service-cluster | :Secret         | pre-production-services | postgres-secret         | N/A     | 2024-07-29 14:20:20 +0200 CEST | ClusterProfile/deploy-pre-production-postgres |\n| managed-services/managed-service-cluster | apps:Deployment | pre-production-services | postgresql              | N/A     | 2024-07-29 14:20:21 +0200 CEST | ClusterProfile/deploy-pre-production-postgres |\n| managed-services/managed-service-cluster | :Service        | pre-production-services | postgresql              | N/A     | 2024-07-29 14:20:22 +0200 CEST | ClusterProfile/deploy-pre-production-postgres |\n| managed-services/managed-service-cluster | :Namespace      |                         | production-services     | N/A     | 2024-07-29 14:20:04 +0200 CEST | ClusterProfile/deploy-production-postgres     |\n| managed-services/managed-service-cluster | :Secret         | production-services     | postgres-secret         | N/A     | 2024-07-29 14:20:06 +0200 CEST | ClusterProfile/deploy-production-postgres     |\n| managed-services/managed-service-cluster | apps:Deployment | production-services     | postgresql              | N/A     | 2024-07-29 14:20:07 +0200 CEST | ClusterProfile/deploy-production-postgres     |\n| managed-services/managed-service-cluster | :Service        | production-services     | postgresql              | N/A     | 2024-07-29 14:20:08 +0200 CEST | ClusterProfile/deploy-production-postgres     |\n| managed-services/managed-service-cluster | :Namespace      |                         | pre-production-services | N/A     | 2024-07-29 14:20:18 +0200 CEST | ClusterProfile/deploy-pre-production-postgres |\n+------------------------------------------+-----------------+-------------------------+-------------------------+---------+--------------------------------+-----------------------------------------------+\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/managed-services/#fetch-postgres-info","title":"Fetch Postgres info","text":"<p>Post-Postgres deployment, we require:</p> <ul> <li>Database credentials for establishing connections.</li> <li>LoadBalancer endpoint information (IP:port) to access the Postgres instance externally.</li> </ul> <p>Sveltos event framework was used to collect such information.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/managed-services/fetch-credentials.yaml\nkubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/managed-services/fetch-service-ip.yaml\n</code></pre> <p>Verify that information was successfully collected</p> <pre><code>kubectl get configmap -n production-services postgres-host-port\nkubectl get configmap -n pre-production-services postgres-host-port\nkubectl get secret -n production-services postgres-credentials\nkubectl get secret -n pre-production-services postgres-credentials\n</code></pre>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/managed-services/#deploy-job-to-the-production-cluster","title":"Deploy Job to the production cluster","text":"<p>With the Postgres service operational on the managed-services-cluster, we can proceed with deploying a Job to the managed production cluster. This Job will create a table within the production Postgres database. To initiate this process, let's construct a ConfigMap containing a Job template.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/managed-services/configmap-with-templated-job.yaml\n</code></pre> <p>Next, we'll instruct Sveltos to instantiate the Job template and deploy it to the production cluster:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/managed-services/deploy-job-to-production.yaml\n</code></pre> <p>By directing kubectl to the production cluster, we can confirm the Job's creation and successful completion. This indicates that the table has been successfully established within the Postgres database on the managed services cluster.</p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/managed-services/#deploy-job-to-the-pre-production-cluster","title":"Deploy Job to the pre-production cluster","text":"<p>A similar Job can be deployed to the pre-production cluster to create a table within the corresponding pre-production Postgres database.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/managed-services/deploy-job-to-pre-production.yaml\n</code></pre> <p>By directing kubectl to the pre-production cluster, we can confirm the Job's creation and successful completion. This indicates that the table has been successfully established within the Postgres database on the managed services cluster.</p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/managed-services/#yet-another-example","title":"Yet another example","text":"<p>Learn how to expose your managed services with Gateway API.</p>","tags":["Kubernetes","managed services","Sveltos","event driven"]},{"location":"events/examples/mcs_api/","title":"Example \u2013 multi-cluster Services via Sveltos Event Framework (KEP-1645)","text":"","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#overview","title":"Overview","text":"<p>KEP-1645: Multi-Cluster Services (MCS) API from SIG Multi-Cluster standardizes how a Service in one cluster can be exported and discovered in others. See the proposal: KEP-1645: Multi-Cluster Services (MCS) API</p> <ul> <li>ServiceExport: Marks a namespaced Service for export.</li> <li>ServiceImport: ClusterSet-scoped discovery surface for consumers.</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#problem-description","title":"Problem Description","text":"<p>Sveltos does not ship a built-in controller for KEP-1645.</p>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#solution","title":"Solution","text":"<p>Use Sveltos' Event Framework to automate the MCS pipeline:</p> <ol> <li>Detect <code>ServiceExport</code> the corresponding <code>Service</code>.</li> <li>Create a derived Service (normalized to <code>ClusterIP</code>) and then a ServiceImport populated from the derived Service.</li> <li>Detect <code>EndpointSlice</code> updates and mirror them across clusters, ensuring label/shape compatibility for CoreDNS.</li> </ol> <p>Note</p> <p>The examples below focus on resource creation and synchronization. <code>status</code> updates (conditions on <code>ServiceExport</code>, health, conflicts) are out of scope here and can be implemented later with an auxiliary controller or an additional Event pipeline.</p>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#kep-1645-specification","title":"KEP-1645 Specification","text":"","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#resource-roles","title":"Resource Roles","text":"<p>The MCS API defines the following resources:</p> <ul> <li>ServiceExport: Created in the source cluster to mark a Service for export</li> <li>ServiceImport: Created in consuming clusters for service discovery</li> <li>Derived Service: A regular Service object (<code>derived-&lt;hash&gt;</code>) that kube-proxy can recognize without modifications</li> <li>EndpointSlice: Contains the actual endpoint information, labeled for multi-cluster discovery</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#service-type-conversion-rules","title":"Service Type Conversion Rules","text":"Source Service Type ServiceImport Type Derived Service Type Notes ClusterIP ClusterSetIP ClusterIP Standard conversion NodePort ClusterSetIP ClusterIP Normalized to ClusterIP LoadBalancer ClusterSetIP ClusterIP Normalized to ClusterIP Headless (<code>clusterIP: None</code>) Headless ClusterIP (headless) Creates headless derived service ExternalName - - Cannot be exported","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#naming-conventions-and-labels","title":"Naming Conventions and Labels","text":"<p>Resource naming:</p> <ul> <li>Derived Service: <code>derived-&lt;adler32sum(ServiceExport name)&gt;</code></li> <li>EndpointSlice: <code>derived-&lt;adler32sum(ServiceExport name)&gt;-&lt;cluster-id&gt;</code></li> </ul> <p>Labels:</p> <ul> <li><code>multicluster.kubernetes.io/service-name</code>: Original service name (on both derived Service and EndpointSlice)</li> <li><code>kubernetes.io/service-name</code>: Derived service name (on EndpointSlice)</li> <li><code>multicluster.kubernetes.io/service-imported: \"true\"</code> (on derived Service)</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#prerequisites","title":"Prerequisites","text":"<ul> <li>Sveltos (and the Event Framework) installed in the management cluster.</li> <li>Source clusters labeled for selection; destination clusters labeled for consumption (examples below use <code>clusterset.k8s.io: environ-1</code> and <code>cluster.clusterset.k8s.io: &lt;cluster-id&gt;</code>).</li> <li>MCS CRDs present (<code>apiVersion: multicluster.x-k8s.io/v1alpha1</code>).</li> <li>Consistent namespaces across clusters namespace sameness.</li> <li>CoreDNS v1.12.2+ with the multicluster capability enabled.</li> <li>Flat Network connectivity: Pod IPs must be directly routable between clusters for this kube-proxy compatible implementation. The mirrored EndpointSlices contain actual Pod IPs that need to be reachable across cluster boundaries.</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#architecture","title":"Architecture","text":"<p>In the management cluster, Sveltos wires two event flows:</p> <ul> <li> <p>EventSource: <code>mcs-service-deriver</code>   Watches <code>ServiceExport</code> + <code>Service</code> and emits a derived Service.</p> </li> <li> <p>EventSource: <code>mcs-serviceimport-generator</code>   Watches the derived Service and emits a ServiceImport whose <code>spec.ips</code> is copied from the derived Service's <code>.spec.clusterIPs</code>.</p> </li> <li> <p>EventSource: <code>mcs-endpoint-mirror</code>   Watches <code>ServiceExport</code> + <code>EndpointSlice</code> and mirrors EndpointSlice data with required labels for DNS.</p> </li> </ul> <p>Each flow uses an EventTrigger plus a templated policy to render target resources in destination clusters. Hashing for derived names uses <code>adler32sum</code> over the ServiceExport name, producing stable <code>derived-&lt;hash&gt;</code> identifiers.</p>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#step-1-detect-serviceexports-services-then-create-derived-service","title":"Step 1: Detect ServiceExports &amp; Services, then create derived Service","text":"<p>This step normalizes <code>ClusterIP</code>/<code>NodePort</code>/<code>LoadBalancer</code> into a derived ClusterIP Service.</p> <p>EventSource + EventTrigger + Policy (derived Service)</p> <pre><code>cat &gt; eventsource-service-deriver.yaml &lt;&lt;'EOF'\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: mcs-service-deriver\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"multicluster.x-k8s.io\"\n    version: \"v1alpha1\"\n    kind: \"ServiceExport\"\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n  aggregatedSelection: | # lua\n    function evaluate()\n      local hs = {}\n      local collectedServices = {}\n      hs.message = \"\"\n\n      local serviceExports = {}\n      local services = {}\n\n      -- Categorize resources by type\n      for _, resource in ipairs(resources) do\n        local group = \"\"\n        if resource.apiVersion ~= nil then\n          local parts = {}\n          for part in string.gmatch(resource.apiVersion, \"[^/]+\") do\n            table.insert(parts, part)\n          end\n          if #parts &gt; 1 then\n            group = parts[1]\n          end\n        end\n\n        if resource.kind == \"ServiceExport\" and group == \"multicluster.x-k8s.io\" then\n          local key = resource.metadata.namespace .. \"/\" .. resource.metadata.name\n          serviceExports[key] = resource\n        elseif resource.kind == \"Service\" and group == \"\" then\n          table.insert(services, resource)\n        end\n      end\n\n      -- Process Services\n      for _, service in ipairs(services) do\n        local serviceKey = service.metadata.namespace .. \"/\" .. service.metadata.name\n\n        -- Check if a corresponding ServiceExport exists\n        if serviceExports[serviceKey] ~= nil then\n          -- Add ServiceExport information to the Service's labels\n          if service.metadata.labels == nil then\n            service.metadata.labels = {}\n          end\n          service.metadata.labels[\"service-export-name\"] = service.metadata.name\n          service.metadata.labels[\"service-export-namespace\"] = service.metadata.namespace\n          service.metadata.labels[\"multicluster.kubernetes.io/service-name\"] = service.metadata.name\n\n          table.insert(collectedServices, service)\n          hs.message = hs.message .. \"Found Service for ServiceExport: \" .. service.metadata.name .. \"\\n\"\n        end\n      end\n\n      if #collectedServices &gt; 0 then\n        hs.resources = collectedServices\n      end\n      return hs\n    end\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: mcs-service-deriver\nspec:\n  sourceClusterSelector:\n    matchExpressions:\n    - key: cluster.clusterset.k8s.io\n      operator: NotIn\n      values:\n      - \"\"\n    - key: clusterset.k8s.io\n      operator: In\n      values:\n      - environ-1\n  eventSourceName: mcs-service-deriver\n  oneForEvent: true\n  syncMode: ContinuousWithDriftDetection\n  policyRefs:\n  - name: mcs-service-deriver\n    namespace: mgmt\n    kind: Secret\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mcs-service-deriver\n  namespace: mgmt\n  annotations:\n    projectsveltos.io/instantiate: ok\ntype: addons.projectsveltos.io/cluster-profile\nstringData:\n  service.yaml: | # helm\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: derived-{{ .Resource.metadata.name | adler32sum }}\n      namespace: {{ .Resource.metadata.namespace }}\n      labels:\n        multicluster.kubernetes.io/service-name: {{ .Resource.metadata.name }}\n        multicluster.kubernetes.io/service-imported: \"true\"\n        app.kubernetes.io/managed-by: sveltos\n        {{- range $key, $value := .Resource.metadata.labels }}\n        {{- if ne $key `service-export-name` }}\n        {{- if ne $key `service-export-namespace` }}\n        {{ $key }}: {{ $value }}\n        {{- end }}\n        {{- end }}\n        {{- end }}\n    spec:\n      {{- if eq .Resource.spec.clusterIP `None` }}\n      clusterIP: None\n      {{- else }}\n      type: ClusterIP\n      {{- end }}\n      {{- if .Resource.spec.selector }}\n      selector:\n        {{- range $key, $value := .Resource.spec.selector }}\n        {{ $key }}: {{ $value }}\n        {{- end }}\n      {{- end }}\n      {{- if .Resource.spec.ports }}\n      ports:\n      {{- range .Resource.spec.ports }}\n      - name: {{ .name }}\n        port: {{ .port }}\n        {{- if .targetPort }}\n        targetPort: {{ .targetPort }}\n        {{- end }}\n        protocol: {{ .protocol }}\n      {{- end }}\n      {{- end }}\nEOF\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#step-2-create-serviceimport-from-the-derived-service","title":"Step 2: Create ServiceImport from the derived Service","text":"<p>This step reacts to the derived Service and generates the <code>ServiceImport</code>. For non-headless Services, <code>spec.ips</code> is copied from <code>.spec.clusterIPs</code>. For headless Services, <code>type: Headless</code> and <code>ips</code> is omitted.</p> <p>EventSource + EventTrigger + Policy (ServiceImport from derived Service)</p> <pre><code>cat &gt; eventsource-serviceimport-creator.yaml &lt;&lt;'EOF'\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: mcs-serviceimport-generator\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    labelFilters:\n    - key: \"multicluster.kubernetes.io/service-imported\"\n      operation: \"Equal\"\n      value: \"true\"\n    - key: \"app.kubernetes.io/managed-by\"\n      operation: \"Equal\"\n      value: \"sveltos\"\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: mcs-serviceimport-generator\nspec:\n  sourceClusterSelector:\n    matchExpressions:\n    - key: cluster.clusterset.k8s.io\n      operator: NotIn\n      values:\n      - \"\"\n    - key: clusterset.k8s.io\n      operator: In\n      values:\n      - environ-1\n  eventSourceName: mcs-serviceimport-generator\n  oneForEvent: true\n  syncMode: ContinuousWithDriftDetection\n  policyRefs:\n  - name: mcs-serviceimport-generator\n    namespace: mgmt\n    kind: Secret\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mcs-serviceimport-generator\n  namespace: mgmt\n  annotations:\n    projectsveltos.io/instantiate: ok\ntype: addons.projectsveltos.io/cluster-profile\nstringData:\n  serviceimport.yaml: | # helm\n    apiVersion: multicluster.x-k8s.io/v1alpha1\n    kind: ServiceImport\n    metadata:\n      name: {{ index .Resource.metadata.labels \"multicluster.kubernetes.io/service-name\" }}\n      namespace: {{ .Resource.metadata.namespace }}\n      annotations:\n        multicluster.kubernetes.io/derived-service: {{ .Resource.metadata.name }}\n    spec:\n      type: {{ if eq .Resource.spec.clusterIP `None` }}Headless{{ else }}ClusterSetIP{{ end }}\n      {{- if ne .Resource.spec.clusterIP `None` }}\n      ips:\n      {{- range .Resource.spec.clusterIPs }}\n      - {{ . }}\n      {{- end }}\n      {{- end }}\n      {{- if .Resource.spec.ports }}\n      ports:\n      {{- range .Resource.spec.ports }}\n      - name: {{ .name }}\n        port: {{ .port }}\n        protocol: {{ .protocol }}\n      {{- end }}\n      {{- end }}\nEOF\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#step-3-detect-endpointslices-mirror-them-for-dns","title":"Step 3: Detect EndpointSlices &amp; mirror them for DNS","text":"<p>This step watches <code>EndpointSlice</code> updates tied to exported Services and mirrors them into destination clusters. It ensures:</p> <ul> <li><code>kubernetes.io/service-name</code> is set to the derived Service (<code>derived-&lt;hash&gt;</code>).</li> <li>EndpointSlice name follows the pattern: <code>derived-&lt;hash&gt;-&lt;cluster-id&gt;</code>.</li> <li>Other labels (e.g., per-cluster identity) are preserved.</li> </ul> <p>EventSource + EventTrigger + Policy (EndpointSlice mirroring)</p> <pre><code>cat &gt; eventsource-endpoint-mirror.yaml &lt;&lt;'EOF'\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: mcs-endpoint-mirror\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"multicluster.x-k8s.io\"\n    version: \"v1alpha1\"\n    kind: \"ServiceExport\"\n  - group: \"discovery.k8s.io\"\n    version: \"v1\"\n    kind: \"EndpointSlice\"\n    labelFilters:\n    - key: \"kubernetes.io/service-name\"\n      operation: \"Different\"\n      value: \"\"\n  aggregatedSelection: | # lua\n    function evaluate()\n      local hs = {}\n      local collectedEndpointSlices = {}\n      hs.message = \"\"\n\n      local serviceExports = {}\n      local endpointSlices = {}\n\n      -- Categorize resources by type\n      for _, resource in ipairs(resources) do\n        local group = \"\"\n        if resource.apiVersion ~= nil then\n          local parts = {}\n          for part in string.gmatch(resource.apiVersion, \"[^/]+\") do\n            table.insert(parts, part)\n          end\n          if #parts &gt; 1 then\n            group = parts[1]\n          end\n        end\n\n        if resource.kind == \"ServiceExport\" and group == \"multicluster.x-k8s.io\" then\n          local key = resource.metadata.namespace .. \"/\" .. resource.metadata.name\n          serviceExports[key] = resource\n        elseif resource.kind == \"EndpointSlice\" and group == \"discovery.k8s.io\" then\n          table.insert(endpointSlices, resource)\n        end\n      end\n\n      -- Process EndpointSlices\n      for _, endpointSlice in ipairs(endpointSlices) do\n        if endpointSlice.metadata.labels ~= nil and\n          endpointSlice.metadata.labels[\"kubernetes.io/service-name\"] ~= nil then\n\n          local serviceName = endpointSlice.metadata.labels[\"kubernetes.io/service-name\"]\n          local serviceExportKey = endpointSlice.metadata.namespace .. \"/\" .. serviceName\n\n          -- Check if a corresponding ServiceExport exists\n          if serviceExports[serviceExportKey] ~= nil then\n            -- Add ServiceExport information to EndpointSlice labels\n            if endpointSlice.metadata.labels == nil then\n              endpointSlice.metadata.labels = {}\n            end\n            endpointSlice.metadata.labels[\"service-export-name\"] = serviceName\n            endpointSlice.metadata.labels[\"service-export-namespace\"] = endpointSlice.metadata.namespace\n            endpointSlice.metadata.labels[\"multicluster.kubernetes.io/service-name\"] = serviceExports[serviceExportKey].metadata.name\n\n            table.insert(collectedEndpointSlices, endpointSlice)\n            hs.message = hs.message .. \"Found EndpointSlice for ServiceExport: \" .. serviceName .. \"\\n\"\n          end\n        end\n      end\n\n      if #collectedEndpointSlices &gt; 0 then\n        hs.resources = collectedEndpointSlices\n      end\n      return hs\n    end\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: mcs-endpoint-mirror\nspec:\n  sourceClusterSelector:\n    matchExpressions:\n    - key: cluster.clusterset.k8s.io\n      operator: NotIn\n      values:\n      - \"\"\n    - key: clusterset.k8s.io\n      operator: In\n      values:\n      - environ-1\n  eventSourceName: mcs-endpoint-mirror\n  oneForEvent: true\n  syncMode: ContinuousWithDriftDetection\n  policyRefs:\n  - name: mcs-endpoint-mirror\n    namespace: mgmt\n    kind: Secret\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mcs-endpoint-mirror\n  namespace: mgmt\n  annotations:\n    projectsveltos.io/instantiate: ok\ntype: addons.projectsveltos.io/cluster-profile\nstringData:\n  endpointslice.yaml: |\n    apiVersion: discovery.k8s.io/v1\n    kind: EndpointSlice\n    metadata:\n      name: derived-{{ index .Resource.metadata.labels \"service-export-name\" | adler32sum }}-{{ index .Cluster.metadata.labels \"cluster.clusterset.k8s.io\" }}\n      namespace: {{ .Resource.metadata.namespace }}\n      labels:\n        {{- range $key, $value := .Resource.metadata.labels }}\n        {{- if eq $key \"kubernetes.io/service-name\" }}\n        {{ $key }}: derived-{{ $value | adler32sum }}\n        {{- else }}\n        {{ $key }}: {{ $value }}\n        {{- end }}\n        {{- end }}\n        multicluster.kubernetes.io/source-cluster: {{ index .Cluster.metadata.labels \"cluster.clusterset.k8s.io\" }}\n        endpointslice.kubernetes.io/managed-by: sveltos\n    addressType: {{ .Resource.addressType }}\n    {{- if .Resource.endpoints }}\n    endpoints:\n    {{- range .Resource.endpoints }}\n    - addresses:\n      {{- range .addresses }}\n      - {{ . }}\n      {{- end }}\n      {{- if .conditions }}\n      conditions:\n        ready: {{ .conditions.ready }}\n        serving: {{ .conditions.serving }}\n        terminating: {{ .conditions.terminating }}\n      {{- end }}\n    {{- end }}\n    {{- end }}\n    {{- if .Resource.ports }}\n    ports:\n    {{- range .Resource.ports }}\n    - name: {{ .name }}\n      port: {{ .port }}\n      protocol: {{ .protocol }}\n    {{- end }}\n    {{- end }}\nEOF\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#step-4-configure-coredns-for-multi-cluster-dns","title":"Step 4: Configure CoreDNS for Multi-Cluster DNS","text":"<p>To enable DNS resolution for <code>clusterset.local</code> domain, CoreDNS needs to be configured with the kubernetes plugin. This configuration is based on Cilium's MCS-API prerequisites.</p>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#update-coredns-version","title":"Update CoreDNS Version","text":"<p>First, ensure CoreDNS is at version 1.12.2 or later which includes multi-cluster capability:</p> <pre><code>kubectl -n kube-system set image deployment/coredns coredns=registry.k8s.io/coredns/coredns:v1.12.2\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#add-rbac-for-serviceimports","title":"Add RBAC for ServiceImports","text":"<p>CoreDNS needs permissions to read ServiceImports:</p> <pre><code># Create ClusterRole for reading ServiceImports\nkubectl create clusterrole coredns-mcsapi \\\n   --verb=list,watch --resource=serviceimports.multicluster.x-k8s.io\n\n# Bind the role to CoreDNS service account\nkubectl create clusterrolebinding coredns-mcsapi \\\n   --clusterrole=coredns-mcsapi --serviceaccount=kube-system:coredns\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#configure-coredns-corefile","title":"Configure CoreDNS Corefile","text":"<p>Update the CoreDNS ConfigMap to add <code>clusterset.local</code> zone and enable the multicluster plugin:</p> <pre><code># Update CoreDNS configuration\nkubectl get configmap -n kube-system coredns -o yaml | \\\n   sed -e 's/cluster\\.local/cluster.local clusterset.local/g' | \\\n   sed -E 's/^(.*)kubernetes(.*)\\{/\\1kubernetes\\2{\\n\\1   multicluster clusterset.local/' | \\\n   kubectl replace -f-\n</code></pre> <p>This configuration:</p> <ul> <li>Adds <code>clusterset.local</code> to the DNS zones handled by CoreDNS</li> <li>Enables the <code>multicluster</code> plugin for the <code>clusterset.local</code> zone</li> <li>Maintains backward compatibility with existing <code>cluster.local</code> resolution</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#apply-configuration","title":"Apply Configuration","text":"<p>Roll out the CoreDNS deployment to apply the changes:</p> <pre><code>kubectl rollout restart deployment -n kube-system coredns\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#verification","title":"Verification","text":"<p>After configuration, services exported via ServiceExport/ServiceImport will be resolvable at:</p> <ul> <li><code>&lt;service-name&gt;.&lt;namespace&gt;.svc.clusterset.local</code> - for ClusterSetIP services</li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#concrete-transformation-examples","title":"Concrete Transformation Examples","text":"","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#clusterip-service-example","title":"ClusterIP Service Example","text":"","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#source-cluster-cluster-a-original-resources","title":"Source Cluster (cluster-a): Original Resources","text":"<pre><code># Original Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: web\n  ports:\n    - name: http\n      port: 80\n      targetPort: 8080\n---\n# ServiceExport\napiVersion: multicluster.x-k8s.io/v1alpha1\nkind: ServiceExport\nmetadata:\n  name: web-service\n  namespace: default\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#destination-clusters-generated-resources","title":"Destination Clusters: Generated Resources","text":"<pre><code># Derived Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: derived-3d8f2a9c  # adler32sum(\"web-service\")\n  namespace: default\n  labels:\n    multicluster.kubernetes.io/service-name: web-service\n    multicluster.kubernetes.io/service-imported: \"true\"\n    app.kubernetes.io/managed-by: sveltos\nspec:\n  type: ClusterIP\n  selector:\n    app: web\n  ports:\n    - name: http\n      port: 80\n      targetPort: 8080\n---\n# ServiceImport\napiVersion: multicluster.x-k8s.io/v1alpha1\nkind: ServiceImport\nmetadata:\n  name: web-service\n  namespace: default\n  annotations:\n    multicluster.kubernetes.io/derived-service: derived-3d8f2a9c\nspec:\n  type: ClusterSetIP\n  ips: [\"10.96.0.120\"]\n  ports:\n    - name: http\n      port: 80\n      protocol: TCP\n---\n# EndpointSlice\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\n  name: derived-3d8f2a9c-cluster-a\n  namespace: default\n  labels:\n    kubernetes.io/service-name: derived-3d8f2a9c\n    multicluster.kubernetes.io/service-name: web-service\n    endpointslice.kubernetes.io/managed-by: sveltos\naddressType: IPv4\nports:\n  - name: http\n    port: 80\n    protocol: TCP\nendpoints:\n  - addresses: [\"10.0.1.1\", \"10.0.1.2\"]\n    conditions:\n      ready: true\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#headless-service-example","title":"Headless Service Example","text":"","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#source-cluster-original-resources","title":"Source Cluster: Original Resources","text":"<pre><code># Headless Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: stateful-service\n  namespace: default\nspec:\n  clusterIP: None\n  selector:\n    app: stateful\n  ports:\n    - name: http\n      port: 80\n---\n# ServiceExport\napiVersion: multicluster.x-k8s.io/v1alpha1\nkind: ServiceExport\nmetadata:\n  name: stateful-service\n  namespace: default\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#destination-clusters-generated-resources_1","title":"Destination Clusters: Generated Resources","text":"<pre><code># Derived Headless Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: derived-4b7e3f8a  # adler32sum(\"stateful-service\")\n  namespace: default\n  labels:\n    multicluster.kubernetes.io/service-name: stateful-service\n    multicluster.kubernetes.io/service-imported: \"true\"\n    app.kubernetes.io/managed-by: sveltos\nspec:\n  clusterIP: None\n  selector:\n    app: stateful\n  ports:\n    - name: http\n      port: 80\n---\n# ServiceImport\napiVersion: multicluster.x-k8s.io/v1alpha1\nkind: ServiceImport\nmetadata:\n  name: stateful-service\n  namespace: default\n  annotations:\n    multicluster.kubernetes.io/derived-service: derived-4b7e3f8a\nspec:\n  type: Headless\n  ports:\n    - name: http\n      port: 80\n      protocol: TCP\n---\n# EndpointSlice for DNS resolution\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\n  name: derived-4b7e3f8a-cluster-a\n  namespace: default\n  labels:\n    kubernetes.io/service-name: derived-4b7e3f8a\n    multicluster.kubernetes.io/service-name: stateful-service\n    endpointslice.kubernetes.io/managed-by: sveltos\naddressType: IPv4\nports:\n  - name: http\n    port: 80\n    protocol: TCP\nendpoints:\n  - addresses: [\"10.0.2.1\"]\n    conditions:\n      ready: true\n</code></pre>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#behavior-notes","title":"Behavior &amp; Notes","text":"<ul> <li> <p>Service types</p> </li> <li> <p><code>ClusterIP</code> / <code>NodePort</code> / <code>LoadBalancer</code> \u2192 derived ClusterIP Service + <code>ServiceImport(type: ClusterSetIP)</code>.</p> </li> <li>Headless (<code>clusterIP: None</code>) \u2192 derived Headless Service + <code>ServiceImport(type: Headless)</code> and mirrored EndpointSlice data to back DNS.</li> <li> <p>ExternalName: not exported (out of scope for this flow).</p> </li> <li> <p>DNS (CoreDNS)   DNS configuration is delegated to CoreDNS. The multicluster capability is available in CoreDNS v1.12.2 and later; enable/configure it in your CoreDNS deployment.   Note: If you are not relying on Multi-Cluster DNS names for service discovery, the CoreDNS multicluster capability is not required. For headless Services, without Multi-Cluster DNS you will not get cross-cluster A/AAAA/SRV records out of the box.</p> </li> <li> <p>Labels</p> </li> <li> <p>Both derived Service and mirrored EndpointSlice include <code>multicluster.kubernetes.io/service-name: &lt;clusterset service&gt;</code>.</p> </li> <li>EndpointSlice includes <code>kubernetes.io/service-name: &lt;derived service name&gt;</code> for kube-proxy.</li> <li> <p>Additional labels from source are preserved (except internal helper labels used during templating).</p> </li> <li> <p>Hashing</p> </li> <li> <p>Derived names use <code>adler32sum</code> of the ServiceExport name: <code>derived-&lt;hash&gt;</code>. This produces stable, compact identifiers.</p> </li> <li> <p>EndpointSlice names include cluster identifier: <code>derived-&lt;hash&gt;-&lt;cluster-id&gt;</code>.</p> </li> <li> <p>Selector Maintenance</p> </li> <li> <p>All imported Services maintain their selectors.</p> </li> <li> <p>This enables Pods with matching labels in the importing cluster to be automatically added as service endpoints.</p> </li> <li> <p>Status</p> </li> <li> <p>Conditions on <code>ServiceExport</code> (e.g., invalid types, headless/non-headless conflicts, per-cluster availability) are not set by the snippets above. Implement these via a lightweight controller or an additional Event flow that writes back <code>status</code>.</p> </li> <li> <p>ServiceImport IPs field</p> </li> <li> <p>The <code>ips</code> field in <code>ServiceImport</code> is automatically populated by the second Event flow, copying from the derived Service's <code>.spec.clusterIPs</code>.</p> </li> </ul>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/mcs_api/#result","title":"Result","text":"<ul> <li>Exported Services are discoverable across clusters with minimal configuration.</li> <li>Consumers resolve via ServiceImport (<code>ClusterSetIP</code> or headless DNS) backed by synchronized EndpointSlices.</li> <li>The entire pipeline is event-driven, declarative.</li> </ul> <p>This pattern accelerates multi-cluster adoption for HA, traffic shaping, and cross-environment integration while keeping the implementation open and extensible.</p>","tags":["Kubernetes","Sveltos","event driven","multi-cluster","MCS","KEP-1645"]},{"location":"events/examples/otel/","title":"Example - OpenTelemetry Collector data to Splunk","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/otel/#introduction","title":"Introduction","text":"<p>The example demonstrates how the Sveltos Event Framework, combined with dynamic templating, allows us to collect pod metrics and logs from dedicated Kubernetes namespaces. The data is then securely forwarded to a Splunk instance using the HTTP Event Collector (HEC).</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/otel/#scenario","title":"Scenario","text":"<p>An <code>EventSource</code> is configured to detect namespaces within different Sveltos managed clusters that do not have the <code>otel-exempt: true</code> label set and whose names are not present in a predefined <code>ignore_list = {\"kube-system\", \"another\", \"more\", \"etc\"}</code>. When such a namespace is detected, Sveltos triggers an Event. The <code>EventTrigger</code> creates a <code>ConfigMap</code> containing the necessary OTEL configuration details within the <code>projectsveltos</code> namespace of the management cluster. This approach automates the setup of OTEL for relevant namespaces. Let's dive into the details.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/otel/#eventsource","title":"EventSource","text":"<p>To define the logic based on the use case, we used Lua. This is how the <code>EventSource</code> resource looks. More information about Lua can be found here.</p> <p>EventSource namespace-watcher</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: namespace-watcher\nspec:\n  collectResources: false\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Namespace\"\n    evaluate: |\n      function isLabelInExcludedList(current_value)\n        local ignore_list = {\"kube-system\", \"another\", \"more\", \"etc\"}\n\n        for _, value in ipairs(ignore_list) do\n          if current_value == value then\n            return true\n          end\n        end\n        return false\n      end\n\n      function evaluate()\n        hs = {}\n        hs.matching = true\n        hs.message = \"\"\n        if obj.metadata.labels ~= nil then\n          for key, value in pairs(obj.metadata.labels) do\n            -- exclude all namaspaces with label otel-exempt: true\n            if key == \"otel-exempt\" then\n              if value == \"true\" then\n                hs.matching = false\n                break\n              end\n            elseif key == \"kubernetes.io/metadata.name\" then\n              -- exclude namespaces with certain names. List is defined in the function\n              if isLabelInExcludedList(value) then\n                hs.matching = false\n                break\n              end\n            end\n          end\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/otel/#eventtrigger","title":"EventTrigger","text":"<p>Once we have a matching namespace, the <code>EventTrigger</code> performs the following actions.</p> <ul> <li>On the management cluster, create a <code>ConfigMap</code> for every managed cluster matching the <code>sourceClusterSelector</code></li> <li>The <code>ConfigMap</code> has the name format <code>{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}-cluster-receiver-value</code>, and is located in the <code>projectsveltos</code> namespace</li> <li>The <code>ConfigMap</code> contains the necessary OTEL Collector details and Sveltos ensure the resource is kept up to date</li> </ul> <p>EventTrigger otel</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: otel\nspec:\n  sourceClusterSelector:\n    matchExpressions:\n      - {key: env, operator: In, values: [\"dev\", \"development\", \"test\", \"testing\"]}\n      - {key: exempt, operator: NotIn, values: [\"true\"]}\n  eventSourceName: namespace-watcher\n  destinationCluster:\n    name: mgmt\n    namespace: mgmt\n    kind: SveltosCluster\n    apiVersion: lib.projectsveltos.io/v1beta1\n  oneForEvent: false\n  configMapGenerator:\n  - name: cluster-receiver-value\n    namespace: default\n    nameFormat: \"{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}-cluster-receiver-value\"\n</code></pre> <p>Note</p> <p>For the example to work, the <code>ConfigMap</code> named cluster-receiver-value needs to be deployed to the management cluster.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/otel/#configmap","title":"ConfigMap","text":"<p>ConfigMap cluster-receiver-value</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-receiver-value\n  namespace: default\n  annotations:\n    projectsveltos.io/instantiate: ok\ndata:\n  cluster-receiver-value.yaml: |\n      clusterReceiver:\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n\n        extraEnvs:\n          {{ range .MatchingResources }}\n          - name: \"SPLUNK_{{ .Name }}_HEC_TOKEN\"\n            valueFrom:\n              secretKeyRef:\n                name: \"{{ .Name }}-access-token\"\n                key: splunk_platform_hec_token\n          {{ end }}\n\n        config:\n          receivers:\n            prometheus:\n              config:\n                scrape_configs:\n                  - job_name: \"kubernetes-pods\"\n                    kubernetes_sd_configs:\n                      - role: pod\n                    relabel_configs:\n                      - action: labelmap\n                        regex: __meta_kubernetes_pod_label_(.+)\n                      - source_labels: [__meta_kubernetes_namespace]\n                        action: replace\n                        target_label: namespace\n                      - source_labels: [__meta_kubernetes_pod_name]\n                        action: replace\n                        target_label: pod\n\n          exporters:\n            {{ range .MatchingResources }}\n            splunk_hec/{{ .Name }}:\n              index: \"{{ .Name }}\"\n              source: kubernetes\n              splunk_app_name: splunk-otel-collector\n              endpoint: https://example.splunkcloud.com:443/services/collector/event\n              token: \"$SPLUNK_{{ .Name }}_HEC_TOKEN\"\n            {{ end }}\n\n          connectors:\n            routing/logs:\n              default_pipelines: [logs/other]\n              table:\n                {{ range .MatchingResources }}\n                - context: resource\n                  condition: attributes[\"k8s.namespace.name\"] == {{ .Name }}\n                  pipelines: \"[logs/{{ .Name }}]\"\n                {{ end }}\n\n          service:\n            pipelines:\n              metrics:\n                receivers:\n                  - k8s_cluster\n                  - prometheus\n              {{ range .MatchingResources }}\n              logs/{{ .Name }}:\n                receivers:\n                  - routingconnector/logs\n                exporters:\n                  - splunk_hec/{{ .Name }}\n              {{ end }}\n</code></pre> <p>Every time we have a matching condition, Sveltos can dynamically update the cluster-receiver-value <code>ConfigMap</code> with information found in the managed cluster. The new <code>ConfigMap</code> will be located in the <code>projectsveltos</code> namespace with the name <code>{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}-cluster-receiver-value</code>.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/otel/#conclusion","title":"Conclusion","text":"<p>The demonstrated approach shows how Sveltos effectively automates the dynamic configuration of OTEL across a multi-cluster environment. By identifying and provisioning OTEL details for the namespace of interest, it smooths the observability setup, ensuring efficient and scalable data collection to Splunk. This method reduces manual overhead for managing monitoring configurations.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/secrets_on_demand/","title":"Example - Create Secret on demand using EventTrigger","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/secrets_on_demand/#introduction","title":"Introduction","text":"<p>The example demonstrates a dynamic replication of a Kubernetes\u00a0<code>Secret</code>\u00a0to any production cluster in a defined namespace. If you are not familiar with the EventTrigger feature or the Sveltos Generators, check out the mentioned guides before proceeding.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/secrets_on_demand/#example-replicate-a-secret-on-demand","title":"Example: Replicate a Secret on Demand","text":"<p>For a dynamic <code>Secret</code> replication, we will establish a system that reacts to newly created namespaces requiring credentials. Initially, an <code>EventSource</code> will monitor for namespaces labeled <code>secret: required</code> within the production Kubernetes clusters. Upon detection, the system will retrieve its name and create a corresponding resource in the Kubernetes management cluster, storing the information using a <code>ConfigMapGenerator</code>.</p> <p>The <code>EventTrigger</code> will then generate a Sveltos <code>ClusterProfile</code>. The ClusterProfile will reference the newly created resource containing the namespace information and the login-credentials Secret from the default namespace of the Kubernetes management cluster. Finally, the ClusterProfile will dynamically fetch the referenced resources, extract the necessary data, and replicate the login-credentials Secret into the identified namespaces within the production clusters.</p> <p>EventSource</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: requiring-credentials\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Namespace\"\n    labelFilters:\n    - key: secret\n      operation: Equal\n      value: required\n</code></pre> <p>EventTrigger</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: distribute-credentials\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: production\n  eventSourceName: requiring-credentials\n  configMapGenerator: # Generates a ConfigMap named after the cluster, storing the namespaces retrieved by Sveltos from the event data.\n  - name: namespaces\n    namespace: default\n    nameFormat: \"{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}-namespaces\"\n  templateResourceRefs:\n  - resource: # This refers to the Secret in the management cluster containing the credentials\n      apiVersion: v1\n      kind: Secret\n      name: login-credentials\n      namespace: default\n    identifier: Credentials\n  - resource: # This refers to the resource that Sveltos dynamically generates using ConfigMapGenerator.\n      apiVersion: v1\n      kind: ConfigMap\n      name: \"{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}-namespaces\"\n      namespace: projectsveltos\n    identifier: Namespaces\n  policyRefs:\n  - kind: ConfigMap\n    name: info\n    namespace: default\n</code></pre> <p>Referenced ConfigMaps</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: namespaces\n  namespace: default\n  annotations: # This annotation indicates Sveltos to instantiate it using Event data, i.e, the namespaces requiring the credentials\n    projectsveltos.io/instantiate: ok\ndata:\n  namespaces: |\n    {{- range $v := .MatchingResources }}\n      {{ $v.Name }}: \"ok\"\n    {{- end }}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: info\n  namespace: default\n  annotations: # This annotation indicates Sveltos the content is a template that needs to be instantiated using resources fetched in TemplateResourceRefs\n    projectsveltos.io/template: ok\ndata:\n  secret.yaml: |\n    {{ $namespaces := ( ( index (getResource \"Namespaces\").data \"namespaces\" ) | fromYaml ) }}\n    {{- range $key, $value := $namespaces }}\n        apiVersion: v1\n        kind: Secret\n        metadata:\n          namespace: {{ $key }}\n          name: {{ (getResource \"Credentials\").metadata.name }}\n        data:\n          {{- range $secretKey, $secretValue := (getResource \"Credentials\").data }}\n            {{ $secretKey }} : {{ $secretValue }}\n          {{- end }}\n    ---\n    {{- end }}\n</code></pre> <p></p> <p>Imagine we have a production cluster named workload residing in the default namespace. Within this cluster, two namespaces, eng and hr, are labeled secret: required.</p> <p>Sveltos will detect that and generate a <code>ConfigMap</code> in the <code>projectsveltos</code> namespace. The ConfigMap, named <code>&lt;cluster namespace&gt;-&lt;cluster name&gt;-namespaces</code> (in this case, default-workload-namespaces), stores the identified namespaces as follows.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  ...\n  name: default-workload-namespaces\n  namespace: projectsveltos\ndata:\n  namespaces: |\n    eng: \"ok\"\n    hr: \"ok\"\n</code></pre> <p>Outcome: The <code>Secret</code> is replicated to the <code>env</code> and <code>hr</code> namespaces.</p> <pre><code>$ sveltosctl show addons\n+-----------------------------+---------------+-----------+-------------------+---------+-------------------------------+---------------------------------------------+\n|           CLUSTER           | RESOURCE TYPE | NAMESPACE |       NAME        | VERSION |             TIME              |                  PROFILES                   |\n+-----------------------------+---------------+-----------+-------------------+---------+-------------------------------+---------------------------------------------+\n| default/workload            | :Secret       | eng       | login-credentials | N/A     | 2025-02-28 14:23:08 +0100 CET | ClusterProfile/sveltos-lbh9me2lr77gokea2u5u |\n| default/workload            | :Secret       | hr        | login-credentials | N/A     | 2025-02-28 14:23:08 +0100 CET | ClusterProfile/sveltos-lbh9me2lr77gokea2u5u |\n+-----------------------------+---------------+-----------+-------------------+---------+-------------------------------+---------------------------------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Secrets Example"]},{"location":"events/examples/service_event_trigger/","title":"Example Service Event Trigger - Project Sveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/service_event_trigger/#example-service-event-trigger","title":"Example: Service Event Trigger","text":"<p>In this example, we want to create an Ingress in the namespace <code>eng</code> as soon as at least one Service is created exposing the HTTPS port.</p> <p>The below EventSource instance will match any Service in namespace eng exposing either port 443 or port 8443.</p> <p>EventSource Definition</p> <pre><code>cat &gt; eventsource.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: https-service\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: \"v1\"\n    kind: \"Service\"\n    namespace: eng\n  evaluate: |\n    function evaluate()\n      hs = {}\n      hs.matching = false\n      if obj.spec.ports ~= nil then\n        for _,p in pairs(obj.spec.ports) do\n          if p.port == 443 or p.port == 8443 then\n            hs.matching = true\n          end\n        end\n      end\n      return hs\n    end\nEOF\n</code></pre> <p>The below EventTrigger instance is referencing the EventSource instance defined above, and it is referencing a ConfigMap containing a template for an Ingress resource.</p> <p>Note</p> <p>The oneForEvent field is set to <code>false</code> and instructs Sveltos to create a single Ingress for all Service instances in the managed cluster matching the EventSource.</p> <p>When oneForEvent is set to <code>false</code>, when instantiating the Ingress template, Resources is an array containing all Services in the managed cluster matching the EventSource. Any field can be accessed.</p> <p>EventTrigger Definition</p> <pre><code>cat &gt; eventtrigger.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: ingress-configuration\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      env: fv\n  eventSourceName: https-service\n  oneForEvent: false\n  policyRefs:\n  - name: ingress\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ingress\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  ingress.yaml: |\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    metadata:\n      name: ingress\n      namespace: default\n      annotations:\n        nginx.ingress.kubernetes.io/rewrite-target: /\n    spec:\n      ingressClassName: http-ingress\n      rules:\n        - http:\n            paths:\n            {{ range $resource := .Resources }}\n            - path: /{{ .metadata.name }}\n              pathType: Prefix\n              backend:\n                service:\n                  name: {{ .metadata.name }}\n                  port:\n                    {{ range .spec.ports }}\n                    {{ if or (eq .port 443 ) (eq .port 8443 ) }}\n                    number: {{ .port }}\n                    {{ end }}\n                    {{ end }}\n            {{ end }}\nEOF\n</code></pre> <p>If we have two Service instance in the managed cluster in the namespace <code>eng</code></p> <pre><code>$ kubectl get service -n eng\nNAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)           AGE\nmy-service     ClusterIP   10.225.83.46   &lt;none&gt;        80/TCP,443/TCP    15m\nmy-service-2   ClusterIP   10.225.108.8   &lt;none&gt;        80/TCP,8443/TCP   14m\n</code></pre> <p>Sveltos will create below Ingress instance.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    projectsveltos.io/hash: sha256:bc1e74450d20acedefca38f20cb998b7b24c12ac34e4b501d19b617568926140\n  creationTimestamp: \"2023-03-16T16:35:11Z\"\n  generation: 1\n  labels:\n    projectsveltos.io/reference-kind: ConfigMap\n    projectsveltos.io/reference-name: sveltos-l6hldpydjngao4r23evm\n    projectsveltos.io/reference-namespace: projectsveltos\n  name: ingress\n  namespace: default\n  ownerReferences:\n  - apiVersion: config.projectsveltos.io/v1beta1\n    kind: ClusterProfile\n    name: sveltos-rgdn6jsy9zivek7e9mtz\n    uid: 29f3552b-be4b-447f-bfc0-aedbad5b21db\n  resourceVersion: \"6186\"\n  uid: 080a6713-4da1-45a4-b189-2ded216fc688\nspec:\n  ingressClassName: http-ingress\n  rules:\n  - http:\n      paths:\n      - backend:\n          service:\n            name: my-service\n            port:\n              number: 443\n        path: /my-service\n        pathType: Prefix\n      - backend:\n          service:\n            name: my-service-2\n            port:\n              number: 8443\n        path: /my-service-2\n        pathType: Prefix\nstatus:\n  loadBalancer: {}\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","event driven"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/","title":"Example \u2013 Sveltos Event Framework Combined With Crossplane To Achieve the GitOps Bridge Pattern","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#introduction","title":"Introduction","text":"<p>The example demonstrates how to integrate the Sveltos Event Framework together with the Crossplane provider for AWS to achieve the GitOps Bridge Pattern in a Kubernetes environment.</p> <p>Crossplane is an open-source, CNCF tool that allows users to manage and provision cloud infrastructure using Kubernetes-style configuration files across multiple environments. For more information about Crossplane, check out the official website.</p> <p>GitOps Bridge Pattern enables Kubernetes administrators to utilise Infrastructure as Code (IaC) and GitOps tools to deploy Kubernetes add-ons and applications across a fleet of clusters. Go through the GitHub repository to learn more about the pattern and its purpose.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster acting as the management cluster</li> <li>A GitOps controller is already available in the management cluster</li> <li>Familiarity with the Sveltos Event Framework. If you are new to Sveltos, take a look at the \"Quick Start\" guide here</li> <li>Basic familiarity with Crossplane</li> <li>Fundamental knowledge of GitOps practices</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#how-does-it-work","title":"How does it work?","text":"<p>To achieve the GitOps Bridge Pattern, a GitOps controller such as ArgoCD or Flux is deployed in the Sveltos management cluster. The YAML manifests (described in the sections below) are stored in a Git repository. The GitOps controller continuously monitors this repository and automatically applies any changes, ensuring the manifests in the Sveltos management cluster are always in sync with Git. This approach guarantees that the add-on deployments for managed clusters are versioned and auditable. Sveltos uses the updated resources to take care of the add-on deployment across the managed clusters. Sveltos has an integration with Flux; for more information, take a look here.</p> <p>As part of the flow, Crossplane provisions the required infrastructure (e.g., IAM Role), and once ready, Sveltos observes it. Sveltos pre-instantiates the manifests deployed and performs actions only when the required infrastructure is ready. For more details about how the <code>EventSource</code> and <code>EventTrigger</code> work, have a look here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#gitops","title":"GitOps","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#managed-cluster-resources","title":"Managed Cluster Resources","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#key-objectives","title":"Key Objectives","text":"<ul> <li>Listen for Infrastructure Events: Use Sveltos <code>EventSource</code> and <code>EventTrigger</code> to monitor infrastructure events.</li> <li>Copy Infrastructure Details: Synchronise relevant infrastructure details to the management cluster</li> <li>Deploy IAM Roles for <code>external-dns</code> service: Provision Identity and Access Management (IAM) roles using Crossplane for the <code>external-dns</code> service</li> <li>Trigger <code>external-dns</code> Deployment: Ensure <code>external-dns</code> is only deployed once the IAM roles are ready.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#advantages","title":"Advantages","text":"<p>The proposed pattern lets us use Sveltos to collect infrastructure details from the managed clusters and pass that information to Crossplane, which then deploys other resources like IAM Roles. This is useful for add-ons that need infrastructure components in place before they can be installed.</p> <p>It\u2019s like laying the foundation before building, ensuring all necessary infrastructure is in place and making add-on deployment smooth and reliable. The approach easily scales to support any number of add-ons that need infrastructure to be ready ahead of time.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#configuration-details","title":"Configuration Details","text":"<p>The <code>iam_config.yaml</code> and <code>infra_details.yaml</code> files need to be deployed in the Sveltos management clusters. Effectively, the Kubernetes cluster Sveltos is deployed.</p> <p>Example - IAM Roles Configuration</p> <pre><code>cat &gt; iam_config.yaml &lt;&lt;'EOF'\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: iam-roles\nspec:\n  collectResources: true\n  resourceSelectors:\n    - group: \"iam.aws.upbound.io\"\n      version: \"v1beta1\"\n      kind: \"Role\"\n      evaluateCEL:\n        - name: \"externalDnsRole\"\n          rule: has(resource.status.atProvider.arn) &amp;&amp; resource.metadata.name == \"external-dns-role\"\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: iam-roles\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      environment: landing-zone\n  eventSourceName: iam-roles\n  oneForEvent: false\n  syncMode: ContinuousWithDriftDetection\n  secretGenerator:\n    - name: iam-roles\n      namespace: projectsveltos\n      nameFormat: \"{{ .Cluster.metadata.name }}-iam-roles\"\n  templateResourceRefs:\n    - resource:\n        apiVersion: v1\n        kind: Secret\n        name: \"{{ .Cluster.metadata.name }}-infra-details\"\n        namespace: projectsveltos\n      identifier: InfrastructureDetails\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: iam-roles\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/instantiate: \"true\"\ntype: addons.projectsveltos.io/cluster-profile\nstringData:\n  resources: |\n    {{- range $resource := .Resources }}\n    {{- if eq $resource.kind \"Role\" }}\n    \"{{ $resource.metadata.name }}\": {{ $resource.status.atProvider.arn}}\n    {{- end }}\n    {{- end }}\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: external-dns\nspec:\n  syncMode: ContinuousWithDriftDetection\n  templateResourceRefs:\n    - resource:\n        apiVersion: v1\n        kind: Secret\n        name: \"{{ .Cluster.metadata.name }}-iam-roles\"\n        namespace: projectsveltos\n      identifier: IamRoles\n  helmCharts:\n    - chartName: external-dns/external-dns\n      chartVersion: \"1.18.0\"\n      helmChartAction: Install\n      releaseName: external-dns\n      releaseNamespace: external-dns\n      repositoryName: external-dns\n      repositoryURL: https://kubernetes-sigs.github.io/external-dns/\n      valuesFrom:\n        - kind: ConfigMap\n          name: external-dns-default-values\n          namespace: projectsveltos\n          optional: true\n        - kind: ConfigMap\n          name: external-dns-{{- if index .Cluster.metadata.labels `environment` -}}{{- index .Cluster.metadata.labels `environment` -}}{{- end -}}-values\n          namespace: projectsveltos\n          optional: true\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: external-dns-landing-zone-values\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  values: |\n    {{- /* Get the generated Secret (IamRoles) */ -}}\n    {{- $resource := (getResource \"IamRoles\") -}}\n    {{- $raw := index $resource.data \"resources\" -}}\n    {{- $decoded := b64dec $raw | trim -}}\n    {{- $parsed := fromYaml $decoded | default (fromJson $decoded) -}}\n    {{- $externalDnsRoleArn := index $parsed \"external-dns-role\" -}}\n\n    provider: aws\n    serviceAccount:\n      name: \"external-dns-sa\"\n      annotations:\n        eks.amazonaws.com/role-arn: \"{{ $externalDnsRoleArn }}\"\n    domainFilters:\n      - {{ index $parsed \"hosted_zone_name\" }}\n    txtOwnerId: {{ .Cluster.metadata.name }}\n    policy: sync\nEOF\n</code></pre> <p>The above YAML snippet is an advanced use-case configuration, but this is what happens after deploying the YAML file to the Sveltos management cluster. The description starts following the top-to-bottom approach.</p> <ol> <li>Monitor for IAM Roles: Sveltos watches the managed clusters for IAM Role resources named <code>external-dns-role</code> that have an Amazon Resource Name (ARN) available</li> <li>Collect Role Details: When such a role is found in a cluster labelled as <code>environment=landing-zone</code>, Sveltos collects the required details</li> <li>Store in a Secret: The IAM Role information (like the ARN) is automatically copied into a Secret in the management cluster</li> <li>Trigger Addon Deployment: The Sveltos ClusterProfile named <code>external-dns</code> uses this Secret to deploy the <code>external-dns</code> Helm chart, ensuring it only happens when the IAM Role is ready</li> </ol> <p>The power of this setup? It ensures the <code>external-dns</code> is safely and automatically installed with the right permissions across all clusters that match the label <code>environment=landing-zone</code>.</p> <p>Example - Infrastructure Details Configuration</p> <pre><code>cat &gt; infra_details.yaml &lt;&lt;'EOF'\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\n  name: infrastructure-details\nspec:\n  collectResources: true\n  resourceSelectors:\n    - group: \"\"\n      version: \"v1\"\n      kind: \"Secret\"\n      name: \"infrastructure-details\"\n      namespace: kube-system\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: infrastructure-details\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      environment: landing-zone\n  eventSourceName: infrastructure-details\n  oneForEvent: false\n  syncMode: ContinuousWithDriftDetection\n  secretGenerator:\n    - name: infrastructure-details\n      namespace: projectsveltos\n      nameFormat: \"{{ .Cluster.metadata.name }}-infra-details\"\n  templateResourceRefs:\n    - resource:\n        apiVersion: v1\n        kind: Secret\n        name: \"{{ .Cluster.metadata.name }}-infra-details\"\n        namespace: projectsveltos\n      identifier: InfrastructureDetails\n  policyRefs:\n    - kind: ConfigMap\n      name: landing-zone-external-dns\n      namespace: projectsveltos\n    - kind: ConfigMap\n      name: landing-zone-external-secrets\n      namespace: projectsveltos\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: infrastructure-details\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/instantiate: \"true\"\ntype: addons.projectsveltos.io/cluster-profile\nstringData:\n  resources: |\n    {{- range $resource := .Resources }}\n    \"{{ $resource.metadata.name }}\": |\n      {{ toJson $resource | trim | indent 6 }}\n    {{- end }}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: landing-zone-external-dns\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  external-dns-role.yaml: |\n    {{- /* Get the generated Secret (InfrastructureDetails) */ -}}\n    {{- $resource := (getResource \"InfrastructureDetails\") -}}\n    {{- $raw := index $resource.data \"resources\" -}}\n    {{- $decoded := b64dec $raw | trim -}}\n    {{- $parsed := fromYaml $decoded | default (fromJson $decoded) -}}\n    {{- $res := index $parsed \"infrastructure-details\" | fromJson -}}\n    {{- $anns := index $res \"metadata\" \"annotations\" | default dict -}}\n    {{- $oidcProvider := index $anns \"oidc_provider\" | default \"not-found\" }}\n    {{- $oidcProviderArn := index $anns \"oidc_provider_arn\" | default \"not-found\" }}\n    apiVersion: iam.aws.upbound.io/v1beta1\n    kind: Role\n    metadata:\n      name: external-dns-role\n    spec:\n      forProvider:\n        assumeRolePolicy: |-\n          {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n              {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                  \"Federated\": \"{{ $oidcProviderArn }}\"\n                },\n                \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n                \"Condition\": {\n                  \"StringEquals\": {\n                    \"{{ $oidcProvider }}:aud\": \"sts.amazonaws.com\",\n                    \"{{ $oidcProvider }}:sub\": \"system:serviceaccount:external-dns:external-dns-sa\"\n                  }\n                }\n              }\n            ]\n          }\n  external-dns-rolepolicy.yaml: |\n    apiVersion: iam.aws.upbound.io/v1beta1\n    kind: RolePolicy\n    metadata:\n      name: external-dns-role\n    spec:\n      forProvider:\n        roleRef:\n          name: external-dns-role\n        policy: |\n          {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n              {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                  \"route53:ChangeResourceRecordSets\"\n                ],\n                \"Resource\": [\n                  \"arn:aws:route53:::hostedzone/*\"\n                ]\n              },\n              {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                  \"route53:ListHostedZones\",\n                  \"route53:ListResourceRecordSets\",\n                  \"route53:ListTagsForResource\"\n                ],\n                \"Resource\": [\n                  \"*\"\n                ]\n              }\n            ]\n          }\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: landing-zone-external-secrets\n  namespace: projectsveltos\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  external-secrets-role.yaml: |\n    apiVersion: iam.aws.upbound.io/v1beta1\n    kind: Role\n    metadata:\n      name: external-secrets-role\n    spec:\n      forProvider:\n        assumeRolePolicy: |-\n          {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n              {\n                \"Sid\": \"AllowEksAuthToAssumeRoleForPodIdentity\",\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                  \"Service\": \"pods.eks.amazonaws.com\"\n                },\n                \"Action\": [\n                  \"sts:AssumeRole\",\n                  \"sts:TagSession\"\n                ]\n              }\n            ]\n          }\n  external-secrets-rolepolicy.yaml: |\n    apiVersion: iam.aws.upbound.io/v1beta1\n    kind: RolePolicy\n    metadata:\n      name: external-secrets-role\n    spec:\n      forProvider:\n        roleRef:\n          name: external-secrets-role\n        policy: |\n          {\n            \"Statement\": [\n              {\n                \"Action\": \"ssm:DescribeParameters\",\n                \"Effect\": \"Allow\",\n                \"Resource\": \"*\"\n              },\n              {\n                \"Action\": [\n                  \"ssm:GetParameters\",\n                  \"ssm:GetParameter\"\n                ],\n                \"Effect\": \"Allow\",\n                \"Resource\": \"arn:aws:ssm:*:*:parameter/platform/*\"\n              },\n              {\n                \"Action\": \"secretsmanager:ListSecrets\",\n                \"Effect\": \"Allow\",\n                \"Resource\": \"*\"\n              },\n              {\n                \"Action\": [\n                  \"secretsmanager:ListSecretVersionIds\",\n                  \"secretsmanager:GetSecretValue\",\n                  \"secretsmanager:GetResourcePolicy\",\n                  \"secretsmanager:DescribeSecret\",\n                  \"secretsmanager:BatchGetSecretValue\"\n                ],\n                \"Effect\": \"Allow\",\n                \"Resource\": \"arn:aws:secretsmanager:*:*:secret:/platform/*\"\n              },\n              {\n                \"Action\": \"kms:Decrypt\",\n                \"Effect\": \"Allow\",\n                \"Resource\": \"arn:aws:kms:*:*:key/*\"\n              }\n            ],\n            \"Version\": \"2012-10-17\"\n          }\nEOF\n</code></pre> <p>Continuing with the infrastructure details, this is what happens after deploying the YAML file to the Sveltos management cluster. The description starts following the top-to-bottom approach.</p> <ol> <li>Watch for Infrastructure Details: Sveltos monitors the managed clusters for the Secret named <code>infrastructure-details</code> in the <code>kube-system</code> namespace</li> <li>Collect and Copy Information: When the Secret is found in clusters with the label set to <code>environment=landing-zone</code>, Sveltos gathers its contents and copies them into a new Secret in the management cluster</li> <li>Make Details Available to Addons: This copied Secret provides infrastructure details which are required for the setup</li> <li>Trigger Policy Templates: The configuration references ConfigMaps containing templates for IAM roles and policies for addons like <code>external-dns</code> and <code>external-secrets</code></li> </ol> <p>The power of Sveltos' advanced Templating allows Kubernetes administrators to automatically generate and configure the correct IAM roles and policies for each cluster, ensuring addons have the permissions and settings they require.</p> <pre><code>$ export KUBECONFIG=&lt;/dir/to/Sveltos/management/cluster/kubeconfig&gt;\n$ kubectl apply -f iam_config.yaml,infra_details.yaml\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#rendered-output-secret","title":"Rendered Output Secret","text":"<pre><code># Example of generated Secret (for IAM roles)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: \"example-iam-roles\"\n  namespace: projectsveltos\ntype: addons.projectsveltos.io/cluster-profile\nstringData:\n  resources: |\n    \"external-dns-role\": arn:aws:iam::1234567890:role/external-dns-role\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"events/examples/sveltos_crossplane_gitops_bridge_pattern/#further-reading","title":"Further Reading","text":"<ol> <li>Sveltos Event Framework Templating</li> <li>Sveltos Example Workflow</li> <li>Next-Level Kubernetes Deployment and Management: Sveltos and Flux in Action</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Event Driven","Generators","Crossplane","GitOps Bridge"]},{"location":"features/configuration_drift/","title":"Sveltos Configuration Drift","text":"","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","configuration drift detection"]},{"location":"features/configuration_drift/#configuration-drift","title":"Configuration Drift","text":"<p>Configuration drift is a commonly used term to describe a change that takes place in an environment. Drift is an issue as it causes systems and parts of a system which supposed to be consistent, to become inconsistent and unpredictable. In our case, configuration drift is a change of a resource deployed by Sveltos down the managed clusters.</p> <p>Sveltos allows users to set the <code>sync</code> mode within a ClusterProfile to ContinuousWithDriftDetection. It enables Sveltos to monitor the state of managed clusters and detect configuration drift for any of the resources deployed by a ClusterProfile.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  syncMode: ContinuousWithDriftDetection\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>When Sveltos detects a configuration drift, it will automatically re-sync the cluster state back to its original state which is described in the management cluster. Sveltos deploys a service in each managed cluster and configures it with a list of Kubernetes resources deployed for each ClusterProfile in SyncModeContinuousWithDriftDetection mode.</p> <p>The service starts a watcher for each GroupVersionKind with at least one resource to watch. When any watched resources are modified (labels, annotations, spec or rules sections), the service notifies the management cluster about potential configuration drifts. The management cluster then reacts by redeploying affected ClusterProfiles.</p> <p>This way, Sveltos ensures that the systems are always consistent and predictable, preventing unexpected issues caused by the configuration drifts.</p> <p></p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","configuration drift detection"]},{"location":"features/configuration_drift/#ignore-annotation","title":"Ignore Annotation","text":"<p>You can stop certain resources from being tracked for configuration drift. This is done by adding a special annotation <code>projectsveltos.io/driftDetectionIgnore</code> to those resources.</p> <p>For instance, following ClusterProfile will deploy Kyverno helm chart. Patches are used to apply annotation to the Kyverno <code>kyverno-admission-controller</code> deployment. This means any changes made to resources deployed by the Helm chart itself will be flagged as a configuration drift. However, any modifications directly to the kyverno-admission-controller deployment won't be detected as drift.</p> <pre><code>    apiVersion: config.projectsveltos.io/v1beta1\n    kind: ClusterProfile\n    metadata:\n      name: deploy-kyverno\n    spec:\n      clusterSelector:\n        matchLabels:\n          env: fv\n      syncMode: ContinuousWithDriftDetection\n      helmCharts:\n      - repositoryURL:    https://kyverno.github.io/kyverno/\n        repositoryName:   kyverno\n        chartName:        kyverno/kyverno\n        chartVersion:     v3.3.3\n        releaseName:      kyverno-latest\n        releaseNamespace: kyverno\n        helmChartAction:  Install\n      patches:\n      - target:\n          group: apps\n          version: v1\n          kind: Deployment\n          name: \"kyverno-admission-controller\"\n        patch: |\n          - op: add\n            path: /metadata/annotations/projectsveltos.io~1driftDetectionIgnore\n            value: \"ok\"\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","configuration drift detection"]},{"location":"features/configuration_drift/#ignore-fields","title":"Ignore Fields","text":"<p>You can optionally specify fields to be excluded from drift detection using JSON Pointers.</p> <p>Here's an example configuration in YAML format:</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: nginx\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  syncMode: ContinuousWithDriftDetection\n  helmCharts:\n  - repositoryURL:    https://helm.nginx.com/stable/\n    repositoryName:   nginx-stable\n    chartName:        nginx-stable/nginx-ingress\n    chartVersion:     1.3.1\n    releaseName:      nginx-latest\n    releaseNamespace: nginx\n    helmChartAction:  Install\n  driftExclusions:\n  - paths:\n    - \"/spec/replicas\"\n    target:\n      kind: Deployment\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","configuration drift detection"]},{"location":"features/configuration_drift/#customize-drift-detection-manager-configuration","title":"Customize drift-detection-manager configuration","text":"<p>In some cases, we might want to tailor the deployment of the <code>drift-detection-manager</code>. To achieve this, the <code>addon-controller</code> pod accepts a new argument named <code>drift-detection-config</code>.</p> <p>The <code>drift-detection-config</code> argument points to a <code>ConfigMap</code> within the projectsveltos namespace. The <code>ConfigMap</code> holds patches that will be applied to the <code>drift-detection-manager</code> before its deployment in the managed cluster.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","configuration drift detection"]},{"location":"features/configuration_drift/#configmap-example","title":"ConfigMap Example","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: drift-detection\n  namespace: projectsveltos\ndata:\n  patch: |-\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: drift-detection-manager\n    spec:\n      template:\n        spec:\n          containers:\n            - name: manager\n              image: registry.company.io/projectsveltos/drift-detection-manager:dev\n              resources:\n                requests:\n                  memory: 256Mi\n</code></pre> <p>Along with creating the <code>ConfigMap</code>, we also need to configure the addon-controller deployment to use it. To do this, add the below argument to the deployment.</p> <pre><code>- args:\n  ...\n  - --drift-detection-config=drift-detection\n</code></pre> <p>With the defined configuration, the <code>drift-detection-manager</code> will get deployed in each managed cluster with the below settings.</p> <ul> <li>Request memory: 256Mi</li> <li>Image: projectsveltos/drift-detection-manager:dev</li> </ul> <p>Tip</p> <p>If you are deploying Sveltos using the official Helm chart and need to patch the <code>drift-detection-manager</code> before deployment, include the necessary values in your <code>values.yaml</code> file.</p> <p>$ helm install projectsveltos projectsveltos/projectsveltos -n projectsveltos -f values.yaml</p> <pre><code>addonController:\n  controller:\n    args:\n    - --diagnostics-address=:8443\n    - --report-mode=0\n    - --shard-key=\n    - --v=5\n    - --version=v0.44.0\n    argsAgentMgmtCluster:\n    - --diagnostics-address=:8443\n    - --report-mode=0\n    - --agent-in-mgmt-cluster\n    - --shard-key=\n    - --v=5\n    - --version=v0.44.0\n    - --drift-detection-config=drift-detection-config\nagent:\n  managementCluster: true\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","configuration drift detection"]},{"location":"features/dependency-management/","title":"Dependency Management","text":"","tags":["Kubernetes","Sveltos","add-ons","renovate","dependencies","dependency"]},{"location":"features/dependency-management/#dependency-management","title":"Dependency Management","text":"<p>Dependency Management is a crucial aspect of maintaining a healthy and secure Kubernetes environment. It ensures that all resources and configurations are up-to-date. In the context of Sveltos, dependency management is particularly important when using Helm-Charts to deploy resources.</p> <p>Renovate is a tool that helps you automate dependency management in your Sveltos resources. It uses the Sveltos manager to update dependencies in Helm-Charts for Sveltos resources.</p>","tags":["Kubernetes","Sveltos","add-ons","renovate","dependencies","dependency"]},{"location":"features/dependency-management/#prerequisites","title":"Prerequisites","text":"<p>Before you can use Renovate to manage dependencies in your Sveltos resources, you need to have Renovate already configured for your Git-Repository. If you haven't set up Renovate yet, follow the Renovate Getting Started Guide. The following documentation shows you how to configure Renovate for Sveltos resources for a public Github-Repository.</p> <p>You can manage your Github organisations/repositories via https://developer.mend.io.</p>","tags":["Kubernetes","Sveltos","add-ons","renovate","dependencies","dependency"]},{"location":"features/dependency-management/#configure-renovate","title":"Configure Renovate","text":"<p>For Github repositories you can use the Renovate Github App. You need to authroize the Renovate Github App for your repository/organisation, where you would like to use it. Renovate will then automatically create a configuration file in your repository.</p>","tags":["Kubernetes","Sveltos","add-ons","renovate","dependencies","dependency"]},{"location":"features/dependency-management/#sveltos-manager","title":"Sveltos Manager","text":"<p>Note: The Sveltos manager was introduced with version 38.124.0 of Renovate. You must use this version or newer to use the Sveltos manager.</p> <p>The Sveltos manager for renovate supports dependency management for <code>helmCharts</code> specs for all relevant sveltos resources. Refer to the upstream manager documentation for a detailed explanation.</p> <p>A very simple configuration for the Sveltos manager looks like this:</p> <pre><code>{\n  \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",\n  \"extends\": [\"config:recommended\", \":dependencyDashboard\"],\n  \"prHourlyLimit\": 0,\n  \"prConcurrentLimit\": 0,\n  \"branchConcurrentLimit\": 0,\n  \"sveltos\": {\n      \"fileMatch\": [\"^.*/*\\\\.yaml$\"]\n  }\n}\n</code></pre> <p>As stated in the manager documentation, you must define a <code>fileMatch</code> pattern. This pattern tells Renovate which files to consider for dependency updates. In the example above, Renovate will only consider files named <code>profile.yaml</code> for dependency updates. This could also be any yaml file (<code>**.yaml</code>) or the convetions you are using in your repository.</p> <p>With this config all the limits for branches and PRs are set to <code>0</code>, which means that Renovate will not be limited. This is required for large repositories with many dependencies. See more Configurations for Renovate.</p>","tags":["Kubernetes","Sveltos","add-ons","renovate","dependencies","dependency"]},{"location":"features/dryrun/","title":"Sveltos dry run","text":"","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"features/dryrun/#what-is-the-dryrun-mode-in-kubernetes","title":"What is the DryRun mode in Kubernetes?","text":"<p>In Kubernetes, the \"dry run\" functionality allows users to simulate the execution of the commands they want to apply.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"features/dryrun/#sveltos-dryrun-explained","title":"Sveltos DryRun - Explained","text":"<p>Sveltos takes it one step further. Imagine we are about to perform important changes to a ClusterProfile, but we are unsure what the results will be. The risk of uncertainty is big and we do not want to  cause any unwanted side effects to the Production environment. That's where the DryRun syncMode configuration comes in!</p> <p>By deploying a ClusterProfile with the <code>syncMode</code> set to <code>DryRun</code>, we can launch a simulation of all the operations that would normally be executed in a live run. The best part? No actual changes will be performed to the matching clusters during this dry run workflow.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"features/dryrun/#configuraton-example","title":"Configuraton Example","text":"<pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  syncMode: DryRun\n  ...\n</code></pre> <p>Once the dry run workflow is complete, you'll receive a detailed list of all the potential changes that would have been made to the matching cluster. This allows us to carefully inspect and validate the changes before deploying the new ClusterProfile configuration.</p> <p>If you are interested in viewing the change list, you can check out the generated Custom Resource Definition (CRD) with the name ClusterReport.</p> <p>Below is a snippet from the sveltosctl utility.</p> <pre><code>$ sveltosctl show dryrun\n\n+-------------------------------------+--------------------------+-----------+----------------+-----------+--------------------------------+------------------+\n|               CLUSTER               |      RESOURCE TYPE       | NAMESPACE |      NAME      |  ACTION   |            MESSAGE             | CLUSTER PROFILE  |\n+-------------------------------------+--------------------------+-----------+----------------+-----------+--------------------------------+------------------+\n| default/sveltos-management-workload | helm release             | kyverno   | kyverno-latest | Install   |                                | dryrun           |\n| default/sveltos-management-workload | helm release             | nginx     | nginx-latest   | Install   |                                | dryrun           |\n| default/sveltos-management-workload | :Pod                     | default   | nginx          | No Action | Object already deployed.       | dryrun           |\n|                                     |                          |           |                |           | And policy referenced by       |                  |\n|                                     |                          |           |                |           | ClusterProfile has not changed |                  |\n|                                     |                          |           |                |           | since last deployment.         |                  |\n| default/sveltos-management-workload | kyverno.io:ClusterPolicy |           | no-gateway     | Create    |                                | dryrun           |\n+-------------------------------------+--------------------------+-----------+----------------+-----------+--------------------------------+------------------+\n</code></pre> <p>To view detailed line-by-line changes for each resource, use the <code>--raw-diff</code> option with the <code>sveltosctl show dryrun</code> command.</p> <pre><code>$ sveltosctl show dryrun --raw-diff\nCluster: default/clusterapi-workload\n--- deployed: ClusterPolicy disallow-latest-tag\n+++ proposed: ClusterPolicy disallow-latest-tag\n@@ -49,10 +49,10 @@\n               name: validate-image-tag\n               skipBackgroundRequests: true\n               validate:\n-                message: Using a mutable image tag e.g. 'latest' is not allowed.\n+                message: Using a mutable image tag e.g. 'latest' is not allowed in this cluster.\n                 pattern:\n                     spec:\n                         containers:\n                             - image: '!*:latest'\n-        validationFailureAction: audit\n+        validationFailureAction: Enforce\n     status: \"\"\n\nCluster: default/clusterapi-workload\n--- deployed: Deployment nginx-deployment\n+++ proposed: Deployment nginx-deployment\n@@ -22,7 +22,7 @@\n         uid: 9ba8bbc1-02fa-4cbb-9073-fe657482277d\n     spec:\n         progressDeadlineSeconds: 600\n-        replicas: 3\n+        replicas: 1\n         revisionHistoryLimit: 10\n         selector:\n             matchLabels:\n</code></pre> <p>Sveltos can also detect changes to deployed Helm charts:</p> <pre><code>$ sveltosctl show dryrun\n+-----------------------------+---------------+------------+----------------+---------------+--------------------------------+-----------------------------------+\n|           CLUSTER           | RESOURCE TYPE | NAMESPACE  |      NAME      |    ACTION     |            MESSAGE             |              PROFILE              |\n+-----------------------------+---------------+------------+----------------+---------------+--------------------------------+-----------------------------------+\n| default/clusterapi-workload | helm release  | kyverno    | kyverno-latest | Update Values | use --raw-diff to see full     | ClusterProfile/deploy-kyverno     |\n|                             |               |            |                |               | diff for helm values           |                                   |\n| default/clusterapi-workload | helm release  | prometheus | prometheus     | Upgrade       | Current version: \"23.4.0\".     | ClusterProfile/prometheus-grafana |\n|                             |               |            |                |               | Would move to version:         |                                   |\n|                             |               |            |                |               | \"26.0.0\"                       |                                   |\n| default/clusterapi-workload | helm release  | grafana    | grafana        | Upgrade       | Current version: \"6.58.9\".     | ClusterProfile/prometheus-grafana |\n|                             |               |            |                |               | Would move to version: \"8.6.4\" |                                   |\n+-----------------------------+---------------+------------+----------------+---------------+--------------------------------+-----------------------------------+\n</code></pre> <pre><code>$ sveltosctl show dryrun --raw-diff\nProfile: ClusterProfile:deploy-kyverno Cluster: default/clusterapi-workload\n--- deployed values\n+++ proposed values\n@@ -1,6 +1,6 @@\n admissionController:\n     replicas: 3\n backgroundController:\n-    replicas: 1\n+    replicas: 3\n reportsController:\n-    replicas: 1\n+    replicas: 3\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"features/dryrun/#more-resources","title":"More Resources","text":"<p>For a quick demonstration of the dry run mode, watch the Sveltos, introduction to DryRun mode video on YouTube.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"features/labels_management/","title":"Kubernetes Cluster Classification - Project Sveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#classifier-automatically-manage-cluster-labels-and-add-ons","title":"Classifier - Automatically Manage Cluster Labels and Add-Ons","text":"<p>Sveltos provides users with the power to decide which add-ons should get deployed to which clusters programmatically by the use of a ClusterSelector. Sometimes the versions of the required and needed add-ons depend on the cluster's runtime state. This is where the Sveltos Classifier comes into play.</p> <p>With the Classifier, Sveltos can be configured to automatically update the cluster labels based on the cluster runtime state. As the runtime state changes, the cluster labels are automatically updated. This ensures that the appropriate ClusterProfile instances match the specified clusters, leading to an automatic upgrade of the Kubernetes add-ons.</p> <p>Once the Classifier is deployed in the management cluster, it is distributed to each cluster, and a Sveltos service running in each managed cluster monitors the cluster runtime state. As soon as a match is found, information is transmitted back to the management cluster, and the cluster labels are appropriately updated by Sveltos.</p> <p>By combining the Classifier with the ClusterProfiles, Sveltos can monitor the runtime status of each cluster, update the cluster labels when the cluster runtime state changes, and deploy, upgrade the Kubernetes add-ons accordingly.</p> <p></p>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#use-case-upgrade-helm-charts-when-kubernetes-cluster-is-upgraded","title":"Use Case: Upgrade Helm Charts when Kubernetes Cluster is Upgraded","text":"<p>Suppose you are managing several Kubernetes clusters with different versions and you want to deploy the below points:</p> <ol> <li>OPA Gatekeeper version 3.10.0 in any Kubernetes cluster whose version is &gt;= v1.25.0</li> <li>OPA Gatekeeper version 3.9.0 in any Kubernetes cluster whose version is &gt;= v1.24.0 &amp;&amp; &lt; v1.25.0</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#management-cluster","title":"Management Cluster","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#clusterprofiles","title":"ClusterProfiles","text":"<pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-gatekeeper-3-10\nspec:\n  clusterSelector:\n    matchLabels:\n      gatekeeper: v3-10\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL: https://open-policy-agent.github.io/gatekeeper/charts\n    repositoryName: gatekeeper\n    chartName: gatekeeper/gatekeeper\n    chartVersion:  3.10.0\n    releaseName: gatekeeper\n    releaseNamespace: gatekeeper\n    helmChartAction: Install\n</code></pre> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-gatekeeper-3-9\nspec:\n  clusterSelector:\n    matchLabels:\n      gatekeeper: v3-9\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL: https://open-policy-agent.github.io/gatekeeper/charts\n    repositoryName: gatekeeper\n    chartName: gatekeeper/gatekeeper\n    chartVersion:  3.9.0\n    releaseName: gatekeeper\n    releaseNamespace: gatekeeper\n    helmChartAction: Install\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#classifiers","title":"Classifiers","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: Classifier\nmetadata:\n  name: deploy-gatekeeper-3-10\nspec:\n  classifierLabels:\n  - key: gatekeeper\n    value: v3-10\n  kubernetesVersionConstraints:\n  - comparison: GreaterThanOrEqualTo\n    version: 1.25.0\n</code></pre> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: Classifier\nmetadata:\n  name: deploy-gatekeeper-3-9\nspec:\n  classifierLabels:\n  - key: gatekeeper\n    value: v3-9\n  kubernetesVersionConstraints:\n  - comparison: GreaterThanOrEqualTo\n    version: 1.24.0\n  - comparison: LessThan\n    version: 1.25.0\n</code></pre> <p>Based on the above configuration, we achieved the below.</p> <ol> <li>Any cluster with a Kubernetes version v1.24.x will get the label gatekeeper:v3.9 added and then the Gatekeeper v3.9.0 helm chart will be deployed;</li> <li>Any cluster with a Kubernetes version v1.25.x will get the label gatekeeper:v3.10 added and then the Gatekeeper v3.10.0 helm chart will be deployed;</li> <li>As soon as a cluster is upgraded from Kubernetes v1.24.x to v1.25.x, Gatekeeper helm chart will be automatically upgraded from 3.9.0 to 3.10.0</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#more-resources","title":"More Resources","text":"<p>To read more about the classifier configuration, with examles using the resources and the Lua script, have a look at the section.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#more-examples","title":"More Examples","text":"<ol> <li>Classify clusters based on their Kubernetes version classifier.yaml</li> <li>Classify clusters based on the number of namespaces classifier.yaml</li> <li>Classify clusters based on their Kubernetes version and resources classifier.yaml</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#use-case-metrics-based-classifier","title":"Use Case: Metrics-based Classifier","text":"<p>There are cases when end-users want to deploy add-ons and applications in Kubernetes clusters that are not heavily loaded. That means, based on the CPU, memory or any other metric, control what will get deployed and where.</p> <p>The functionality can be achieved by allowing the deployedResourceConstraint to reference resources that live in the management cluster. The available feature request is located here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#example","title":"Example","text":"<pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: Classifier\nmetadata:\n  name: low-load\nspec:\n  classifierLabels:\n  - key: load # Label applied to matching clusters\n    value: low\n  deployedResourceConstraint:\n    resourceSelectors:\n    - group: metrics.keptn.sh\n      version: v1alpha1 # Adjust based on the Keptn version\n      kind: KeptnMetric\n      namespace: sveltos-metrics # KeptnMetric runs in the mgmt cluster\n      name: keptnmetric-cpu-{{ .cluster }}\n      evaluate: | # Use Lua for conditional checks\n        function evaluate()\n          local hs = { matching = false, message = \"\" }\n          local v = tonumber(obj.status.value)\n          if v and v &lt; 0.5 then\n            hs.matching = true          -- cluster is \u201clow load\u201d\n          else\n            hs.message = \"CPU util \" .. tostring(v)\n          end\n          return hs\n        end\n  # &lt;--- new field proposed in #375\n  sourceClusterScope: Local\n</code></pre> <p>How does the proposed resource work? The KeptnMetric.status.value already carries the live metric. The <code>Lua</code> code used will evaluate the block checks and the threshold directly. No additional controller is required. When the defined metric goes over the defined limit, the label is either added or removed, and any <code>ClusterProfile</code> will react automatically.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#classifier-crd-deep-dive","title":"Classifier CRD - Deep dive","text":"<p>Classifier CRD is the CRD used to instructs Sveltos on how to classify a cluster.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#classifier-labels","title":"Classifier Labels","text":"<p>The field classifierLabels contains all the labels (key/value pair) which will be added automatically to any cluster matching a Classifier instance.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#kubernetes-version-constraints","title":"Kubernetes version constraints","text":"<p>The field kubernetesVersionConstraints can be used to classify a cluster based on its current Kubernetes version.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#resource-constraints","title":"Resource constraints","text":"<p>The field deployedResourceConstraint can be used to classify a cluster based on current deployed resources. Resources are identified by Group/Version/Kind and can be filtered based on their namespace and labels and some fields. It supports Lua script as well.</p> <p>Following classifier, matches any cluster with a Service with label sveltos:fv.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: Classifier\nmetadata:\n  name: sveltos-service\nspec:\n  classifierLabels:\n  - key: sveltos-service\n    value: present\n  deployedResourceConstraint:\n    resourceSelectors:\n    - group: \"\"\n      version: v1\n      kind: Service\n      labelFilters:\n      - key: sveltos\n        operation: Equal\n        value: fv\n</code></pre> <p>Following classifier, matches any cluster with a ClusterIssuer using acme-staging-v02.api.letsencrypt.org</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: Classifier\nmetadata:\n  name: acme-staging-v02\nspec:\n  classifierLabels:\n  - key: issuer\n    value: acme-staging-v02\n  deployedResourceConstraint:\n    resourceSelectors:\n    - group: \"cert-manager.io\"\n      version: v1\n      kind: ClusterIssuer\n      evaluate: |\n        function evaluate()\n          hs = {}\n          hs.matching = false\n          hs.message = \"\"\n          if obj.spec.acme ~= nil then\n            if string.find(obj.spec.acme.server, \"acme-staging-v02.api.letsencrypt.org\", 1, true) then\n              hs.matching = true\n            end\n          end\n          return hs\n        end\n</code></pre> <p>A Classifier can also look at the resources of different kinds all together.</p> <p>AggregatedClassification is optional and can be used to specify a Lua function that will be used to further detect whether the subset of the resources selected using the ResourceSelectors field are a match for this Classifier. The function will receive the array of resources selected by ResourceSelectors. If this field is not specified, a cluster is a match for Classifier instance, if all ResourceSelectors returns at least one match. This field allows to perform more complex evaluation on the resources, looking at all resources together. This can be useful for more sophisticated tasks, such as identifying resources that are related to each other or that have similar properties. The Lua function must return a struct with:</p> <ul> <li>\"matching\" field: boolean indicating whether cluster is a match;</li> <li>\"message\" field: (optional) message.</li> </ul> <p>Note</p> <p>Keep in mind the CEL language can be used as a way to express logic.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/labels_management/#classifier-controller-configuration","title":"Classifier controller configuration","text":"<ol> <li>concurrent-reconciles: By default Sveltos manager reconcilers runs with a parallelism set to 10. This arg can be used to change level of parallelism;</li> <li>worker-number: Number of workers performing long running task. By default this is set to 20. If number of Classifier instances is in the hundreds, please consider increasing this;</li> <li>report-mode: By default Classifier controller running in the management cluster periodically collects ClassifierReport instances from each managed cluster. Setting report-mode to \"1\" will change this and have each Classifier Agent send back ClassifierReport to management cluster. When setting report-mode to 1, control-plane-endpoint must be set as well. When in this mode, Sveltos automatically creates a ServiceAccount in the management cluster for Classifier Agent. Only permissions granted for this ServiceAccount are update of ClassifierReports.</li> <li>control-plane-endpoint: The management cluster controlplane endpoint. Format &lt;ip&gt;:&lt;port&gt;. This must be reachable frm each managed cluster.</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","kubernetes cluster classification","cluster runtime state","multi-tenancy"]},{"location":"features/mcp/","title":"Sveltos MCP Server - A Management Tool","text":"<p>Video</p> <p>To learn more about the Sveltos MCP Server, check out the Video! \ud83d\ude0a</p> <p>The Sveltos Management Cluster Protocol (MCP) Server is a management tool that connects AI assistants and chatbots to the Sveltos management cluster. It provides a structured, programmatic interface that allows AI agents to interact with Sveltos using natural language. This enables powerful features such as automated troubleshooting, real-time cluster analysis, and streamlined operational tasks across all the clusters Sveltos manages.</p>","tags":["MCP","Kubernetes","multi-cluster management","AI","automation","SRE"]},{"location":"features/mcp/#how-does-it-work","title":"How does it work?","text":"<p>The Sveltos MCP Server acts as a communication layer between AI agents and the Sveltos-managed infrastructure. Instead of an AI directly executing kubectl commands on individual clusters, it sends requests to the MCP Server using the Management Cluster Protocol (MCP). The server then translates these high-level requests into specific Sveltos tool calls.</p> <p>The described process is not a simple command translation; it involves performing comprehensive checks on cluster statuses, the state of the Sveltos deployments, and the state of the Sveltos Custom Resources. Additionally, the server correlates the state of different resources that are tied to each other, providing a holistic view of the system's health. By consolidating these checks, the MCP Server provides a single, unified result to the AI, which can then present the findings to a user in a clear, conversational format.</p> <p>This abstraction allows AI to perform complex operations, such as diagnosing deployment failures or comparing cluster configurations, with a single, high-level instruction, making Sveltos a powerful tool for automated, multi-cluster management.</p>","tags":["MCP","Kubernetes","multi-cluster management","AI","automation","SRE"]},{"location":"features/mcp/#key-capabilities","title":"Key Capabilities","text":"<p>The Sveltos MCP Server empowers AI with the ability to:</p> <ul> <li> <p>Analyze Cluster State: The AI can read the operational status of all Sveltos-managed clusters, including the health of Sveltos agents and the state of deployed resources. This provides a single source of truth for your entire fleet.</p> </li> <li> <p>Automate Troubleshooting: When a user reports an issue, the AI can use the MCP Server's tools to perform diagnostic checks automatically. For example, it can call the analyze_profile_deployment tool to investigate a deployment failure, identify the specific error, and provide a resolution plan.</p> </li> <li> <p>Ensure Configuration Consistency: The compare_managed_clusters tool allows AI to proactively monitor for configuration drift between clusters. It can quickly identify what's different and alert operators to potential compliance issues or misconfigurations.</p> </li> <li> <p>Streamline Operations: The server's tools enable AI to handle routine operational tasks, such as listing deployed resources on a cluster or verifying the Sveltos installation status. This frees up human operators to focus on more complex tasks.</p> </li> </ul>","tags":["MCP","Kubernetes","multi-cluster management","AI","automation","SRE"]},{"location":"features/mcp/#integrated-dashboard-functionality","title":"Integrated Dashboard Functionality","text":"<p>The Sveltos dashboard is designed to enhance users' troubleshooting experience by integrating a built-in Sveltos MCP client. The client connects directly to the Sveltos MCP server, providing powerful diagnostic capabilities right from the user interface.</p> <p>The integrated client can:</p> <ul> <li> <p>Debug Sveltos Installation: Instantly verify the health of the Sveltos installation on the management cluster. The client talks to the server, which runs comprehensive checks to ensure all Sveltos components are running correctly. It also verifies and checks the status of the Sveltos agents deployed across all the managed clusters. This ensures that every cluster is properly configured and communicating with the management cluster.</p> </li> <li> <p>Diagnose Deployment Issues: The dashboard can be used to pinpoint deployment problems on any Sveltos managed cluster. The MCP client sends a request to the server, which analyzes the state of Sveltos' resources and deployments, and returns a detailed report on any failures or inconsistencies.</p> </li> </ul> <p>\ud83c\udfa5 Dashboard MCP Video</p> <p>The seamless integration transforms the dashboard from a simple monitoring tool into a proactive, powerful debugging console, leveraging the full capabilities of the Sveltos MCP server to simplify multi-cluster management.</p> <p>Note</p> <p>You can test the Sveltos MCP Server directly from the Sveltos dashboard to verify your Sveltos installation and diagnose failures on a given cluster. If you want to integrate the Sveltos MCP Server with your AI Site Reliability Engineering (SRE) tools, contact us at <code>support@projectsveltos.io</code>  to discuss licensing options tailored to your specific needs.</p>","tags":["MCP","Kubernetes","multi-cluster management","AI","automation","SRE"]},{"location":"features/mcp/#kubernetes-deployment-details","title":"Kubernetes Deployment Details","text":"<p>The Sveltos MCP Server is deployed in the projectsveltos namespace.</p> <ul> <li>Deployment: mcp-server</li> <li>Service: mcp-server</li> </ul> <p>The service type is ClusterIP, which means it is only accessible from within the cluster. If you need to expose the Sveltos MCP Server to external clients, you'll need to change the service type from ClusterIP to a different type, such as LoadBalancer or NodePort.</p>","tags":["MCP","Kubernetes","multi-cluster management","AI","automation","SRE"]},{"location":"features/multi-tenancy-full-isolation/","title":"Kubernetes multi-tenancy - Full Isolation","text":"<p>A common multi-tenant scenario involves assigning dedicated namespaces within the management cluster for each tenant. Tenant admins then create and manage their clusters within their designated namespace and use Profile instances to define list of add-ons and applications to deploy in their managed clusters.</p> <p>Similar to ClusterProfiles, Profiles utilize a cluster selector and list of add-ons and applications. However,  Profiles operate within a specific namespace, matching only clusters created in that namespace.</p> <p></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/multi-tenancy-sharing-cluster/","title":"Kubernetes multi-tenancy - Sharing Cluster","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#introduction-to-multitenancy","title":"Introduction to Multitenancy","text":"<p>With Sveltos, a management cluster is used to manage add-ons in tens of clusters. When managing tens of clusters, multitenancy plays an important role.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#common-forms-of-multitenancy","title":"Common forms of multitenancy","text":"<ol> <li>Share a cluster between multiple teams within an organization, each of whom may operate one or more workloads. These workloads frequently need to communicate with each other, and with other workloads located on the same or different clusters;</li> <li>One (or more) cluster(s) fully reserved for an organization.</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#defined-roles","title":"Defined Roles","text":"<ol> <li>Platform admin: Is the admin with the cluster-admin access to all the managed clusters;</li> <li>Tenant admin: Is the admin with access to the clusters/namespaces assigned to them by the platform admin. Tenant admin manages applications for a tenant.</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#sveltos-solution","title":"Sveltos Solution","text":"<ol> <li>Platform admin onboards tenant admins and easily define what each tenant can do in which clusters;</li> <li>Tenant admin manage tenant applications from a single place, the management cluster.</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#sveltos-rolerequest-crd","title":"Sveltos RoleRequest CRD","text":"<p><code>RoleRequest</code> is the CRD introduced by Sveltos to allow platform admins to grant permissions to various tenant admins.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: RoleRequest\nmetadata:\n  name: full-access\nspec:\n  serviceAccountName: \"eng\"\n  serviceAccountNamespace: \"default\"\n  clusterSelector:\n    matchLabels:\n      dep: eng\n  roleRefs:\n  - name: full-access\n    namespace: default\n    kind: ConfigMap\n</code></pre> <p>Based on the above YAML definition, we specify the below fields:</p> <ul> <li><code>serviceAccountName</code>: The service account the permission will be applied to;</li> <li><code>serviceAccountNamespace</code>: The namespace the service account has been deployed in the management cluster</li> <li><code>clusterSelector</code>: This is a Kubernetes label selector. Sveltos uses the label to detect all the clusters where permissions need to be granted;</li> <li><code>roleRefs</code>: References ConfigMaps/Secrets each containing one or more Kubernetes ClusterRoles/Roles defining the permissions to be granted.</li> </ul> <p>An example of a ConfigMap containing a ClusterRole granting definition with full edit permissions can be found below.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: full-access\n  namespace: default\ndata:\n  role.yaml: |\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRole\n    metadata:\n      name: eng-full-access\n    rules:\n    - apiGroups: [\"*\"]\n      resources: [\"*\"]\n      verbs: [\"*\"]\n</code></pre> <p></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#more-examples","title":"More Examples","text":"<p>More examples can be found here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#example-clusterprofile-definition","title":"Example - ClusterProfile Definition","text":"<p>After a tenant is onboarded by the platform admin, the service account created in the step above can use a ClusterProfiles and Sveltos will take care of deploying the defined resources to all matching clusters.</p> <p>Sveltos expects the following labels to be set on each ClusterProfile.</p> <pre><code>projectsveltos.io/serviceaccount-name: &lt;service account name&gt;\nprojectsveltos.io/serviceaccount-namespace: &lt;service account defined namespace&gt;\n</code></pre> <p>If:</p> <ol> <li>Each tenant admin is a ServiceAccount in the management cluster;</li> <li>Kyverno is deployed in the management cluster;</li> </ol> <p>Sveltos suggests using the below Kyverno ClusterPolicy, which will take care of adding proper labels to each ClusterProfile at creation time.</p> <pre><code>---\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: add-labels\n  annotations:\n    policies.kyverno.io/title: Add Labels\n    policies.kyverno.io/description: &gt;-\n      Adds projectsveltos.io/admin-name label on each ClusterProfile\n      created by tenant admin. It assumes each tenant admin is\n      represented in the management cluster by a ServiceAccount.\nspec:\n  background: false\n  rules:\n  - exclude:\n      any:\n      - clusterRoles:\n        - cluster-admin\n    match:\n      all:\n      - resources:\n          kinds:\n          - ClusterProfile\n    mutate:\n      patchStrategicMerge:\n        metadata:\n          labels:\n            +(projectsveltos.io/serviceaccount-name): '{{serviceAccountName}}'\n            +(projectsveltos.io/serviceaccount-namespace): '{{serviceAccountNamespace}}'\n    name: add-labels\n  validationFailureAction: enforce\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#example-tenant-cluster-reservation","title":"Example - Tenant Cluster Reservation","text":"<p>In the example below, all clusters matching the Kubernetes label selector org=foo.io will be assigned to the service account with the name <code>foo</code>.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: full-access\n  namespace: default\ndata:\n  role.yaml: |\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRole\n    metadata:\n      name: foo-full-access\n    rules:\n    - apiGroups: [\"*\"]\n      resources: [\"*\"]\n      verbs: [\"*\"]\n\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: RoleRequest\nmetadata:\n  name: full-access\nspec:\n  serviceAccountName: \"foo\"\n  serviceAccountNamespace: \"default\"\n  clusterSelector:\n    matchLabels:\n      org: foo.io\n  roleRefs:\n  - name: full-access\n    namespace: default\n    kind: ConfigMap\n</code></pre> <p>We can use of the sveltosctl to check the permissions given to the service account <code>foo</code>. We expect the service account to have full access to the managed cluster with the label set to <code>env:production</code></p> <pre><code>$ sveltosctl show admin-rbac\n+-------------------------------+--------------+-----------+------------+-----------+----------------+-------+\n|            CLUSTER            |    ADMIN     | NAMESPACE | API GROUPS | RESOURCES | RESOURCE NAMES | VERBS |\n+-------------------------------+--------------+-----------+------------+-----------+----------------+-------+\n| SveltosCluster:gke/production |     foo      | *         | *          | *         | *              | *     |\n+-------------------------------+--------------+-----------+------------+-----------+----------------+-------+\n</code></pre> <p>As soon as the service account <code>foo</code> posts the below ClusterProfile, Sveltos will deploy Kyverno in any cluster matching the label selector set to org=foo.io.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\n  labels:\n    projectsveltos.io/serviceaccount-name: foo\n    projectsveltos.io/serviceaccount-namespace: default\nspec:\n  clusterSelector:\n    matchLabels:\n      org: foo.io\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre> <p>If the same service account tries to deploy Kyverno in a cluster not assigned to it, Sveltos will fail the deployment.</p> <p>For instance, if the <code>ClusterProfile.Spec.ClusterSelector</code> is set to org=bar.io, the deployment will fail.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\n  labels:\n    projectsveltos.io/serviceaccount-name: foo\n    projectsveltos.io/serviceaccount-namespace: default\nspec:\n  clusterSelector:\n    matchLabels:\n      org: bar.io\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#example-share-cluster-between-tenants","title":"Example - Share Cluster Between Tenants","text":"<p>In the below examples, all clusters matching the label selector env=internal are shared between two tenants:</p> <ol> <li>Service Account eng is granted full access to namespaces foo-eng and foo-hr</li> <li>Service Account hr is granted full access to namespace bar-resource</li> </ol> <pre><code># ConfigMap contains a Role which gives\n# full access to namespaces ci-cd and build\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: foo-shared-access\n  namespace: default\ndata:\n  ci_cd_role.yaml: |\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: Role\n    metadata:\n      name: edit-role\n      namespace: foo-eng\n    rules:\n    - apiGroups: [\"*\"]\n      resources: [\"*\"]\n      verbs: [\"*\"]\n  build_role.yaml: |\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: Role\n    metadata:\n      name: edit-role\n      namespace: foo-hr\n    rules:\n    - apiGroups: [\"*\"]\n      resources: [\"*\"]\n      verbs: [\"*\"]\n---\n# RoleRequest gives the service account 'eng' access to namespaces\n# 'ci-cd' and 'build' in all clusters matching the label\n# selector env=internal\napiVersion: lib.projectsveltos.io/v1beta1\nkind: RoleRequest\nmetadata:\n  name: foo-access\nspec:\n  serviceAccountName: \"eng\"\n  serviceAccountNamespace: \"default\"\n  clusterSelector:\n    matchLabels:\n      env: internal\n  roleRefs:\n  - name: foo-shared-access\n    namespace: default\n    kind: ConfigMap\n\n---\n# ConfigMap contains a Role which gives\n# full access to the namespace human-resource\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: bar-shared-access\n  namespace: default\ndata:\n  ci_cd_role.yaml: |\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: Role\n    metadata:\n      name: edit-role\n      namespace: bar-resource\n    rules:\n    - apiGroups: [\"*\"]\n      resources: [\"*\"]\n      verbs: [\"*\"]\n---\n# RoleRequest gives service account 'hr' access to the namespace\n# 'human-resource' in all clusters matching the label\n# selector env=internal\napiVersion: lib.projectsveltos.io/v1beta1\nkind: RoleRequest\nmetadata:\n  name: bar-access\nspec:\n  serviceAccountName: \"hr\"\n  serviceAccountNamespace: \"default\"\n  clusterSelector:\n    matchLabels:\n      env: internal\n  roleRefs:\n  - name: bar-shared-access\n    namespace: default\n    kind: ConfigMap\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/multi-tenancy-sharing-cluster/#display-tenant-admin-permissions","title":"Display Tenant Admin Permissions","text":"<p>Sveltos heavily focuses on the visibility of the clusters. The Sveltosctl can be used to display permissions granted to each tenant admin in each managed cluster.</p> <p>If we have two clusters, a ClusterAPI powered and a SveltosCluster, both matching the label selector <code>env=internal</code> and we post the RoleRequests, we get the below output.</p> <pre><code>$ sveltosctl show admin-rbac\n+---------------------------------------------+-------+----------------+------------+-----------+----------------+-------+\n|                   CLUSTER                   | ADMIN |   NAMESPACE    | API GROUPS | RESOURCES | RESOURCE NAMES | VERBS |\n+---------------------------------------------+-------+----------------+------------+-----------+----------------+-------+\n| Cluster:default/sveltos-management-workload | eng   | build          | *          | *         | *              | *     |\n| Cluster:default/sveltos-management-workload | eng   | ci-cd          | *          | *         | *              | *     |\n| Cluster:default/sveltos-management-workload | hr    | human-resource | *          | *         | *              | *     |\n| SveltosCluster:gke/prod-cluster             | eng   | build          | *          | *         | *              | *     |\n| SveltosCluster:gke/prod-cluster             | eng   | ci-cd          | *          | *         | *              | *     |\n| SveltosCluster:gke/prod-cluster             | hr    | human-resource | *          | *         | *              | *     |\n+---------------------------------------------+-------+----------------+------------+-----------+----------------+-------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multitenancy"]},{"location":"features/post-renderer-patches/","title":"Post Renderer Patches","text":"<p>Sveltos offers a powerful capability called post-rendering. This allows you to make adjustments to generated manifests before deploying them to your managed clusters.</p> <p>Imagine you're installing a Helm chart that lacks built-in label configuration. You want to add a <code>enviroment: production</code> label to all deployed instances. Here's where post-rendering shines! By using a post-render patch, you can achieve this without modifying the original chart.</p> <p>The provided YAML snippet demonstrates this concept. It defines a ClusterProfile that targets deployments and injects a <code>enviroment: production</code> label using a strategic merge patch. This ensures all deployments receive the label during installation.</p> <pre><code>kind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n  policyRefs:\n  - name: disallow-latest\n    namespace: default\n    kind: ConfigMap\n  patches:\n  - target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \".*\"\n    patch: |\n      - op: add\n        path: /metadata/labels/environment\n        value: production\n</code></pre> <p>This is yet another example</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: nginx\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  helmCharts:\n  - chartName: nginx-stable/nginx-ingress\n    chartVersion: 1.1.3\n    helmChartAction: Install\n    releaseName: nginx-latest\n    releaseNamespace: nginx\n    repositoryName: nginx-stable\n    repositoryURL: https://helm.nginx.com/stable/\n  patches:\n  - patch: |-\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name:  nginx-latest-nginx-ingress-controller\n      spec:\n        template:\n          spec:\n            containers:\n            - name: nginx-ingress\n              imagePullPolicy: Always\n              securityContext:\n                readOnlyRootFilesystem: true\n    target:\n      group: apps\n      kind: Deployment\n      version: v1\n</code></pre>","tags":["Kubernetes","add-ons","rolling upgrades"]},{"location":"features/rolling_upgrade/","title":"Deployments, Daemonsets Statefulsets rolling upgrades","text":"","tags":["Kubernetes","add-ons","rolling upgrades"]},{"location":"features/rolling_upgrade/#introduction-to-rolling-upgrades","title":"Introduction to Rolling Upgrades","text":"<p>ConfigMaps and Secrets are Kubernetes resources designed to decouple configuration details from application code, fostering a clear separation between the configuration data and the application logic. This separation yields several compelling advantages, making the practice of mounting ConfigMaps and Secrets a crucial component of Kubernetes deployments.</p> <ol> <li>Configuration Flexibility: Applications often require various configuration settings to function optimally across different environments. By mounting ConfigMaps, developers can modify configuration parameters without altering the application code. This not only streamlines the development process but also allows for on-the-fly adjustments, facilitating smooth transitions between development, testing, and production environments.</li> <li>Enhanced Security: Secrets, such as sensitive authentication tokens, passwords, and API keys, contain critical information that must be safeguarded. Instead of hardcoding these sensitive details directly into the application code, they can be stored as Secrets and mounted into pods only when necessary. This mitigates the risk of inadvertent exposure and helps maintain a higher level of security.</li> </ol>","tags":["Kubernetes","add-ons","rolling upgrades"]},{"location":"features/rolling_upgrade/#react-to-configmapsecret-changes","title":"React to ConfigMap/Secret changes","text":"<p>Sveltos has the capability to monitor changes within ConfigMap and Secret resources and facilitate rolling upgrades for Deployments, StatefulSets, and DaemonSets. This functionality can be activated by setting the reloader field to <code>true</code> in the ClusterProfile, as demonstrated in the below YAML configuration.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: nginx\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  reloader: true\n  policyRefs:\n  - name: nginx-data\n    namespace: default\n    kind: ConfigMap\n  - name: nginx\n    namespace: default\n    kind: ConfigMap\n</code></pre> <p>The nginx ConfigMap contains a Deployment mounting a ConfigMap<sup>1</sup>.</p> <p>The above ClusterProfile is responsible for deploying both a ConfigMap instance and a Deployment instance, with the latter mounting a ConfigMap.</p> <pre><code>$ sveltosctl show addons\n+-----------------------------+---------------------------------+-----------+---------------------+---------+-------------------------------+------------------------------+\n|           CLUSTER           |          RESOURCE TYPE          | NAMESPACE |        NAME         | VERSION |             TIME              |       CLUSTER PROFILES       |\n+-----------------------------+---------------------------------+-----------+---------------------+---------+-------------------------------+------------------------------+\n| default/clusterapi-workload | :ConfigMap                      | nginx     | nginx-config        | N/A     | 2023-08-09 05:00:45 -0700 PDT | nginx                        |\n| default/clusterapi-workload | apps:Deployment                 | nginx     | nginx-deployment    | N/A     | 2023-08-09 05:00:45 -0700 PDT | nginx                        |\n+-----------------------------+---------------------------------+-----------+---------------------+---------+-------------------------------+------------------------------+\n</code></pre> <p>Whenever the ConfigMap that is mounted by a Deployment undergoes modifications, Sveltos will automatically initiate a rolling upgrade process for the Deployment.</p> <p></p> <p>By setting the reloader field to <code>true</code>, you enable automated rolling upgrades that ensure the latest configurations are consistently applied down the applications. This significantly simplifies the maintenance and enhancement of your Kubernetes cluster, promoting stability and efficient resource utilization.</p> <ol> <li>Deployments: When a ConfigMap or Secret mounted by a Deployment is modified, Sveltos promptly detects the change and initiates a rolling upgrade for that Deployment. This ensures that any changes in configuration or secrets are promptly and securely propagated to the running instances of the application.</li> <li>StatefulSets: Similar to Deployments, StatefulSets can take advantage of Sveltos' monitoring capabilities. Modifications to the ConfigMap or Secret mounted by a StatefulSet will trigger rolling updates for the StatefulSet instances. This allows for controlled and consistent updates to stateful applications while maintaining data integrity.</li> <li>DaemonSets: Sveltos extends its monitoring to DaemonSets. If a ConfigMap or a Secret used by a DaemonSet is modified, Sveltos takes the initiative to perform a rolling upgrade across all the nodes where the DaemonSet is deployed. This way, any changes made to the resources are efficiently propagated throughout the cluster.</li> </ol> <ol> <li> <p>nginx-data ConfigMap <pre><code>---\napiVersion: v1\ndata:\n  configmap.yaml: \"# nginx-config.yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n\n    \\ name: nginx-config\\n  namespace: nginx\\ndata:\\n  nginx.conf: |\\n    server {\\n\n    \\     listen 80;\\n      server_name example.com;\\n      \\n      location / {\\n\n    \\       root /usr/share/nginx/html;\\n        index index.html;\\n      }\\n    }\\n\"\nkind: ConfigMap\nmetadata:\n  name: nginx-data\n  namespace: default\n</code></pre> and the nginx ConfigMap <pre><code>---\napiVersion: v1\ndata:\n  deployment.yaml: |\n    # nginx-deployment.yaml\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: nginx-deployment\n      namespace: nginx\n    spec:\n      replicas: 1\n      selector:\n        matchLabels:\n          app: nginx\n      template:\n        metadata:\n          labels:\n            app: nginx\n        spec:\n          containers:\n            - name: nginx\n              image: nginx:latest\n              ports:\n                - containerPort: 80\n              volumeMounts:\n                - name: nginx-config-volume\n                  mountPath: /etc/nginx/conf.d/default.conf\n                  subPath: nginx.conf\n          volumes:\n            - name: nginx-config-volume\n              configMap:\n                name: nginx-config\nkind: ConfigMap\nmetadata:\n  name: nginx\n  namespace: default\n</code></pre> \u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","rolling upgrades"]},{"location":"features/set/","title":"Sveltos ClusterSet/Set","text":"","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#clusterset-and-set","title":"ClusterSet and Set","text":"<p>Sveltos offers two resources, <code>ClusterSet</code> and <code>Set</code>, to manage groups of clusters and dynamically select a subset of those clusters for deployments based on specific criteria. This enables automated deployments and failover across healthy clusters in your environment.</p>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#key-capabilities","title":"Key Capabilities","text":"<ol> <li>Selection: Choose clusters using a defined clusterSelector (e.g., label matching).</li> <li>Capping: Limit the number of selected clusters with maxReplicas.</li> <li>Failover: If a selected cluster becomes unavailable, Sveltos automatically picks another healthy one from the matching pool to maintain the desired number of active clusters (up to the maxReplicas limit).</li> </ol>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#referencing-clustersetset-in-a-clusterprofileprofile","title":"Referencing ClusterSet/Set in a ClusterProfile/Profile","text":"<p>A ClusterProfile or Profile can reference a ClusterSet or Set by specifying its name. The add-ons defined in the profile will only be deployed to the currently selected clusters within the referenced set.</p> <p>The add-ons defined in the ClusterProfile/Profile will be deployed only to the currently selected clusters within the referenced ClusterSet/Set. This enables dynamic deployment management based on the available and healthy clusters in the set.</p> <p>This feature is particularly useful for scenarios where you want to implement active/passive failover: create a ClusterSet/Set with maxReplicas: 1 and have it match two clusters in the clusterSelector. This ensures only one cluster is active at a time. If the active cluster goes down, the backup cluster will be automatically selected for deployments.</p>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#activepassive-failover-example","title":"Active/Passive Failover Example","text":"<p>This scenario demonstrates active/passive failover with a ClusterSet.</p> <p></p>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#register-clusters","title":"Register Clusters:","text":"<p>We have two Civo clusters registered with Sveltos, all labeled <code>env:prod</code>.</p> <pre><code>$ kubectl get sveltoscluster -A --show-labels\nNAMESPACE  NAME    READY  VERSION    LABELS\ncivo    cluster1  true  v1.29.2+k3s1  env=prod\ncivo    cluster2  true  v1.28.7+k3s1  env=prod\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#create-a-clusterset","title":"Create a ClusterSet","text":"<p>A ClusterSet named prod is created with <code>clusterSelector</code> to match all prod clusters and <code>maxReplicas: 1</code> to ensure only one cluster is active at a time.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterSet\nmetadata:\n  name: prod\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  maxReplicas: 1\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#sveltos-detects-matching-clusters","title":"Sveltos Detects Matching Clusters","text":"<p>Sveltos identifies both clusters as matches (<code>status.matchingClusterRefs</code>) and selects one (e.g., cluster2) as the active cluster (<code>status.selectedClusterRefs</code>).</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterSet\nmetadata:\n  name: prod\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  maxReplicas: 1\nstatus:\n  matchingClusterRefs:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster1\n    namespace: civo\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster2\n    namespace: civo\n  namespace: civo\n  selectedClusterRefs:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster2\n    namespace: civo\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#deploy-a-clusterprofile","title":"Deploy a ClusterProfile","text":"<p>A ClusterProfile named kyverno is deployed referencing the prod ClusterSet.</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  helmCharts:\n  - chartName: kyverno/kyverno\n    chartVersion: v3.3.3\n    helmChartAction: Install\n    releaseName: kyverno-latest\n    releaseNamespace: kyverno\n    repositoryName: kyverno\n    repositoryURL: https://kyverno.github.io/kyverno/\n  setRefs:\n  - prod # name of the ClusterSet\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#sveltos-deploys-kyverno","title":"Sveltos Deploys Kyverno","text":"<p>Sveltos deploys the Kyverno charts specified in the ClusterProfile onto the cluster selected by the ClusterSet (e.g., civo/cluster3).</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  helmCharts:\n  - chartName: kyverno/kyverno\n    chartVersion: v3.3.3\n    helmChartAction: Install\n    releaseName: kyverno-latest\n    releaseNamespace: kyverno\n    repositoryName: kyverno\n    repositoryURL: https://kyverno.github.io/kyverno/\n  setRefs:\n  - prod\nstatus:\n  matchingClusters:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster2\n    namespace: civo\n</code></pre> <pre><code>$ sveltosctl show addons\n+---------------+---------------+-----------+----------------+---------+-------------------------------+------------------------+\n|    CLUSTER    | RESOURCE TYPE | NAMESPACE |      NAME      | VERSION |             TIME              |        PROFILES        |\n+---------------+---------------+-----------+----------------+---------+-------------------------------+------------------------+\n| civo/cluster2 | helm chart    | kyverno   | kyverno-latest | 3.0.1   | 2024-03-13 10:19:06 +0100 CET | ClusterProfile/kyverno |\n+---------------+---------------+-----------+----------------+---------+-------------------------------+------------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#selected-cluster-becomes-unhealthy","title":"Selected Cluster Becomes Unhealthy","text":"<p>If the cluster selected by <code>ClusterSet</code> instance becomes unhealthy, Sveltos detects that and ClusterSet selects another healthy cluster out of the matching ones.</p> <p>In this example, cluster2 was deleted.</p> <p>Sveltos detected that and marked the cluster as not ready</p> <pre><code>$ kubectl get sveltoscluster -A\nNAMESPACE NAME   READY VERSION\ncivo    cluster1 true  v1.29.2+k3s1\ncivo    cluster2       v1.28.7+k3s1\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#clusterset-selects-new-cluster","title":"ClusterSet Selects New Cluster","text":"<p>The ClusterSet automatically picks another healthy cluster from the matching ones (e.g., cluster1) as the new active cluster.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterSet\nmetadata:\n  name: prod\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  maxReplicas: 1\nstatus:\n  matchingClusterRefs:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster1\n    namespace: civo\n  selectedClusterRefs:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster1\n    namespace: civo\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#clusterprofile-re-deploys-add-ons","title":"ClusterProfile Re-deploys Add-ons","text":"<p>The ClusterProfile reacts to the change and re-deploys its add-ons (Kyverno in this case) to the newly selected cluster (civo/cluster1).</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: kyverno\nspec:\n  helmCharts:\n  - chartName: kyverno/kyverno\n    chartVersion: v3.3.3\n    helmChartAction: Install\n    releaseName: kyverno-latest\n    releaseNamespace: kyverno\n    repositoryName: kyverno\n    repositoryURL: https://kyverno.github.io/kyverno/\n  setRefs:\n  - prod\nstatus:\n  matchingClusters:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster1\n    namespace: civo\n</code></pre> <pre><code>$ sveltosctl show addons\n+---------------+---------------+-----------+----------------+---------+-------------------------------+------------------------+\n|    CLUSTER    | RESOURCE TYPE | NAMESPACE |      NAME      | VERSION |             TIME              |        PROFILES        |\n+---------------+---------------+-----------+----------------+---------+-------------------------------+------------------------+\n| civo/cluster1 | helm chart    | kyverno   | kyverno-latest | 3.0.1   | 2024-03-13 10:27:46 +0100 CET | ClusterProfile/kyverno |\n+---------------+---------------+-----------+----------------+---------+-------------------------------+------------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/set/#summary","title":"Summary","text":"<p>ClusterSet and Set in Sveltos provide a powerful mechanism for managing cluster deployments and enabling automated failover for high availability in your applications.</p>","tags":["Kubernetes","Sveltos","add-ons","dynamic deployment","failover","high availability"]},{"location":"features/techsupport/","title":"Techsupport","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#introduction-to-tech-support-feature","title":"Introduction to Tech Support Feature","text":"<p>To simplify debugging, we need a single, user-friendly command users can execute to gather all necessary diagnostic data from both management and managed Kubernetes clusters.</p> <p>The Techsupport Custom Resource Definition (CRD) within Sveltos provides this capability, allowing for the collection of technical support information (tech support) from Kubernetes clusters, either on a scheduled basis or as needed.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#motivation","title":"Motivation","text":"<p>Because software inevitably contains bugs, it's impossible to know in advance exactly what data will be needed for debugging. This diagnostic tool aims to gather a comprehensive initial dataset, reducing the need for repeated requests to users.</p> <p>Simplicity is crucial. Managing multiple Kubernetes clusters is inherently complex and prone to issues. Therefore, the diagnostic tool must be as simple as possible to avoid adding further complexity. Given the likelihood of using multiple tools across these clusters, a single, programmable tool is required to specify what data to collect and from which clusters.</p> <p>Relying on external data collection products creates dependencies on different priorities, development styles, and release cycles. While this might seem easier initially, it ultimately introduces undesirable constraints. If a team can ship their software, they can also ship a matching, self-contained diagnostic tool. If you are using Sveltos, this is the tool for you.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#techsupport-crd","title":"Techsupport CRD","text":"<p>The below YAML example demonstrates how to configure Techsupport collection:</p> Example 1 <pre><code>cat &gt; techsupport.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: Techsupport\nmetadata:\n  name: techsupport\nspec:\n  onDemand: true  # Enables on-demand tech support collection\n\n  deliveryMethods:  # Defines how to delivery tech support\n    - name: slack   # name must be unique within a TechSupport instance\n      type: Slack   # Type can be Webex, Slack, Discord, Telegram, SMTP, SFTP\n      secretRef:\n        name: slack\n        namespace: default\n\n  # Data collection from the management cluster\n  fromManagement:\n    resources:  # Specific resources to collect\n      - group: \"\"  # Empty group indicates core Kubernetes resources\n        version: v1\n        kind: ConfigMap\n        namespace: projectsveltos\n      - group: \"\"\n        version: v1\n        kind: Secret\n        namespace: projectsveltos\n    logs:  # Collect logs from specific namespaces\n      - namespace: projectsveltos\n    events:  # Collect events from specified namespaces. Filter by type\n      - namespace: projectsveltos\n      - namespace: kube-system\n        type: Warning  # Only collect Warning events from kube-system\n\n  # Data collection from managed clusters\n  fromManaged:\n    clusterSelector:  # Filter managed clusters for collection\n      matchLabels:\n        env: fv  # Only collect data from clusters with label \"env: fv\"\n    resources:  # Specific resources to collect\n      - group: \"\"\n        version: v1\n        kind: Secret\n        namespace: projectsveltos\n      - group: apps\n        version: v1\n        kind: Deployments\n        namespace: projectsveltos\n    logs:  # Collect logs from specific namespaces\n      - namespace: projectsveltos\n    events:  # Collect events from specified namespaces and types\n      - namespace: projectsveltos\n      - namespace: kube-system\n        type: Warning  # Only collect Warning events from kube-system\nEOF\n</code></pre> <p>The Techsupport CRD allows filtering pods and resources using the label and the field selectors.</p> Example 2 <pre><code>cat &gt; techsupport_advanced.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: Techsupport\nmetadata:\n  name: hourly\nspec:\n  schedulingConfig:\n    schedule: \"00 * * * *\"\n\n  deliveryMethods:\n  - name: discord\n    type: Discord\n    secretRef:\n      name: discord\n      namespace: default\n  - name: telegram\n    type: Telegram\n    secretRef:\n      name: telegram\n      namespace: default\n\n  fromManaged:\n    clusterSelector:\n      matchLabels:\n        env: fv\n    logs:\n    - labelFilters:\n      - key: env\n        operation: Equal\n        value: production\n      - key: department\n        operation: Different\n        value: eng\n      namespace: default\n      sinceSeconds: 600\n    resources:\n    - group: \"apps\"\n      version: v1\n      kind: Deployment\n      labelFilters:\n      - key: env\n        operation: Equal\n        value: production\n      - key: department\n        operation: Different\n        value: eng\n      namespace: default\n    - group: \"\"\n      version: v1\n      kind: Service\n      labelFilters:\n      - key: env\n        operation: Equal\n        value: production\n      - key: department\n        operation: Different\n        value: eng\n      namespace: default\nEOF\n</code></pre> <ul> <li>schedulingConfig.schedule field specifies when a tech-support needs to be collected. It is in Cron format.</li> </ul> <p>Note</p> <p>The CEL language can be used as a way to express logic.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#delivery-options","title":"Delivery Options","text":"<p>Supported Delivery Options:</p> <ol> <li> Slack</li> <li>  Webex</li> <li> Discord</li> <li> Telegram</li> <li> SMTP</li> <li> SFTP</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#creating-secrets","title":"Creating Secrets","text":"<p>This guide demonstrates how to create Kubernetes <code>Secrets</code> for each supported platform to store your credentials/necessary info.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#slack","title":"Slack","text":"<pre><code>$ kubectl create secret generic slack \\\n  --from-literal=SLACK_CHANNEL_ID=&lt;your channel id&gt; \\\n  --from-literal=SLACK_TOKEN=&lt;your token&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre> <pre><code>deliveryMethods:\n- name: slack\n  type: Slack\n  secretRef:\n    name: slack\n    namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#webex","title":"Webex","text":"<pre><code>$ kubectl create secret generic webex \\\n  --from-literal=WEBEX_ROOM_ID=&lt;your channel id&gt; \\\n  --from-literal=WEBEX_TOKEN=&lt;your token&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre> <pre><code>deliveryMethods:\n- name: webex\n  type: Webex\n  secretRef:\n    name: webext\n    namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#discord","title":"Discord","text":"<pre><code>$ kubectl create secret generic discord \\\n  --from-literal=DISCORD_CHANNEL_ID=&lt;your channel id&gt; \\\n  --from-literal=DISCORD_TOKEN=&lt;your token&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre> <pre><code>deliveryMethods:\n- name: discord\n  type: Discord\n  secretRef:\n    name: discord\n    namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#telegram","title":"Telegram","text":"<pre><code>$ kubectl create secret generic telegram \\\n  --from-literal=TELEGRAM_CHAT_ID=&lt;your int64 chat id&gt; \\\n  --from-literal=TELEGRAM_TOKEN=&lt;your token&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre> <pre><code>deliveryMethods:\n- name: telegram\n  type: Telegram\n  secretRef:\n    name: telegram\n    namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#smtp","title":"SMTP","text":"<pre><code>$ kubectl create secret generic smtp \\\n  --from-literal=SMTP_RECIPIENTS=&lt;COMMA-SEPARATED EMAIL ADDRESSES&gt; \\\n  --from-literal=SMTP_CC=&lt;OPTIONAL, COMMA-SEPARATED EMAIL ADDRESSES&gt; \\\n  --from-literal=SMTP_BCC=&lt;OPTIONAL, COMMA-SEPARATED EMAIL ADDRESSES&gt; \\\n  --from-literal=SMTP_SENDER=&lt;EMAIL ADDRESS&gt; \\\n  --from-literal=SMTP_PASSWORD=&lt;OPTIONAL, SMTP PASSWORD FOR EMAIL ADDRESS IF APPLICABLE&gt; \\\n  --from-literal=SMTP_HOST=&lt;SMTP SERVER HOSTNAME&gt; \\\n  --from-literal=SMTP_PORT=&lt;OPTIONAL, SMTP SERVER PORT, DEFAULTS TO \"587\"&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre> <p>Note</p> <p>You might need to create an app password, like this.</p> <pre><code>deliveryMethods:\n- name: smtp\n  type: SMTP\n  secretRef:\n    name: smtp\n    namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#sftp","title":"SFTP","text":"<pre><code>$  kubectl create secret generic sftp \\\n  --from-literal=SFTP_HOST=&lt;SFTP Server IP&gt; \\\n  --from-literal=SFTP_PORT=&lt;SFTP Server Port&gt; \\\n  --from-literal=SFTP_USERNAME=&lt;username&gt; \\\n  --from-literal=SFTP_PASSWORD=&lt;password, optional ... set SFTP_CERT otherwise&gt;  \\\n  --from-literal=SFTP_CERT=&lt;cert, optional ... set SFTP_PASSWORD otherwise&gt;  \\\n  --from-literal=SFTP_PATH=\"/data/\" \\\n  --from-literal=SFTP_HOST_KEY=\"ssh-ed25519..., optional if not set insecure ignore host will be set\" \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre> <pre><code>deliveryMethods:\n- name: sftp\n  type: SFTP\n  secretRef:\n    name: sftp\n    namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"features/techsupport/#output-format","title":"Output Format","text":"<p>The collected techsupport contains:</p> <ol> <li>errors.txt: This file contains any errors encountered during the collection process.</li> <li>management-cluster: This directory holds logs, resources, and events collected from the management cluster.</li> <li>managed-clusters: This directory contains subdirectories for each managed cluster matching the specified cluster selector. Each managed cluster subdirectory contains collected logs, resources, and events.</li> </ol> <p>For example, if a ClusterAPI cluster named clusterapi-workload exists in the default namespace, the following directory structure would be present:</p> <pre><code>managed-clusters/Cluster/default/clusterapi-workload\n</code></pre> <p>Within each managed cluster directory, you'll find the below subdirectories:</p> <ol> <li>logs: Contains collected logs.</li> <li>resources: Contains collected Kubernetes resource information.</li> <li>events: Contains collected events.</li> </ol> <p>For instance, logs collected from the projectsveltos namespace within the clusterapi-workload cluster would be located at:</p> <pre><code>managed-clusters/Cluster/default/clusterapi-workload/logs/projectsveltos/\n</code></pre> <pre><code>\u251c\u2500\u2500 errors.txt\n\u251c\u2500\u2500 management-cluster\n\u2502   \u251c\u2500\u2500 events\n\u2502   \u2502   \u251c\u2500\u2500 one directory per namespace\n\u2502   |   |   \u251c\u2500\u2500 2025-01-03-09-08-41-component.yaml\n\u2502   \u251c\u2500\u2500 logs\n\u2502   \u2502   \u251c\u2500\u2500 one directory per namespace\n\u2502   |   |   \u251c\u2500\u2500 &lt;pod-name&gt;-&lt;container-name&gt;.yaml\n\u2502   \u251c\u2500\u2500 resources\n\u2502   \u2502   \u251c\u2500\u2500 one directory per Kind\n\u2502   |   |   \u251c\u2500\u2500 one directory per namespace\n|   |   |   |   \u251c\u2500\u2500 &lt;resource-name&gt;.yaml\n\u251c\u2500\u2500 managed-clusters\n\u2502   \u251c\u2500\u2500 one path per cluster (cluster type/cluster namespace/cluster name)\n|   \u2502   \u251c\u2500\u2500 events\n|   \u2502   \u2502   \u251c\u2500\u2500 one directory per namespace\n|   \u2502   |   |   \u251c\u2500\u2500 2025-01-03-09-08-41-component.yaml\n|   \u2502   \u251c\u2500\u2500 logs\n|   \u2502   \u2502   \u251c\u2500\u2500 one directory per namespace\n|   \u2502   |   |   \u251c\u2500\u2500 &lt;pod-name&gt;-&lt;container-name&gt;.yaml\n|   \u2502   \u251c\u2500\u2500 resources\n|   \u2502   \u2502   \u251c\u2500\u2500 one directory per Kind\n|   \u2502   |   |   \u251c\u2500\u2500 one directory per namespace\n|   |   |   |   |   \u251c\u2500\u2500 &lt;resource-name&gt;.yaml\n</code></pre> <p>For more information, refer to the CRD.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/air_gapped_installation/","title":"How to install Sveltos in an Air-Gapped Environment","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/air_gapped_installation/#what-is-sveltos","title":"What is Sveltos?","text":"<p>Sveltos is a set of Kubernetes controllers deployed in the management cluster. From the management cluster, it can manage add-ons and applications to multiple clusters.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/air_gapped_installation/#air-gapped-installation","title":"Air-Gapped Installation","text":"<p>Note</p> <p>This documentation assumes that Sveltos is installed using <code>Helm</code>.</p> <p>Sveltos can be installed in an air-gapped environment. An air-gapped environment is a highly secure environment completely cut off from the Internet and any other external networks. That implies, getting the required Sveltos images from the <code>Docker Hub</code> is not possible. This method can also be useful if the cluster runs in an environment where access to certain image registries is restricted and a custom registry or registry cache needs to be used (e.g. in large enterprises).</p> <p>When installing Sveltos using the official <code>Helm chart</code>, the drift-detection-manager and the sveltos-agent will be deployed in each managed cluster or on the management cluster when <code>agent.managementCluster=true</code> is set. However, in restricted environments, additional values are required for the installation. The drift-detection-manager and the sveltos-agent deployments will be dynamically deployed instead of from the Sveltos installation directly. This means that the patches to these deployments are done during runtime instead of upfront.</p> <p>There are two types of patches that can be applied:</p> <ul> <li>Strategic Merge Patch</li> <li>JSON Patch (RFC6902)</li> </ul> <p>Patches of both types can be persisted in a <code>ConfigMap</code> and passed to the components that will deploy the drift-detection-manager and the sveltos-agent respectively.</p> <p>The <code>Helm</code> chart offers a way to only specify the patches and the <code>ConfigMaps</code> will be created automatically so that they will be applied to the deployments before applying the drift-detection-manager and sveltos-agent.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/air_gapped_installation/#drift-detection-manager-configuration","title":"drift-detection-manager Configuration","text":"<p>To customize the drift-detection-manager deployment you can add your patches to the <code>Helm</code> values like here:</p> <pre><code>...\naddonController:\n  driftDetectionManagerPatchConfigMap:\n    data:\n      patch: |-\n        apiVersion: apps/v1\n        kind: Deployment\n        metadata:\n          name: drift-detection-manager\n        spec:\n          template:\n            spec:\n              imagePullSecrets:\n                - name: my-registry-secret\n              containers:\n                - name: manager\n                  image: registry.company.io/projectsveltos/drift-detection-manager:dev\n...\n</code></pre> <p>This example makes use of the <code>Strategic Merge Patch</code>. The key of the <code>data</code> in the <code>ConfigMap</code> (here <code>patch</code>) is arbitrary and can be changed to any other value.</p> <p>The drift-detection-manager image is located here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/air_gapped_installation/#sveltos-agent-configuration","title":"sveltos-agent Configuration","text":"<p>The sveltos-agent can be patched in the same way. In order to edit the deployment the following values can be used:</p> <pre><code>...\nclassifierManager:\n  agentPatchConfigMap:\n    data:\n      image-patch: |-\n        - op: replace\n          path: /spec/template/spec/containers/0/image\n          value: registry.company.io/projectsveltos/sveltos-agent:dev\n        - op: add\n          path: /spec/template/spec/imagePullSecrets\n          value:\n            - name: my-registry-secret\n...\n</code></pre> <p>This example makes use of <code>JSON Patch (RFC 6902)</code> to change deployment values. It's not limited to only one item in <code>data</code>.</p> <p>The sveltos-agent image is located here.</p> <p>The sveltos-agent will be deployed in the management cluster with the bellow settings.</p> <ul> <li>Custom image from private registry: registry.company.io/projectsveltos/sveltos-agent:dev</li> <li>Private registry credentials: my-registry-secret (the secret must be present in the projectsveltos namespace)<sup>1</sup></li> </ul> <p>Tip</p> <p>Replace the <code>image: registry.company.io/projectsveltos/sveltos-agent:dev</code> argument with your private registry details.</p> <p>To create the <code>my-registry-secret</code> Secret, provide your credentials directly using the command: <code>kubectl create secret docker-registry my-registry-secret -n projectsveltos --docker-server=&lt;your-registry-server&gt; --docker-username=&lt;your-name&gt; --docker-password=&lt;your-pword&gt; --docker-email=&lt;your-email&gt;</code></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/air_gapped_installation/#sveltos-applier-configuration","title":"sveltos-applier Configuration","text":"<p>Once we have registered a cluster in pull mode, Sveltos takes over the management of the sveltos-applier agent. On every Sveltos upgrade, it automatically generates the configuration needed to upgrade the applier, ensuring the agent on the managed cluster is always up-to-date.</p> <p>If we made any custom changes to the sveltos-applier configuration during the initial registration process, Sveltos provides a way to persist those changes during future upgrades. We can use the agentPatchSveltosApplierConfigMap field to provide a patch that will be applied to the applier's configuration, preventing the custom settings from being overwritten by the automatic upgrade process. This allows us to maintain full control while still benefiting from Sveltos's automated lifecycle management.</p> <pre><code>classifierManager:\n  agentPatchSveltosApplierConfigMap:\n    name: sveltos-applier-config\n    data:\n      patch: |\n        [\n          {\n            \"op\": \"replace\",\n            \"path\": \"/spec/template/spec/containers/0/args/4\",\n            \"value\": \"--secret-with-kubeconfig=pullmode-secret\"\n          }\n        ]\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/air_gapped_installation/#helm-installation","title":"Helm Installation","text":"<p>On the Kubernetes management cluster, install ProjectSveltos!</p> <pre><code>$ helm repo add projectsveltos &lt;private-repo-url&gt;\n\n$ helm repo update\n\n$ helm install projectsveltos projectsveltos/projectsveltos -n projectsveltos --create-namespace -f custom_values.yaml\n\n$ helm list -n projectsveltos\n</code></pre> <p>Note</p> <p>The <code>custom_values.yaml</code> file holds all the changes performed on the Helm chart above.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/air_gapped_installation/#next-steps","title":"Next Steps","text":"<p>Continue with the sveltoctl command-line interface (CLI) definition and installation here.</p> <ol> <li> <p>A Sveltos ClusterProfile can deploy your Secret to managed clusters. Assuming the Secret is named image-pull-secret and resides in the default namespace, it will be deployed to all clusters labeled environment: air-gapped <pre><code>    apiVersion: config.projectsveltos.io/v1beta1\n    kind: ClusterProfile\n    metadata:\n      name: deploy-resources\n    spec:\n      clusterSelector:\n        matchLabels:\n          environment: air-gapped\n      templateResourceRefs:\n      - resource:\n          apiVersion: v1\n          kind: Secret\n          name: image-pull-secret\n          namespace: default\n        identifier: ImagePullSecret\n      policyRefs:\n      - kind: ConfigMap\n        name: info\n        namespace: default\n    ---\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: info\n      namespace: default\n      annotations:\n        projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\n    data:\n      secret.yaml: |\n        {{ copy \"ImagePullSecret\" }}\n</code></pre> \u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/","title":"How to install Sveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#what-is-sveltos","title":"What is Sveltos?","text":"<p>Sveltos is a set of Kubernetes controllers deployed in the management cluster. From the management cluster, it can manage add-ons and applications to multiple clusters.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#installation-modes","title":"Installation Modes","text":"<p>Sveltos supports two modes: Mode 1 and Mode 2.</p> <ul> <li> <p>Mode 1: Will deploy up to two agents, sveltos-agent and drift-detection-manager<sup>1</sup>, in each managed cluster.</p> </li> <li> <p>Mode 2: Sveltos agents will be created, per managed cluster, in the management cluster<sup>2</sup>. The agents, while centrally located, will still monitor their designated managed cluster\u2019s API server. Sveltos leaves no footprint on managed clusters in this mode.</p> </li> </ul> <p>Tip</p> <p>Once Sveltos is deployed to the management cluster, it is automatically registered in the <code>mgmt</code> namespace with the name <code>mgmt</code>. Add-ons and applications can be deployed as soon as the appropriate Kubernetes labels are added to the cluster. For more details, see the registration section.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#mode-1-local-agent-mode-manifest","title":"Mode 1: Local Agent Mode (Manifest)","text":"<p>Execute the below commands.</p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/v1.1.1/manifest/manifest.yaml\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#mode-2-centralized-agent-mode-manifest","title":"Mode 2: Centralized Agent Mode (Manifest)","text":"<p>If you do not want to have any Sveltos agent in any managed cluster, run the commands below.</p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/v1.1.1/manifest/agents_in_mgmt_cluster_manifest.yaml\n</code></pre> <p>Warning</p> <p>Sveltos is deployed in the <code>projectsveltos</code> namespace.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#deployment-options","title":"Deployment Options","text":"<p>Sveltos can be installed as a <code>Helm Chart</code> or with <code>Kustomize</code>. By default, Mode 1 will get deployed unless otherwise specified.</p> <p>Warning</p> <p>Ensure Sveltos is deployed in the <code>projectsveltos</code> namespace.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#helm-installation","title":"Helm Installation","text":"Helm Chart Upgrade Notes <p>When deploying Sveltos with Helm, the <code>helm upgrade</code> command will not automatically update Sveltos's Custom Resource Definitions (CRDs) if they have changed in the new chart version. This is a standard Helm behavior to prevent accidental changes to CRDs that might disrupt existing resources. Manually update of the CRDs before upgrading Sveltos is required. <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/v1.0.0/manifest/crds/sveltos_crds.yaml\n</code></pre> Sveltos offers a dedicated Helm chart for managing its CRDs, which is the recommended and most reliable approach. <pre><code>$ helm install projectsveltos/sveltos-crds projectsveltos/sveltos-crds\n</code></pre></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#retrieve-latest-helm-chart","title":"Retrieve Latest Helm Chart","text":"<pre><code>$ helm repo add projectsveltos https://projectsveltos.github.io/helm-charts\n\n$ helm repo update\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#mode-1-local-agent-mode","title":"Mode 1: Local Agent Mode","text":"<pre><code>$ helm install projectsveltos projectsveltos/projectsveltos -n projectsveltos --create-namespace\n\n$ helm list -n projectsveltos\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#mode-2-centralized-agent-mode","title":"Mode 2: Centralized Agent Mode","text":"<pre><code>$ helm install projectsveltos projectsveltos/projectsveltos -n projectsveltos --create-namespace --set agent.managementCluster=true\n\n$ helm list -n projectsveltos\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#kustomize-installation","title":"Kustomize Installation","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#mode-1-local-agent-mode_1","title":"Mode 1: Local Agent Mode","text":"<pre><code>$ kustomize build https://github.com/projectsveltos/sveltos.git//kustomize/base\\?timeout\\=120\\&amp;ref\\=v1.1.1 |kubectl apply -f -\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#mode-2-centralized-agent-mode_1","title":"Mode 2: Centralized Agent Mode","text":"<pre><code>$ kustomize build https://github.com/projectsveltos/sveltos.git//kustomize/overlays/agentless-mode\\?timeout\\=120\\&amp;ref\\=v1.1.1 |kubectl apply -f -\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#sveltos-verification","title":"Sveltos Verification","text":"<p>Get the Sveltos status and verify that all pods are Up and Running.</p> <pre><code>projectsveltos access-manager-69d7fd69fc-7r4lw         2/2     Running   0  40s\nprojectsveltos addon-controller-df8965884-x7hp5        2/2     Running   0  40s\nprojectsveltos classifier-manager-6489f67447-52xd6     2/2     Running   0  40s\nprojectsveltos hc-manager-7b6d7c4968-x8f7b             2/2     Running   0  39s\nprojectsveltos sc-manager-cb6786669-9qzdw              2/2     Running   0  40s\nprojectsveltos event-manager-7b885dbd4c-tmn6m          2/2     Running   0  40s\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#optional-components","title":"Optional Components","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#sveltos-dashboard","title":"Sveltos Dashboard","text":"<p>To include the Sveltos Dashboard, follow the instructions found in the dashboard section.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>Sveltos also offers a Grafana dashboard to help users track and visualize a number of operational metrics. More can be found in the Sveltos Grafana Dashboard section.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/install/#next-steps","title":"Next Steps","text":"<p>Continue with the sveltoctl command-line interface (CLI) definition and installation here.</p> <ol> <li> <p>sveltos-agent will be deployed if there is at least one Classifier instance in the management cluster. Drift detection manager will be deployed if there is a ClusterProfile instance with SyncMode set to ContinuousWithDriftDetection.\u00a0\u21a9</p> </li> <li> <p>If Prometheus operator is not present in your management cluster, you will see (and can ignore) following error: error: unable to recognize \"https://raw.githubusercontent.com/projectsveltos/sveltos/v1.1.1/manifest/manifest.yaml\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\" \u21a9</p> </li> <li> <p>Sveltos collects minimal, anonymised data. That includes the <code>version information</code> alognside <code>cluster management data</code> (number of managed SveltosClusters, CAPI clusters, number of ClusterProdiles/Profiles and ClusterSummaries). To opt-out, for Helm-based installations use <code>helm install projectsveltos projectsveltos/projectsveltos -n projectsveltos --create-namespace --set telemetry.disabled=true</code> and for manual deployment use the <code>--disable-telemetry=true</code> flag in the Sveltos <code>addon-controller</code> configuration.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/","title":"Kubernetes add-ons management for tens of clusters quick start","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#what-is-sveltos","title":"What is Sveltos?","text":"<p>Sveltos is a set of Kubernetes controllers deployed in a management cluster. A management cluster is the Centralized Kubernetes cluster where Sveltos orchestrates and manages the deployments of Kubernetes resources to managed clusters.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#before-you-begin","title":"Before you Begin","text":"<p>To continue with the demo setup, ensure the below are satisfied.</p> <ul> <li>Docker</li> <li>GoLang</li> <li>kubectl</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#deploy-demo-environment","title":"Deploy Demo Environment","text":"<p>The main goal of Sveltos is to deploy add-ons and applications in managed Kubernetes clusters. To try out Sveltos in a test cluster, execute the commands below.</p> <pre><code>$ git clone https://github.com/projectsveltos/addon-controller &amp;&amp; cd addon-controller/\n\n$ git checkout v1.1.1\n\n$ make quickstart\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#demo-outline","title":"Demo Outline","text":"<ol> <li>A management cluster using Kind</li> <li>clusterAPI Deployment in the management cluster</li> <li>Sveltos Deployment in the management cluster</li> <li>A workload cluster powered by clusterAPI using Docker</li> </ol> Sveltos Dashboard <p>The Sveltos Dashboard is an optional component of Sveltos. To include it in the deployment, follow the instructions found in the dashboard section.</p> <p>v0.38.4 is the first Sveltos release that includes the dashboard and it is compatible with Kubernetes v1.28.0 and higher.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#validation","title":"Validation","text":"<p>Patience is a virtue \u23f3\ud83c\udf1f. The demo environment will be ready in just a few minutes \u2013 hang tight!</p> <p>If the execution was successful, you should see a management cluster named <code>sveltos-management</code> and a managed cluster named <code>clusterapi-workload</code> already set up and ready to go! \ud83d\ude80</p> <pre><code>$ kind get clusters\nclusterapi-workload\nsveltos-management\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#access-management-cluster","title":"Access Management Cluster","text":"<pre><code>$ kubectl config set-context kind-sveltos-management\n$ kind export kubeconfig --name sveltos-management\n\n$ kubectl get nodes\nNAME                               STATUS   ROLES           AGE   VERSION\nsveltos-management-control-plane   Ready    control-plane   69m   v1.32.2\nsveltos-management-worker          Ready    &lt;none&gt;          69m   v1.32.2\nsveltos-management-worker2         Ready    &lt;none&gt;          69m   v1.32.2\n\n$ kubectl get clusters --show-labels\nNAME                  CLUSTERCLASS   PHASE         AGE   VERSION   LABELS\nclusterapi-workload   quick-start    Provisioned   70m   v1.32.0   cluster.x-k8s.io/cluster-name=clusterapi-workload,env=fv,sveltos-agent=present,topology.cluster.x-k8s.io/owned=\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#deploy-helm-charts","title":"Deploy Helm Charts","text":"<p>Sveltos can deploy Helm Charts from public and private registries seamlessly. The <code>ClusterProfile</code> resource will match any cluster with the label set to env:fv. For this demo, the <code>clusterapi-workload</code> gets matched.</p> <p>Example - Kyverno Helm Chart</p> <pre><code>cat &gt; clusterprofile_kyverno.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.7\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\nEOF\n</code></pre> <pre><code>$ kubectl apply -f clusterprofile_kyverno.yaml\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#validation_1","title":"Validation","text":"<pre><code>$ kubectl config set-context kind-clusterapi-workload\n$ kind export kubeconfig --name clusterapi-workload\n\n$ kubectl get pods -n kyverno\nNAME                                             READY   STATUS    RESTARTS   AGE\nkyverno-admission-controller-6f6589ffb7-xh7dq    1/1     Running   0          102s\nkyverno-background-controller-6989c5bf45-mbkbh   1/1     Running   0          102s\nkyverno-cleanup-controller-788ffb4596-w6t46      1/1     Running   0          102s\nkyverno-reports-controller-bfb4856f8-5sd69       1/1     Running   0          102s\n</code></pre> <p>\u2705 Success! You have deployed Kyverno on your managed cluster! \ud83d\ude80</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#deploy-raw-yamljson","title":"Deploy Raw YAMl/JSON","text":"<p>Sveltos can deploy raw <code>yaml</code> and <code>json</code> resources. For this example, we will deploy Nginx in the <code>dev</code> namespace.</p> <ol> <li> <p>Connect to the management cluster     <pre><code>$ kubectl config set-context kind-sveltos-management\n$ kind export kubeconfig --name sveltos-management\n</code></pre></p> </li> <li> <p>Specify the Nginx deployment details (namespace, deployment, service) <pre><code>cat &gt; nginx_deploy.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: dev\nspec:\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: ClusterIP\nEOF\n</code></pre></p> </li> <li> <p>Create a <code>ConfigMap</code> resource referencing the <code>.yaml</code> file     <pre><code>$ kubectl create configmap nginx --from-file=nginx_deploy.yaml\n</code></pre></p> </li> <li> <p>Create and apply the <code>ClusterProfile</code> resource referencing the <code>ConfigMap</code></p> <pre><code>cat &gt; clusterprofile_nginx.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: nginx-deploy\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  policyRefs:\n  - name: nginx\n    namespace: default\n    kind: ConfigMap\nEOF\n</code></pre> <pre><code>$ kubectl apply -f clusterprofile_nginx.yaml\n</code></pre> </li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#validation_2","title":"Validation","text":"<pre><code>$ kubectl config set-context kind-clusterapi-workload\n$ kind export kubeconfig --name clusterapi-workload\n\n$ kubectl get pods,svc -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npod/nginx-96b9d695-tmqgf   1/1     Running   0          46s\n\nNAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nservice/nginx-service   ClusterIP   10.225.11.22   &lt;none&gt;        80/TCP    46s\n</code></pre> <p>\u2705 Success! As expected, Sveltos deployed all the referenced resources on your managed cluster! \ud83d\ude80</p> <p>For advanced examples, check out the Kong Gateway API deployment.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#deploy-kustomize-using-gitops","title":"Deploy Kustomize using GitOps","text":"<p>Sveltos can work alongside FluxCD to deploy content of Kustomize directories.</p> <p>Example - Kustomize</p> <pre><code>cat &gt; clusterprofile_flux.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: flux-system\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  kustomizationRefs:\n  - namespace: flux-system\n    name: flux-system\n    kind: GitRepository\n    path: ./helloWorld/\n    targetNamespace: eng\nEOF\n</code></pre> <p>The <code>ClusterProfile</code> can reference:</p> <ol> <li>GitRepository (synced with FluxCD);</li> <li>OCIRepository (synced with FluxCD);</li> <li>Bucket (synced with FluxCD);</li> <li>ConfigMap whose BinaryData section contains kustomize.tar.gz entry with tar.gz of kustomize directory;</li> <li>Secret (type addons.projectsveltos.io/cluster-profile) whose Data section contains kustomize.tar.gz entry with tar.gz of kustomize directory;</li> </ol> <p>An example list is found here. For more information about the Sveltos and FluxCD integration, check out the information.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/install/quick_start/#carvel-ytt-and-jsonnet","title":"Carvel ytt and Jsonnet","text":"<p>Sveltos offers support for Carvel ytt and Jsonnet as tools to define add-ons that can be deployed in a managed cluster. For additional information, please consult the Carvel ytt and Jsonnet sections.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/optional/dashboard/","title":"How to install Sveltos dashboard","text":"<p>Video</p> <p>To learn more about the Sveltos Dashboard, check out the Youtube Video. If you find this valuable, we would be thrilled if you shared it! \ud83d\ude0a</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/optional/dashboard/#introduction-to-sveltos-dashboard","title":"Introduction to Sveltos Dashboard","text":"<p>The Sveltos Dashboard is not part of the generic Sveltos installation. It is a manifest file that will get deployed on top. If you have not installed Sveltos, check out the documentation here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/optional/dashboard/#manifest-installation","title":"Manifest Installation","text":"<p>To deploy the Sveltos Dashboard, run the below command using the <code>kubectl</code> utility.</p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/main/manifest/dashboard-manifest.yaml\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/optional/dashboard/#helm-installation","title":"Helm Installation","text":"<pre><code>$ helm repo add projectsveltos https://projectsveltos.github.io/helm-charts\n\n$ helm repo update\n</code></pre> <pre><code>$ helm install sveltos-dashboard projectsveltos/sveltos-dashboard -n projectsveltos\n\n$ helm list -n projectsveltos\n</code></pre> <p>Warning</p> <p>v0.38.4 is the first Sveltos release that includes the dashboard and it is compatible with Kubernetes v1.28.0 and higher.</p> <p>To access the dashboard, expose the <code>dashboard</code> service in the <code>projectsveltos</code> namespace. The deployment, by default, is configured as a ClusterIP service. To expose the service externally, we can edit it to either a LoadBalancer service or use an Ingress/Gateway API.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/optional/dashboard/#authentication","title":"Authentication","text":"<p>To authenticate with the Sveltos Dashboard, we will utilise a <code>serviceAccount</code>, a <code>ClusterRoleBinding</code>/<code>RoleBinding</code> and a <code>token</code>.</p> <p>Let's create a <code>service account</code> in the desired namespace.</p> <pre><code>$ kubectl create sa &lt;user&gt; -n &lt;namespace&gt;\n</code></pre> <p>Let's provide the service account permissions to access the managed clusters in the management cluster.</p> <pre><code>$ kubectl create clusterrolebinding &lt;binding_name&gt; --clusterrole &lt;role_name&gt; --serviceaccount &lt;namespace&gt;:&lt;service_account&gt;\n</code></pre> Argument Description <code>binding_name</code> It is a descriptive name for the rolebinding. <code>role_name</code> It is one of the default cluster roles (or a custom cluster role) specifying permissions (i.e., which managed clusters this serviceAccount can see). <code>namespace</code> It is the service account's namespace. <code>service_account</code> It is the service account that the permissions are being associated with.","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/optional/dashboard/#platform-administrator-example","title":"Platform Administrator Example","text":"<pre><code>$ kubectl create sa platform-admin\n$ kubectl create clusterrolebinding platform-admin-access --clusterrole cluster-admin --serviceaccount default:platform-admin\n</code></pre> <p>Create a login token for the service account with the name <code>platform-admin</code> in the <code>default</code> namespace. The token will be valid for 24 hours.<sup>1</sup></p> <pre><code>$ kubectl create token platform-admin --duration=24h\n</code></pre> <p>Copy the token generated, login to the Sveltos Dashboard and submit it.</p> <ol> <li> <p>While the example uses cluster-admin for simplicity, the dashboard only requires read access to Sveltos CRs and Cluster API cluster instances.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/optional/grafanadashboard/","title":"Introduction to the Sveltos Grafana Dashboard","text":"<p>The Sveltos Dashboard is designed to help users monitor key operational metrics and the status of their sveltosclusters in real-time. Grafana helps users visualize this data effectively, so they can make more efficient and informed operational decisions.</p> <p></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#getting-started","title":"Getting Started","text":"<p>With the latest Sveltos release, users can take full advantage of the Sveltos Grafana dashboard. Before we start using the capabilities, ensure Grafana and Prometheus are deployed on the Sveltos management cluster.</p> <p>To allow Prometheus to collect metrics from the Sveltos management cluster, perform the below if Sveltos was installed using the Helm chart.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#helm-chart","title":"Helm Chart","text":"<pre><code>$ helm upgrade &lt;your release name&gt; projectsveltos/projectsveltos -n projectsveltos --set prometheus.enabled=true\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#interactive-import","title":"Interactive Import","text":"<p>Once Grafana and Prometheus are available, proceed by adding the Prometheus data source to Grafana and then importing the below Grafana dashboard.</p> <pre><code>https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/sveltosgrafanadashboard.json\n</code></pre> <p>Note</p> <p>Depending on the Grafana/Prometheus installation, identify the <code>serviceMonitorSelector</code> label of the Prometheus instance and import it to the Sveltos <code>servicemonitor</code> resources as a label. Check out the example below.</p> <pre><code>$ kubectl get servicemonitor -n projectsveltos\n$ kubectl patch servicemonitor addon-controller -n projectsveltos -p '{\"metadata\":{\"labels\":{\"prometheus\":\"example-label\"}}}' --type=merge\n</code></pre> <p>Confirm that all metrics are linked to their corresponding panels. The dashboard should automatically detect data connections from Prometheus.</p> <p>Refresh to begin plotting tracked metrics. Customize the dashboard to maximize utility -- by updating thresholds, adding/removing/editing panels, and transforming metrics tracked.</p> <p>Note</p> <p>Some metrics only appear on Grafana when their value is non-zero, e.g. <code>projectsveltos_reconcile_operations_total</code> and <code>projectsveltos_total_drifts</code>. As long as Prometheus and Grafana have been configured correctly, this should not be a problem.</p> <p>Detailed descriptions of the panels available on the dashboard and the tracked metrics are listed below.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#automated-import-with-sidecar","title":"Automated Import with Sidecar","text":"<p>Planning to use a sidecar for the dashboard import, feel free to use the below <code>.json</code> file. The same notes apply as in the above section.</p> <pre><code>https://raw.githubusercontent.com/projectsveltos/sveltos/main/docs/assets/sveltosgrafanadashboard_sidecar.json\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#available-metrics","title":"Available Metrics","text":"<p>Sveltos lets users track and visualize a number of key operational metrics, which include:</p> <ul> <li> <p><code>projectsveltos_cluster_connectivity_status</code>: Gauge indicating the connectivity status of each cluster, where <code>0</code> means healthy and <code>1</code> means disconnected.</p> </li> <li> <p><code>projectsveltos_kubernetes_version_info:</code> Gauge providing the Kubernetes version (major.minor.patch) of each cluster.</p> </li> <li> <p><code>projectsveltos_program_charts_time_seconds_count:</code> Counter of the total number of Helm charts deployed.</p> </li> <li> <p><code>projectsveltos_program_charts_time_seconds_bucket:</code> Histogram of the durations taken to deploy Helm charts on workload clusters.  </p> </li> <li> <p><code>projectsveltos_program_resources_time_seconds_count:</code> Counter of the total number of resources deployed.</p> </li> <li> <p><code>projectsveltos_program_resources_time_seconds_bucket:</code> Histogram of the durations taken to deploy resources on workload clusters</p> </li> <li> <p><code>projectsveltos_reconcile_operations_total:</code> Counter of the total number of reconcile operations performed for Helm charts, Resources, and Kustomizations across clusters.</p> </li> <li> <p><code>projectsveltos_total_drifts:</code> Counter of the total number of configuration drifts detected in clusters, categorized by cluster and feature.</p> </li> <li> <p>Per-Cluster <code>program_resources_time_seconds</code> Histograms: Histograms (per cluster) of durations taken to deploy resources, indexed by cluster information.</p> </li> <li> <p>Per-Cluster <code>program_charts_time_seconds</code> Histograms: Histograms (per cluster) of durations taken to deploy Helm charts, indexed by cluster information.</p> </li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#dashboard-panels","title":"Dashboard Panels","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#1-cluster-connectivity-status","title":"1. Cluster Connectivity Status","text":"<ul> <li>Type: Gauge</li> <li>Purpose: Displays the connectivity status of each Kubernetes cluster managed by Sveltos.</li> <li>Query Used: <code>projectsveltos_cluster_connectivity_status</code></li> <li>Interpretation: A \u201cHealthy\" cluster is one that is connected ( projectsveltos_cluster_connectivity_status: 0) and depicted in green. A \"Disconnected\" cluster (projectsveltos_cluster_connectivity_status: 1) is shown in red, to help users rapidly identify and address connectivity issues.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#2-cluster-kubernetes-version","title":"2. Cluster Kubernetes Version","text":"<ul> <li>Type: Table</li> <li>Purpose: Lists the Kubernetes version deployed in each sveltoscluster.</li> <li>Query Used: <code>projectsveltos_kubernetes_version_info</code></li> <li>Interpretation: The table displays clusters with their respective Kubernetes versions, to help users identify clusters in need of updates, and ensure compatibility everywhere.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#3-total-helm-charts-deployments","title":"3. Total Helm Charts Deployments","text":"<ul> <li>Type: Stat</li> <li>Purpose: Counts the number of Helm chart deployments.</li> <li>Query Used: <code>projectsveltos_program_charts_time_seconds_count</code></li> <li>Interpretation: Displays the number of Helm charts deployed across all sveltosclusters. This helps users assess the workload managed by Sveltos, track deployment activity, correlate any change in application performance with deployments, and optimize deployment strategies accordingly.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#4-total-resources-deployments","title":"4. Total Resources Deployments","text":"<ul> <li>Type: Stat</li> <li>Purpose: Counts the number of resource deployments.</li> <li>Query Used: <code>projectsveltos_program_resources_time_seconds_count</code></li> <li>Interpretation: Displays the total count of resources deployed across all sveltosclusters. This helps users assess the workload managed by Sveltos, track deployment activity, correlate any change in application performance with deployments, and optimize deployment strategies accordingly.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#5-time-to-deploy-helm-charts-in-a-profile","title":"5. Time to Deploy Helm Charts in a Profile","text":"<ul> <li>Type: Bar Chart</li> <li>Purpose: Depicts the time required for deploying Helm Charts, by visualizing the 50th and 90th percentile of deployment times.</li> <li>Queries Used: <code>histogram_quantile(0.90, projectsveltos_program_charts_time_seconds_bucket)</code> <code>histogram_quantile(0.50, projectsveltos_program_charts_time_seconds_bucket)</code></li> <li>Interpretation: Provides deeper insights into the deployment times required by Helm Charts. By plotting both the 50th and the 90th percentile, this chart intends to help users gauge performance consistency and distribution, and update their deployment strategies accordingly.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#6-time-to-deploy-resources-in-a-profile","title":"6. Time to Deploy Resources in a Profile","text":"<ul> <li>Type: Bar Chart</li> <li>Purpose: Depicts the time required for deploying Resources, by visualizing the 50th and 90th percentile of deployment times.</li> <li>Queries Used: <code>histogram_quantile(0.90, projectsveltos_program_resources_time_seconds_bucket)</code> <code>histogram_quantile(0.50, projectsveltos_program_resources_time_seconds_bucket)</code></li> <li>Interpretation: Provides deeper insights into the resource deployment times. By plotting both the 50th and the 90th percentile, this chart intends to help users gauge performance consistency and distribution, and update their deployment strategies accordingly.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#7time-to-deploy-helm-charts-in-a-profile-histogram","title":"7.Time to Deploy Helm Charts in a Profile - Histogram","text":"<ul> <li>Type: Bar Gauge</li> <li>Purpose: Provides a histogram view of deployment times for Helm charts.</li> <li>Query Used: <code>projectsveltos_program_charts_time_seconds_bucket</code></li> <li>Interpretation: Captures the distribution of deployment times for Helm charts, and allows users to track and address long-tail latencies.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#8-time-to-deploy-resources-in-a-profile-histogram","title":"8. Time to Deploy Resources in a Profile - Histogram","text":"<ul> <li>Type: Bar Gauge</li> <li>Purpose: Offers a histogram vieew of resource deployment times.</li> <li>Query Used: <code>projectsveltos_program_resources_time_seconds_bucket</code></li> <li>Interpretation: Captures the distribution of deployment times for resources, and allows users to track and address long-tail latencies.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#9-deploy-helm-charts-in-a-profile-latency-heatmap","title":"9. Deploy Helm Charts in a Profile - Latency Heatmap","text":"<ul> <li>Type: Heatmap</li> <li>Purpose: Provides a heatmap of Helm chart deployment latencies</li> <li>Query Used: <code>sum(rate(projectsveltos_program_charts_time_seconds_bucket[5m]))</code></li> <li>Interpretation: Highlights the frequency and duration of Helm chart deployment latencies to help users identify patterns and optimize deployment management.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#10-deploy-resources-in-a-profile-latency-heatmap","title":"10. Deploy Resources in a Profile - Latency Heatmap","text":"<ul> <li>Type: Heatmap</li> <li>Purpose: Provides a heatmap of Resource deployment latencies</li> <li>Query Used: <code>sum(rate(projectsveltos_program_resources_time_seconds_bucket[5m]))</code></li> <li>Interpretation: Highlights the frequency and duration of resource deployment latencies to help users identify patterns and optimize deployment management.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#11-reconciliation-operations","title":"11. Reconciliation Operations","text":"<ul> <li>Type: Time Series</li> <li>Purpose: Shows the number of reconciliation operations performed, categorized by cluster (type, namespace, name) and feature.</li> <li>Query Used: <code>projectsveltos_reconcile_operations_total</code></li> <li>Interpretation: Helps users monitor reconciliation processes triggered by Sveltos across clusters, to ensure operational stability.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/optional/grafanadashboard/#12-drifts","title":"12. Drifts","text":"<ul> <li>Type: Time Series</li> <li>Purpose: Tracks and displays drifts, categorized by cluster (type, namespace, name) and feature.</li> <li>Query Used: <code>projectsveltos_total_drifts</code></li> <li>Interpretation: Allows users to monitor configuration drifts, crucial for maintaining consistency and compliance across sveltosclusters, so they may detect and rectify discrepancies in workload clusters.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Grafana","Dashboard"]},{"location":"getting_started/sveltosctl/sveltosctl/","title":"Install sveltosctl","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/sveltosctl/#introduction-to-sveltosctl","title":"Introduction to sveltosctl","text":"<p>The Sveltosctl is the command-line interface (CLI) for Sveltos. It is an available option to query Sveltos resources but not a mandatory option.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/sveltosctl/#option-1-binaries","title":"Option 1: Binaries","text":"<p>It offers a convenient CLI experience. The Binaries for each release are available on the releases page.</p> <p>The Binaries are sufficient to register worker clusters with Sveltos, query resources etc.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/sveltosctl/#option-2-run-sveltosctl-as-pod","title":"Option 2: Run sveltosctl as Pod","text":"<p>If you choose to run sveltosctl as a pod in the management cluster, the YAML configuration can be found here.</p> <p>Once the pod is running, <pre><code>$ sveltosctl --help\n</code></pre></p> <p>You might also want to change the timezone of sveltosctl pod by using specific timezone config and hostPath volume to set specific timezone. Currently:</p> <pre><code>  volumes:\n  - hostPath:\n      path: /usr/share/zoneinfo/America/Los_Angeles\n      type: File\n    name: tz-config\n</code></pre> <p>Tip</p> <p>The Sveltos CLI pod cannot be used as a way to register a worker Kubernetes cluster. For that, use the Sveltos Binaries.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/sveltosctl/#next-steps","title":"Next Steps","text":"<p>Discover the <code>sveltoctl features</code> available here or continue with <code>Sveltos Cluster Registration</code> section.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/features/dryrun/","title":"Sveltos dry run","text":"","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"getting_started/sveltosctl/features/dryrun/#what-is-the-dryrun-mode-in-kubernetes","title":"What is the DryRun mode in Kubernetes?","text":"<p>In Kubernetes, the \"dry run\" functionality allows users to simulate the execution of the commands they want to apply.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"getting_started/sveltosctl/features/dryrun/#sveltos-dryrun-explained","title":"Sveltos DryRun - Explained","text":"<p>Sveltos takes it one step further. Imagine we are about to perform important changes to a ClusterProfile, but we are unsure what the results will be. The risk of uncertainty is big and we do not want to  cause any unwanted side effects to the Production environment. That's where the DryRun syncMode configuration comes in!</p> <p>By deploying a ClusterProfile with the <code>syncMode</code> set to <code>DryRun</code>, we can launch a simulation of all the operations that would normally be executed in a live run. The best part? No actual changes will be performed to the matching clusters during this dry run workflow.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"getting_started/sveltosctl/features/dryrun/#configuraton-example","title":"Configuraton Example","text":"<pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  syncMode: DryRun\n  ...\n</code></pre> <p>Once the dry run workflow is complete, you'll receive a detailed list of all the potential changes that would have been made to the matching cluster. This allows us to carefully inspect and validate the changes before deploying the new ClusterProfile configuration.</p> <p>If you are interested in viewing the change list, you can check out the generated Custom Resource Definition (CRD) with the name ClusterReport.</p> <p>Below is a snippet from the sveltosctl utility.</p> <pre><code>$ sveltosctl show dryrun\n\n+-------------------------------------+--------------------------+-----------+----------------+-----------+--------------------------------+------------------+\n|               CLUSTER               |      RESOURCE TYPE       | NAMESPACE |      NAME      |  ACTION   |            MESSAGE             | CLUSTER PROFILE  |\n+-------------------------------------+--------------------------+-----------+----------------+-----------+--------------------------------+------------------+\n| default/sveltos-management-workload | helm release             | kyverno   | kyverno-latest | Install   |                                | dryrun           |\n| default/sveltos-management-workload | helm release             | nginx     | nginx-latest   | Install   |                                | dryrun           |\n| default/sveltos-management-workload | :Pod                     | default   | nginx          | No Action | Object already deployed.       | dryrun           |\n|                                     |                          |           |                |           | And policy referenced by       |                  |\n|                                     |                          |           |                |           | ClusterProfile has not changed |                  |\n|                                     |                          |           |                |           | since last deployment.         |                  |\n| default/sveltos-management-workload | kyverno.io:ClusterPolicy |           | no-gateway     | Create    |                                | dryrun           |\n+-------------------------------------+--------------------------+-----------+----------------+-----------+--------------------------------+------------------+\n</code></pre> <p>To view detailed line-by-line changes for each resource, use the <code>--raw-diff</code> option with the <code>sveltosctl show dryrun</code> command.</p> <pre><code>$ sveltosctl show dryrun --raw-diff\nCluster: default/clusterapi-workload\n--- deployed: ClusterPolicy disallow-latest-tag\n+++ proposed: ClusterPolicy disallow-latest-tag\n@@ -49,10 +49,10 @@\n               name: validate-image-tag\n               skipBackgroundRequests: true\n               validate:\n-                message: Using a mutable image tag e.g. 'latest' is not allowed.\n+                message: Using a mutable image tag e.g. 'latest' is not allowed in this cluster.\n                 pattern:\n                     spec:\n                         containers:\n                             - image: '!*:latest'\n-        validationFailureAction: audit\n+        validationFailureAction: Enforce\n     status: \"\"\n\nCluster: default/clusterapi-workload\n--- deployed: Deployment nginx-deployment\n+++ proposed: Deployment nginx-deployment\n@@ -22,7 +22,7 @@\n         uid: 9ba8bbc1-02fa-4cbb-9073-fe657482277d\n     spec:\n         progressDeadlineSeconds: 600\n-        replicas: 3\n+        replicas: 1\n         revisionHistoryLimit: 10\n         selector:\n             matchLabels:\n</code></pre> <p>Sveltos can also detect changes to deployed Helm charts:</p> <pre><code>sveltosctl show dryrun\n+-----------------------------+---------------+------------+----------------+---------------+--------------------------------+-----------------------------------+\n|           CLUSTER           | RESOURCE TYPE | NAMESPACE  |      NAME      |    ACTION     |            MESSAGE             |              PROFILE              |\n+-----------------------------+---------------+------------+----------------+---------------+--------------------------------+-----------------------------------+\n| default/clusterapi-workload | helm release  | kyverno    | kyverno-latest | Update Values | use --raw-diff to see full     | ClusterProfile/deploy-kyverno     |\n|                             |               |            |                |               | diff for helm values           |                                   |\n| default/clusterapi-workload | helm release  | prometheus | prometheus     | Upgrade       | Current version: \"23.4.0\".     | ClusterProfile/prometheus-grafana |\n|                             |               |            |                |               | Would move to version:         |                                   |\n|                             |               |            |                |               | \"26.0.0\"                       |                                   |\n| default/clusterapi-workload | helm release  | grafana    | grafana        | Upgrade       | Current version: \"6.58.9\".     | ClusterProfile/prometheus-grafana |\n|                             |               |            |                |               | Would move to version: \"8.6.4\" |                                   |\n+-----------------------------+---------------+------------+----------------+---------------+--------------------------------+-----------------------------------+\n</code></pre> <pre><code>sveltosctl show dryrun --raw-diff\nProfile: ClusterProfile:deploy-kyverno Cluster: default/clusterapi-workload\n--- deployed values\n+++ proposed values\n@@ -1,6 +1,6 @@\n admissionController:\n     replicas: 3\n backgroundController:\n-    replicas: 1\n+    replicas: 3\n reportsController:\n-    replicas: 1\n+    replicas: 3\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"getting_started/sveltosctl/features/dryrun/#more-resources","title":"More Resources","text":"<p>For a quick demonstration of the dry run mode, watch the Sveltos, introduction to DryRun mode video on YouTube.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"getting_started/sveltosctl/features/visibility/","title":"Sveltos Visibility","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/features/visibility/#sveltoctl-visibility","title":"Sveltoctl Visibility","text":"<p>sveltosctl nicely displays the add-ons deployed in every Sveltos managed Kubernetes cluster.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/features/visibility/#show-addons","title":"show addons","text":"<p>show addons can be used to display a list of Kubernetes add-ons deployed in each clusters by Sveltos.</p> <p>The displayed information are:</p> <ul> <li>The CAPI/Sveltos Cluster in the form namespace/name;</li> <li>Resource/helm chart information;</li> <li>Time resource/helm chart was deployed;</li> <li>ClusterProfiles that caused resource/helm chart to be deployed in the cluster.</li> </ul> <pre><code>$ sveltosctl show addons\n+-------------------------------------+---------------+-----------+----------------+---------+-------------------------------+------------------+\n|               CLUSTER               | RESOURCE TYPE | NAMESPACE |      NAME      | VERSION |             TIME              | CLUSTER PROFILE |\n+-------------------------------------+---------------+-----------+----------------+---------+-------------------------------+------------------+\n| default/sveltos-management-workload | helm chart    | kyverno   | kyverno-latest | v2.5.0  | 2022-09-30 11:48:45 -0700 PDT | clusterprofile1   |\n| default/sveltos-management-workload | :Pod          | default   | nginx          | N/A     | 2022-09-30 13:41:05 -0700 PDT | clusterprofile2   |\n+-------------------------------------+---------------+-----------+----------------+---------+-------------------------------+------------------+\n</code></pre> <p>show addons command allows filtering by:</p> <ul> <li>clusters' namespace</li> <li>clusters' name</li> <li>ClusterProfile</li> </ul> <pre><code>$ sveltosctl show addons --help\nUsage:\n  sveltosctl show features [options] [--namespace=&lt;name&gt;] [--cluster=&lt;name&gt;] [--clusterprofile=&lt;name&gt;] [--verbose]\n\n     --namespace=&lt;name&gt;      Show features deployed in clusters in this namespace. If not specified all namespaces are considered.\n     --cluster=&lt;name&gt;        Show features deployed in cluster with name. If not specified all cluster names are considered.\n     --clusterprofile=&lt;name&gt; Show features deployed because of this clusterprofile. If not specified all clusterprofile names are considered.\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/features/visibility/#show-resources","title":"show resources","text":"<p>Using Projectsveltos can facilitate the display of information about resources in managed clusters.</p> <p>Checkout the observability section for more details.</p> <pre><code>$ sveltosctl show resources --kind=pod --namespace=nginx\n+-----------------------------+---------------+-----------+-----------------------------------+-------------------+\n|           CLUSTER           |      GVK      | NAMESPACE |               NAME                |      MESSAGE      |\n+-----------------------------+---------------+-----------+-----------------------------------+-------------------+\n| default/clusterapi-workload | /v1, Kind=Pod | nginx     | nginx-deployment-85996f8dbd-7tctq | Deployment: nginx |\n|                             |               | nginx     | nginx-deployment-85996f8dbd-tz4gd | Deployment: nginx |\n| gke/pre-production          |               | nginx     | nginx-deployment-c4f7848dc-6jtwg  | Deployment: nginx |\n|                             |               | nginx     | nginx-deployment-c4f7848dc-trllk  | Deployment: nginx |\n| gke/production              |               | nginx     | nginx-deployment-676cf9b46d-k84pb | Deployment: nginx |\n|                             |               | nginx     | nginx-deployment-676cf9b46d-mmbl4 | Deployment: nginx |\n+-----------------------------+---------------+-----------+-----------------------------------+-------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/features/visibility/#show-usage","title":"show usage","text":"<p>show usage displays below information:</p> <ul> <li>Which clusters are currently a match for a ClusterProfile;</li> <li>For ConfigMap/Secret list of clusters where their content is currently deployed.</li> </ul> <p>Such information is useful to see what clusters would be affected by a change before making such a change.</p> <pre><code>$ sveltosctl show usage\n+----------------+--------------------+----------------------------+-------------------------------------+\n| RESOURCE KIND  | RESOURCE NAMESPACE |       RESOURCE NAME        |              CLUSTERS               |\n+----------------+--------------------+----------------------------+-------------------------------------+\n| ClusterProfile |                    | kyverno                    | default/sveltos-management-workload |\n| ConfigMap      | default            | kyverno-disallow-gateway   | default/sveltos-management-workload |\n+----------------+--------------------+----------------------------+-------------------------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"getting_started/sveltosctl/features/visibility/#show-admin-rbac","title":"show admin-rbac","text":"<p>show admin-rbac can be used to display permissions granted to tenant admins in each managed clusters by the platform admin.</p> <p>If we have two clusters, a ClusterAPI powered one and a SveltosCluster, both matching label selector <code>env=internal</code> and we post RoleRequests, we get:</p> <pre><code>$ sveltosctl show admin-rbac\n+---------------------------------------------+-------+----------------+------------+-----------+----------------+-------+\n|                   CLUSTER                   | ADMIN |   NAMESPACE    | API GROUPS | RESOURCES | RESOURCE NAMES | VERBS |\n+---------------------------------------------+-------+----------------+------------+-----------+----------------+-------+\n| Cluster:default/sveltos-management-workload | eng   | build          | *          | *         | *              | *     |\n| Cluster:default/sveltos-management-workload | eng   | ci-cd          | *          | *         | *              | *     |\n| Cluster:default/sveltos-management-workload | hr    | human-resource | *          | *         | *              | *     |\n| SveltosCluster:gke/prod-cluster             | eng   | build          | *          | *         | *              | *     |\n| SveltosCluster:gke/prod-cluster             | eng   | ci-cd          | *          | *         | *              | *     |\n| SveltosCluster:gke/prod-cluster             | hr    | human-resource | *          | *         | *              | *     |\n+---------------------------------------------+-------+----------------+------------+-----------+----------------+-------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"help_and_tutorials/troubleshooting/","title":"Sveltos - Troubleshooting Section","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#introduction","title":"Introduction","text":"<p>In this section, we will help Sveltos users identify ways to troubleshoot the Sveltos installation alongside deployed add-ons down the different clusters. In general, it is a good practice to get the Sveltos version alongside the <code>sveltosctl</code> version (if used) as it is helpful for the team to provide better assistance and recommendations.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#sveltos-version","title":"Sveltos Version","text":"<pre><code>$ kubectl get job register-mgmt-cluster-job -n projectsveltos -o=jsonpath='{.spec.template.spec.containers[0].image}'\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#sveltosctl-version","title":"sveltosctl Version","text":"<pre><code>$ sveltosctl version\nI0428 09:05:01.496691 2181388 version.go:64] \"Client Version:   v0.27.0-17-2fb25f7e7a15a3\"\nI0428 09:05:01.496715 2181388 version.go:65] \"Git commit:       2fb25f7e7a15a3adc351e569f79ec1f80ae1ac7e\"\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#common-issues","title":"Common Issues","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#sveltos-installation-namespace","title":"Sveltos Installation Namespace","text":"<p>It is a requirement for Sveltos to get installed in the <code>projectsveltos</code> namespace. If Sveltos is installed in a different namespace, issues with the Kubernetes resources deployment will arise.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#sveltos-clusterprofile-profile-is-not-applied-to-the-clusters","title":"Sveltos ClusterProfile, Profile is not applied to the cluster/s","text":"<p>This is a very common case scenario where the deployed Sveltos <code>ClusterProfile</code>, and <code>Profile</code> resources are not deployed to the targeted cluster/s. This might be due to an issue with the Sveltos installation, incorrect Sveltos namespace installation, incorrect <code>cluster-label</code> set to the cluster or something else that might be disallowing the deployment.</p> <p>The Status section of a ClusterProfile or Profile instance displays all clusters that meet its criteria. These matching clusters are listed under the matchingClusters field.</p> <p>Here's an example of what the Status section might look like:</p> <pre><code>status:\n  matchingClusters:\n  - apiVersion: cluster.x-k8s.io/v1beta1\n    kind: Cluster\n    name: clusterapi-workload\n    namespace: default\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: mgmt\n    namespace: mgmt\n</code></pre> <p>To confirm if a specific cluster is considered a match for a ClusterProfile or Profile, check the matchingClusters list within the Status section of the ClusterProfile/Profile instance. If the cluster details are present in the list, then Sveltos considered it a matching cluster.</p> <p>Sveltos automatically creates a ClusterSummary resource whenever a cluster aligns with a configured ClusterProfile or Profile. This summary serves as a record of the cluster's configuration and deployment status.</p> <p>Imagine a SveltosCluster named mgmt residing in the mgmt namespace, with labels indicating environment (env=fv). Now, consider a ClusterProfile named deploy-kyverno that has a <code>clusterSelector</code> targeting clusters with the label env=fv. If these conditions are met, Sveltos will generate a ClusterSummary within the mgmt namespace. This ClusterSummary will resemble the following:</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterSummary\nmetadata:\n  name: deploy-kyverno-sveltos-mgmt\n  namespace: mgmt\nspec:\n  clusterName: mgmt\n  clusterNamespace: mgmt\n  clusterType: Sveltos\n  clusterProfileSpec:\n    clusterSelector:\n      matchLabels:\n        env: fv\n    helmCharts:\n    - chartName: kyverno/kyverno\n      chartVersion: v3.3.3\n      helmChartAction: Install\n      releaseName: kyverno-latest\n      releaseNamespace: kyverno\n      repositoryName: kyverno\n      repositoryURL: https://kyverno.github.io/kyverno/\nstatus:\n  dependencies: no dependencies\n  featureSummaries:\n  - featureID: Helm\n    hash: ujsdjTgHzPfqEx3bHtAIFcs3kjSvcuTkRCXc3o7AqrY=\n    lastAppliedTime: \"2024-05-10T14:25:58Z\"\n    status: Provisioned\n</code></pre> <p>The Status section of a ClusterSummary is crucial. It reflects whether the configured add-ons or applications are successfully deployed (Provisioned). If any issues arise during deployment, a FailureMessage field will appear, providing details about the encountered error.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#check-the-overall-sveltos-installation-management-cluster","title":"Check the Overall Sveltos Installation (Management Cluster)","text":"<pre><code>$ kubectl get pods -n projectsveltos\n</code></pre> <p>All the pods need to be in a <code>Running</code> state. If a pod is in a different state, perform the below commands to get a better understanding. The Events section or the logs provided by the pod will be sufficient to get an understanding of what might be failing.</p> <pre><code>$ kubectl describe pod &lt;pod-name&gt; -n projectsveltos\n\n$ kubectl logs &lt;pod-name&gt; -n projectsveltos -f\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#fixing-cannot-re-use-a-name-that-is-still-in-use","title":"Fixing \u2018Cannot Re-Use a Name That Is Still In Use\u2019","text":"<p>Do you encounter the error \"cannot re-use a name that is still in use\" while deploying Helm charts with Sveltos? Don't worry, this is a common issue with a straightforward solution.</p> <p>The error typically arises when a secret related to a previous Helm chart deployment still lingers in the target namespace of the managed cluster. These lingering secrets can cause naming conflicts when deploying new charts.</p> <p>Pointing to the managed cluster, run the below command to list all secrets in the desired namespace, filtering for those associated with Helm.</p> <pre><code>$ kubectl -n &lt;your namespace&gt; get secrets | grep helm\n</code></pre> <p>If the command reveals any secrets with a status of \"pending-install\", proceed and delete them.</p> <p>By removing the lingering secrets, we eliminate potential naming conflicts and pave the way for smooth Helm chart deployments.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#check-sveltos-registered-clusters","title":"Check Sveltos Registered Clusters","text":"<pre><code>$ kubectl get sveltosclusters -A\n</code></pre> <p>Ensure the Sveltos clusters are in a <code>READY=true</code> state. If Sveltos is unable to communicate with a cluster, we will see spot it directly from the output above.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#healthy-cluster-state","title":"Healthy Cluster State","text":"<pre><code>$ kubectl get sveltoscluster -A\nNAMESPACE        NAME            READY   VERSION\nprojectsveltos   vcluster-dev    true    v1.29.0+k3s1\nprojectsveltos   vcluster-prod   true    v1.29.0+k3s1\nmgmt             mgmt            true    v1.28.7+k3s1\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#unhealthy-cluster-state","title":"Unhealthy Cluster State","text":"<pre><code>$ kubectl get sveltoscluster -A\nNAMESPACE        NAME            READY   VERSION\nprojectsveltos   vcluster-dev\nmgmt             mgmt            true    v1.28.7+k3s1\n</code></pre> <p>In the output above, we can spot that the <code>vcluster-dev</code> cluster in the <code>projectsveltos</code> namespace is not in a <code>READY</code> state. That could mean network issues disallowing communication with the cluster or something is wrong with the cluster itself.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#how-to-work-with-an-unhealthy-cluster","title":"How to work with an Unhealthy Cluster?","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#step-1-ensure-the-correct-kubeconfig-provided","title":"Step 1: Ensure the correct kubeconfig provided","text":"<p>This would mean that during the registration of the cluster, the provided kubeconfig is sufficient to authenticate with the clusters.</p> <p>On a new terminal, perform the below.</p> <pre><code>$ export KUBECONFIG=&lt;directory of the provided cluster kubeconfig&gt;\n$ kubectl get nodes\n$ kubectl get pods -A\n</code></pre> <p>If you are not able to reach the cluster via the specified kubeconfig file, it could be an invalid kubeconfig. Doublecheck the file generated and ensure is the correct one.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#step-2-network-connectivity","title":"Step 2: Network Connectivity","text":"<p>If <code>Step 1</code> is fine and we can access the cluster resources with the <code>kubeconfig</code>, continue the investigation with the network and firewall setup in the environment. Ensure nothing is blocking the traffic from the management cluster to the managed cluster.</p> <p>Tip</p> <p>In specific Operating Systems (Suse Enterprise Linux), the security hardening disallowed the <code>sveltosctl</code> to validate and register the cluster due to the certificate issue. In this case, we have to import the cluster certificate to the trusted store.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#check-labels-set-to-registered-clusters","title":"Check Labels set to Registered Clusters","text":"<p><pre><code>$ kubectl get sveltosclusters -A --show-labels\n</code></pre> Ensure the labels set to the Sveltos clusters do match the labels defined in the Sveltos <code>ClusterProfile</code>, <code>Profile</code>.</p> <p>If the cluster labels are incorrect, we can overwrite them with the below command.</p> <p><pre><code>$ kubectl label sveltoscluster &lt;cluster-name&gt; -n &lt;cluster namespace&gt; env=dev --overwrite\n</code></pre> - <code>env=dev</code> is the new label set to the cluster</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#validate","title":"Validate","text":"<pre><code>$ sveltosctl show addons\n</code></pre> <p>We assume the output above is empty. This implies that Sveltos was not able to deploy an add-on to a cluster or a set of clusters. Continue with the <code>clusterprofiles</code> and <code>clustersummary</code> resource investigation.</p> <pre><code>$ kubectl get clustersummary,clusterprofile -A\n</code></pre> <p>Even if the Kubernetes add-ons are not deployed, both resources will be available to the management cluster.</p> <pre><code>$ kubectl get clusterprofile &lt;clusterprofile name&gt; -n &lt;clusterprofile namespace&gt; -o jsonpath='{.status}'\n</code></pre> <pre><code>$ kubectl get clustersummary &lt;clustersummary name&gt; -n &lt;clustersummary namespace&gt; -o jsonpath='{.status}'\n</code></pre> <p>We are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to join us.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"help_and_tutorials/troubleshooting/#debugging","title":"Debugging","text":"<p>Sveltos provides a custom resource called <code>DebuggingConfiguration</code> that allows users to configure the log level for various system components. This is useful for troubleshooting specific issues by increasing verbosity.</p> <p>The following example sets the log level of AddonManager to LogLevelDebug:</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: DebuggingConfiguration\nmetadata:\n  ...\n  ...\nspec:\n  configuration:\n  - component: AccessManager\n    logLevel: LogLevelInfo\n  - component: AddonManager\n    logLevel: LogLevelDebug\n  - component: Classifier\n    logLevel: LogLevelInfo\n  - component: DriftDetectionManager\n    logLevel: LogLevelInfo\n  - component: EventManager\n    logLevel: LogLevelInfo\n  - component: HealthCheckManager\n    logLevel: LogLevelInfo\n  - component: SveltosClusterManager\n    logLevel: LogLevelInfo\n  - component: UIBackend\n    logLevel: LogLevelInfo\n  - component: SveltosAgent\n    logLevel: LogLevelInfo\n  - component: ShardController\n    logLevel: LogLevelInfo\n  - component: ConversionWebhook\n    logLevel: LogLevelInfo\n  - component: Techsupport\n    logLevel: LogLevelInfo\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","troubleshooting"]},{"location":"observability/different_resources/","title":"Viewing resources - Projectsveltos","text":"<p>HealthCheck can also look at resources of different types together.  This allows to perform more complex evaluations. This can be useful for more sophisticated tasks, such as identifying resources that are related to each other or that have similar properties. This HealthChech instance finds all Pods instances in all namespaces mounting Secrets. It identifies and reports any pods that are accessing Secrets that have been modified since the pod's creation. This is crucial for maintaining data integrity and security,  as it prevents pods from accessing potentially outdated or compromised secrets.</p> <pre><code># This HealthChech instance finds all Pods instances in all namespaces mounting Secrets.\n# It identifies and reports any pods that are accessing Secrets that have been modified\n# since the pod's creation.\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\n    name: list-pods-with-outdated-secret-data\nspec:\n    resourceSelectors:\n    - kind: Pod\n      group: \"\"\n      version: v1\n    - kind: Secret\n      group: \"\"\n      version: v1\n    evaluateHealth: |\n      function getKey(namespace, name)\n        return namespace .. \":\" .. name\n      end\n\n      --  Convert creationTimestamp \"2023-12-12T09:35:56Z\"\n      function convertTimestampString(timestampStr)\n        local convertedTimestamp = string.gsub(\n          timestampStr,\n          '(%d+)-(%d+)-(%d+)T(%d+):(%d+):(%d+)Z',\n          function(y, mon, d, h, mi, s)\n            return os.time({\n              year = tonumber(y),\n              month = tonumber(mon),\n              day = tonumber(d),\n              hour = tonumber(h),\n              min = tonumber(mi),\n              sec = tonumber(s)\n            })\n          end\n        )\n        return convertedTimestamp\n      end\n\n      function getLatestTime(times)\n        local latestTime = nil\n        for _, time in ipairs(times) do\n          if latestTime == nil or os.difftime(tonumber(time), tonumber(latestTime)) &gt; 0 then\n            latestTime = time\n          end\n        end\n        return latestTime\n      end\n\n      function getSecretUpdateTime(secret)\n        local times = {}\n        if secret.metadata.managedFields ~= nil then\n          for _, mf in ipairs(secret.metadata.managedFields) do\n            if mf.time ~= nil then\n              table.insert(times, convertTimestampString(mf.time))\n            end\n          end\n        end\n\n        return getLatestTime(times)\n      end\n\n      function isPodOlderThanSecret(podTimestamp, secretTimestamp)\n        timeDifference = os.difftime(tonumber(podTimestamp), tonumber(secretTimestamp))\n        return  timeDifference &lt; 0\n      end\n\n      function hasOutdatedSecret(pod, secrets)\n        podTimestamp = convertTimestampString(pod.metadata.creationTimestamp)\n\n        if pod.spec.containers ~= nil then\n          for _, container in ipairs(pod.spec.containers) do\n\n            if container.env ~= nil then\n              for _, env in ipairs(container.env) do\n                if env.valueFrom ~= nil and env.valueFrom.secretKeyRef ~= nil then\n                  key = getKey(pod.metadata.namespace, env.valueFrom.secretKeyRef.name)\n                  if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                    return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                  end\n                end\n              end\n            end\n\n            if  container.envFrom ~= nil then\n              for _, envFrom in ipairs(container.envFrom) do\n                if envFrom.secretRef ~= nil then\n                  key = getKey(pod.metadata.namespace, envFrom.secretRef.name)\n                  if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                    return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                  end\n                end\n              end\n            end\n          end\n        end\n\n        if pod.spec.initContainers ~= nil then\n          for _, initContainer in ipairs(pod.spec.initContainers) do\n            if initContainer.env ~= nil then\n              for _, env in ipairs(initContainer.env) do\n                if env.valueFrom ~= nil and env.valueFrom.secretKeyRef ~= nil then\n                  key = getKey(pod.metadata.namespace, env.valueFrom.secretKeyRef.name)\n                  if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                    return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                  end\n                end\n              end\n            end\n          end\n        end\n\n        if pod.spec.volumes ~= nil then\n          for _, volume in ipairs(pod.spec.volumes) do\n            if volume.secret ~= nil then\n              key = getKey(pod.metadata.namespace, volume.secret.secretName)\n              if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                return true, \"secret \" .. key .. \" has been updated after pod creation\"\n              end\n            end\n\n            if volume.projected ~= nil and volume.projected.sources ~= nil then\n              for _, projectedResource in ipairs(volume.projected.sources) do\n                if projectedResource.secret ~= nil then\n                  key = getKey(pod.metadata.namespace, projectedResource.secret.name)\n                  if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                    return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                  end\n                end\n              end\n            end\n          end\n        end\n\n        return false\n      end\n\n      function evaluate()\n        local hs = {}\n        hs.message = \"\"\n\n        local pods = {}\n        local secrets = {}\n\n        -- Separate secrets and pods\n        for _, resource in ipairs(resources) do\n          local kind = resource.kind\n          if kind == \"Secret\" then\n            key = getKey(resource.metadata.namespace, resource.metadata.name)\n            updateTimestamp = getSecretUpdateTime(resource)\n            secrets[key] = updateTimestamp\n          elseif kind == \"Pod\" then\n            table.insert(pods, resource)\n          end\n        end\n\n        local podsWithOutdatedSecret = {}\n\n        for _, pod in ipairs(pods) do\n          outdatedData, message = hasOutdatedSecret(pod, secrets)\n          if outdatedData then\n            table.insert(podsWithOutdatedSecret, {resource= pod, status=\"Degraded\", message = message})\n          end\n        end\n\n        if #podsWithOutdatedSecret &gt; 0 then\n          hs.resources = podsWithOutdatedSecret\n        end\n        return hs\n      end\n</code></pre> <p>Following is a report generated for above HealthCheck</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheckReport\nmetadata:\n  labels:\n    projectsveltos.io/healthcheck-name: list-pods-with-outdated-secret-data\n  name: list-pods-with-outdated-secret-data\n  namespace: projectsveltos\n  ...\nspec:\n  ...\n  healthCheckName: list-pods-with-outdated-secret-data\n  resourceStatuses:\n  - healthStatus: Degraded\n    message: secret foo:dotfile-secret has been updated after pod creation\n    objectRef:\n      apiVersion: v1\n      kind: Pod\n      name: secret-dotfiles-pod\n      namespace: foo\n...\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/display_resources/","title":"Display Resources - Projectsveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/display_resources/#introduction-to-resources","title":"Introduction to Resources","text":"<p>Managing multiple clusters effectively requires a centralized location for viewing a summary of all the deployed resources. Below are some of the key reasons why it is important.</p> <ol> <li>Centralized Visibility: A central location provides a unified view of resource summaries. It allows monitoring and visualisation of the health of every cluster in one place. It simplifies issue detection, trend identification, and problem troubleshooting across multiple clusters.</li> <li>Efficient Troubleshooting and Issue Resolution: With a centralized resource view, we can swiftly identify the affected clusters when an issue arises, compare it with others, and narrow down potential causes. This comprehensive overview of resource states and dependencies enables efficient troubleshooting and quicker problem resolution.</li> <li>Enhanced Security and Compliance: Centralized resource visibility strengthens security and compliance monitoring. It enables monitoring of the cluster configurations, identify security vulnerabilities, and ensures consistent adherence to compliance standards across all clusters. We can easily track and manage access controls, network policies, and other security-related aspects from a single location.</li> </ol> <p>Using Projectsveltos can facilitate the display of information about all the resources resources in the managed clusters.</p> <p></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/display_resources/#example-display-deployment-replicas-managed-clusters","title":"Example: Display Deployment Replicas Managed Clusters","text":"<p>To showcase information about the deployments in every managed cluster, we can utilize a combination of a ClusterHealthCheck and a HealthCheck.</p> <p>Follow the steps below to set up Projectsveltos to display deployment replicas from the managed clusters.</p> <ol> <li>Create a <code>HealthCheck</code> instance that contains a Lua script responsible for examining all the deployments in the managed clusters. In the example below, deployments with a difference between the number of available replicas and requested replicas are identified as <code>degraded</code>.</li> </ol> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\n  name: deployment-replicas\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"apps\"\n    version: v1\n    kind: Deployment\n  evaluateHealth: |\n    function evaluate()\n      local statuses = {}\n\n      status = \"Progressing\"\n      message = \"\"\n\n      for _,resource in ipairs(resources) do\n        if resource.spec.replicas == 0 then\n          continue\n        end\n\n        if resource.status ~= nil then\n          if resource.status.availableReplicas ~= nil then\n            if resource.status.availableReplicas == resource.spec.replicas then\n              status = \"Healthy\"\n              message = \"All replicas \" .. resource.spec.replicas .. \" are healthy\"\n            else\n              status = \"Progressing\"\n              message = \"expected replicas: \" .. resource.spec.replicas .. \" available: \" .. resource.status.availableReplicas\n            end\n          end\n          if resource.status.unavailableReplicas ~= nil then\n            status = \"Degraded\"\n            message = \"deployments have unavailable replicas\"\n          end\n        end\n        table.insert(statuses, {resource=resource, status = status, message = message})\n      end\n\n      local hs = {}\n      if #statuses &gt; 0 then\n        hs.resources = statuses\n      end\n      return hs\n    end\n</code></pre> <ol> <li>Use the <code>ClusterHealthCheck</code> and set the <code>clusterSelector</code> field to filter the managed cluster deployments that should be examined. In the below example, all managed clusters that match the cluster label selector <code>env=fv</code> are considered.</li> </ol> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: deployment\n    type: HealthCheck\n    livenessSourceRef:\n      kind: HealthCheck\n      apiVersion: lib.projectsveltos.io/v1beta1\n      name: deployment-replicas\n  notifications:\n  - name: event\n    type: KubernetesEvent\n</code></pre> <p>The approach describe above enables us to display information about all the deployments across specific managed clusters effectively.</p> <p>To obtain a consolidated view of resource information, the sveltosctl show resources command can be used.</p> <pre><code>$ sveltosctl show resources --kind=deployment\n+-----------------------------+--------------------------+----------------+-----------------------------------------+----------------------------+\n|           CLUSTER           |           GVK            |   NAMESPACE    |                  NAME                   |          MESSAGE           |\n+-----------------------------+--------------------------+----------------+-----------------------------------------+----------------------------+\n| default/clusterapi-workload | apps/v1, Kind=Deployment | kube-system    | calico-kube-controllers                 | All replicas 1 are healthy |\n|                             |                          | kube-system    | coredns                                 | All replicas 2 are healthy |\n|                             |                          | kyverno        | kyverno-admission-controller            | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-background-controller           | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-cleanup-controller              | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-reports-controller              | All replicas 1 are healthy |\n|                             |                          | projectsveltos | sveltos-agent-manager                   | All replicas 1 are healthy |\n| gke/pre-production          |                          | gke-gmp-system | gmp-operator                            | All replicas 1 are healthy |\n|                             |                          | gke-gmp-system | rule-evaluator                          | All replicas 1 are healthy |\n|                             |                          | kube-system    | antrea-controller-horizontal-autoscaler | All replicas 1 are healthy |\n|                             |                          | kube-system    | egress-nat-controller                   | All replicas 1 are healthy |\n|                             |                          | kube-system    | event-exporter-gke                      | All replicas 1 are healthy |\n|                             |                          | kube-system    | konnectivity-agent                      | All replicas 4 are healthy |\n|                             |                          | kube-system    | konnectivity-agent-autoscaler           | All replicas 1 are healthy |\n|                             |                          | kube-system    | kube-dns                                | All replicas 2 are healthy |\n|                             |                          | kube-system    | kube-dns-autoscaler                     | All replicas 1 are healthy |\n|                             |                          | kube-system    | l7-default-backend                      | All replicas 1 are healthy |\n|                             |                          | kube-system    | metrics-server-v0.5.2                   | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-admission-controller            | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-background-controller           | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-cleanup-controller              | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-reports-controller              | All replicas 1 are healthy |\n|                             |                          | nginx          | nginx-deployment                        | All replicas 2 are healthy |\n|                             |                          | projectsveltos | sveltos-agent-manager                   | All replicas 1 are healthy |\n| gke/production              |                          | gke-gmp-system | gmp-operator                            | All replicas 1 are healthy |\n|                             |                          | gke-gmp-system | rule-evaluator                          | All replicas 1 are healthy |\n|                             |                          | kube-system    | antrea-controller-horizontal-autoscaler | All replicas 1 are healthy |\n|                             |                          | kube-system    | egress-nat-controller                   | All replicas 1 are healthy |\n|                             |                          | kube-system    | event-exporter-gke                      | All replicas 1 are healthy |\n|                             |                          | kube-system    | konnectivity-agent                      | All replicas 3 are healthy |\n|                             |                          | kube-system    | konnectivity-agent-autoscaler           | All replicas 1 are healthy |\n|                             |                          | kube-system    | kube-dns                                | All replicas 2 are healthy |\n|                             |                          | kube-system    | kube-dns-autoscaler                     | All replicas 1 are healthy |\n|                             |                          | kube-system    | l7-default-backend                      | All replicas 1 are healthy |\n|                             |                          | kube-system    | metrics-server-v0.5.2                   | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-admission-controller            | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-background-controller           | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-cleanup-controller              | All replicas 1 are healthy |\n|                             |                          | kyverno        | kyverno-reports-controller              | All replicas 1 are healthy |\n|                             |                          | projectsveltos | sveltos-agent-manager                   | All replicas 1 are healthy |\n+-----------------------------+--------------------------+----------------+-----------------------------------------+----------------------------+\n</code></pre> <p>Below are all the available options to filter what the <code>show resources</code> output displays.</p> <pre><code>--group=&lt;group&gt;: Show Kubernetes resources deployed in clusters matching this group. If not specified, all groups are considered.\n--kind=&lt;kind&gt;: Show Kubernetes resources deployed in clusters matching this Kind. If not specified, all kinds are considered.\n--namespace=&lt;namespace&gt;: Show Kubernetes resources in this namespace. If not specified, all namespaces are considered.\n--cluster-namespace=&lt;name&gt;: Show Kubernetes resources in clusters in this namespace. If not specified, all namespaces are considered.\n--cluster=&lt;name&gt;: Show Kubernetes resources in the cluster with the specified name. If not specified, all cluster names are considered.\n</code></pre> <p>Additionally, with the use of the --full option, we can display the complete details of the resources.</p> <pre><code>$ sveltosctl show resources --full\nCluster:  default/clusterapi-workload\nObject:  object:\n  apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n    creationTimestamp: \"2023-07-11T14:03:15Z\"\n    ...\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/display_resources/#example-display-deployment-images-managed-clusters","title":"Example: Display Deployment Images Managed Clusters","text":"<p>The below HealthCheck instance will instruct Sveltos to collect and display the deployment images from every managed cluster.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\n  name: deployment-replicas\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"apps\"\n    version: v1\n    kind: Deployment\n    namespace: nginx\n  evaluateHealth: |\n    function evaluate()\n      hs = {}\n      hs.status = \"Progressing\"\n      hs.message = \"\"\n      if obj.status ~= nil then\n        if obj.status.availableReplicas ~= nil then\n          if obj.status.availableReplicas == obj.spec.replicas then\n            hs.status = \"Healthy\"\n          else\n            hs.status = \"Progressing\"\n          end\n        end\n        if obj.status.unavailableReplicas ~= nil then\n          hs.status = \"Degraded\"\n        end\n      end\n\n      for i, container in ipairs(obj.spec.template.spec.containers) do\n        hs.message = \"Image: \" .. container.image\n      end\n      return hs\n    end\n</code></pre> <pre><code>$ sveltosctl show resources\n+-----------------------------+--------------------------+-----------+------------------+---------------------+\n|           CLUSTER           |           GVK            | NAMESPACE |       NAME       |       MESSAGE       |\n+-----------------------------+--------------------------+-----------+------------------+---------------------+\n| default/clusterapi-workload | apps/v1, Kind=Deployment | nginx     | nginx-deployment | Image: nginx:1.14.2 |\n| gke/pre-production          |                          | nginx     | nginx-deployment | Image: nginx:latest |\n| gke/production              |                          | nginx     | nginx-deployment | Image: nginx:1.14.2 |\n+-----------------------------+--------------------------+-----------+------------------+---------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/display_resources/#example-display-pod-names-based-on-deployment","title":"Example: Display Pod Names Based on Deployment","text":"<p>The below HealthCheck instance will instruct Sveltos to collect and display the pod names based on a specified Deployment name.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\n  name: pod-in-deployment\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: \"\"\n    version: v1\n    kind: Pod\n  evaluateHealth: |\n    function setContains(set, key)\n      return set[key] ~= nil\n    end\n\n    function evaluate()\n      hs = {}\n      hs.status = \"Healthy\"\n      hs.message = \"\"\n      hs.ignore = true\n      if obj.metadata.labels ~= nil then\n        if setContains(obj.metadata.labels, \"app\") then\n          if obj.status.phase == \"Running\" then\n            hs.ignore = false\n            hs.message = \"Deployment: \" .. obj.metadata.labels[\"app\"]\n          end\n        end\n      end\n      return hs\n    end\n</code></pre> <pre><code>$ sveltosctl show resources --kind=pod --namespace=nginx\n+-----------------------------+---------------+-----------+-----------------------------------+-------------------+\n|           CLUSTER           |      GVK      | NAMESPACE |               NAME                |      MESSAGE      |\n+-----------------------------+---------------+-----------+-----------------------------------+-------------------+\n| default/clusterapi-workload | /v1, Kind=Pod | nginx     | nginx-deployment-85996f8dbd-7tctq | Deployment: nginx |\n|                             |               | nginx     | nginx-deployment-85996f8dbd-tz4gd | Deployment: nginx |\n| gke/pre-production          |               | nginx     | nginx-deployment-c4f7848dc-6jtwg  | Deployment: nginx |\n|                             |               | nginx     | nginx-deployment-c4f7848dc-trllk  | Deployment: nginx |\n| gke/production              |               | nginx     | nginx-deployment-676cf9b46d-k84pb | Deployment: nginx |\n|                             |               | nginx     | nginx-deployment-676cf9b46d-mmbl4 | Deployment: nginx |\n+-----------------------------+---------------+-----------+-----------------------------------+-------------------+\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/display_resources/#example-display-kyverno-policyreports","title":"Example: Display Kyverno PolicyReports","text":"<p>In this example we will define an <code>HealthCheck</code> instance with a Lua script that will:</p> <ol> <li>Examine all the Kyverno PolicyReports;</li> <li>Will report all the resources in violation of the policy and the rules defined</li> </ol> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\n  name: deployment-replicas\nspec:\n  collectResources: true\n  resourceSelectors:\n  - group: wgpolicyk8s.io\n    version: v1alpha2\n    kind: PolicyReport\n  evaluateHealth: |\n    function evaluate()\n      local statuses  = {}\n      status = \"Healthy\"\n      message = \"\"\n\n      for _,resource in ipairs(resources) do\n        for i, result in ipairs(resource.results) do\n          if result.result == \"fail\" then\n            status = \"Degraded\"\n            for j, r in ipairs(result.resources) do\n              message = message .. \" \" .. r.namespace .. \"/\" .. r.name\n            end\n          end\n        end\n\n        if status ~= \"Healthy\" then\n          table.insert(statuses, {resource=resource, status = status, message = message})\n        end\n      end\n\n      local hs = {}\n      if #statuses &gt; 0 then\n        hs.resources = statuses\n      end\n\n      return hs\n    end\n</code></pre> <p>As before, we need to have a <code>ClusterHealthCheck</code> instance to instruct Sveltos which clusters to watch for.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: kyverno-policy-reports\n    type: HealthCheck\n    livenessSourceRef:\n      kind: HealthCheck\n      apiVersion: lib.projectsveltos.io/v1beta1\n      name: kyverno-policy-reports\n  notifications:\n  - name: event\n    type: KubernetesEvent\n</code></pre> <p>We assume we have deployed an nginx deployment using the latest image in the managed cluster<sup>1</sup></p> <pre><code>$ sveltosctl show resources\n+-------------------------------------+--------------------------------+-----------+--------------------------+-----------------------------------------+\n|               CLUSTER               |              GVK               | NAMESPACE |           NAME           |                 MESSAGE                 |\n+-------------------------------------+--------------------------------+-----------+--------------------------+-----------------------------------------+\n| default/sveltos-management-workload | wgpolicyk8s.io/v1alpha2,       | nginx     | cpol-disallow-latest-tag |  nginx/nginx-deployment                 |\n|                                     | Kind=PolicyReport              |           |                          | nginx/nginx-deployment-6b7f675859       |\n|                                     |                                |           |                          | nginx/nginx-deployment-6b7f675859-fp6tm |\n|                                     |                                |           |                          | nginx/nginx-deployment-6b7f675859-kkft8 |\n+-------------------------------------+--------------------------------+-----------+--------------------------+-----------------------------------------+\n</code></pre> <p><pre><code>  ---\n  apiVersion: config.projectsveltos.io/v1beta1\n  kind: ClusterProfile\n  metadata:\n    name: kyverno\n  spec:\n    clusterSelector:\n      matchLabels:\n        env: fv\n    helmCharts:\n    - chartName: kyverno/kyverno\n      chartVersion: v3.3.3\n      helmChartAction: Install\n      releaseName: kyverno-latest\n      releaseNamespace: kyverno\n      repositoryName: kyverno\n      repositoryURL: https://kyverno.github.io/kyverno/\n    policyRefs:\n    - deploymentType: Remote\n      kind: ConfigMap\n      name: kyverno-latest\n      namespace: default\n</code></pre> - The ConfigMap contains this Kyverno ClusterPolicy.</p> <pre><code>$ wget https://github.com/kyverno/policies/raw/main//best-practices/disallow-latest-tag/disallow-latest-tag.yaml\n\n$ kubectl create configmap kyverno-latest --from-file disallow-latest-tag.yaml\n</code></pre> <ol> <li> <p>To deploy Kyverno and a ClusterPolicy in each managed cluster matching the label selector env=fv we can use the below <code>ClusterProfile</code> definition.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/example_addon_notification/","title":"Notifications - Projectsveltos","text":"<p>In these examples, we will configure Sveltos to send a notification whenever all the add-ons and applications listed in the <code>ClusterProfile</code> are deployed to the clusters matching the cluster label selector set to env=fv.</p> <p>Once the defined conditions are met, a notification will be generated and send out.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack","Teams"]},{"location":"observability/example_addon_notification/#slack","title":"Slack","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: addons\n    type: Addons\n  notifications:\n  - name: slack\n    type: Slack\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: slack\n      namespace: default\n</code></pre> <p>The above <code>slack</code> secret contains the Slack channel and the token.</p> <pre><code>$ kubectl create secret generic slack \\\n  --from-literal=SLACK_CHANNEL_ID=&lt;your channel id&gt; \\\n  --from-literal=SLACK_TOKEN=&lt;your token&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack","Teams"]},{"location":"observability/example_addon_notification/#webex","title":"Webex","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: addons\n    type: Addons\n  notifications:\n  - name: webex\n    type: Webex\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: webex\n      namespace: default\n</code></pre> <p>The above <code>Webex</code> secret contains the Webex room id and the token.</p> <pre><code>$ kubectl create secret generic webex \\\n  --from-literal=WEBEX_ROOM_ID=&lt;your channel id&gt; \\\n  --from-literal=WEBEX_TOKEN=&lt;your token&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack","Teams"]},{"location":"observability/example_addon_notification/#teams","title":"Teams","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: addons\n    type: Addons\n  notifications:\n  - name: teams\n    type: Teams\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: teams\n      namespace: default\n</code></pre> <p>The above <code>Teams</code> secret contains the Teams webhook URL.</p> <pre><code>$ kubectl create secret generic teams \\\n  --from-literal=TEAMS_WEBHOOK_URL=\"&lt;your URL&gt;\" \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack","Teams"]},{"location":"observability/example_addon_notification/#discord","title":"Discord","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: addons\n    type: Addons\n  notifications:\n  - name: discord\n    type: Discord\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: discord\n      namespace: default\n</code></pre> <p>The above <code>discord</code> secret contains the Discord channel id and the token.</p> <pre><code>$ kubectl create secret generic discord \\\n  --from-literal=DISCORD_CHANNEL_ID=&lt;your channel id&gt; \\\n  --from-literal=DISCORD_TOKEN=&lt;your token&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack","Teams"]},{"location":"observability/example_addon_notification/#telegram","title":"Telegram","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: addons\n    type: Addons\n  notifications:\n  - name: telegram\n    type: Telegram\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: telegram\n      namespace: default\n</code></pre> <p>The above <code>telegram</code> secret contains the Telegram chat id and the token.</p> <pre><code>$ kubectl create secret generic telegram \\\n  --from-literal=TELEGRAM_CHAT_ID=&lt;your int64 chat id&gt; \\\n  --from-literal=TELEGRAM_TOKEN=&lt;your token&gt; \\\n  --type=addons.projectsveltos.io/cluster-profile\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack","Teams"]},{"location":"observability/example_addon_notification/#smtp","title":"SMTP","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: addons\n    type: Addons\n  notifications:\n  - name: smtp\n    type: SMTP\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: smtp\n      namespace: default\n</code></pre> <p>The above <code>smtp</code> secret contains the SMTP info.</p> <pre><code>$ kubectl create secret generic smtp \\\n  --from-literal=SMTP_RECIPIENTS=&lt;to email addresses&gt; \\\n  --from-literal=SMTP_BCC=&lt;optional, cc email addresses&gt; \\\n  --from-literal=SMTP_BCC=&lt;optional, bcc email addresses&gt; \\\n  --from-literal=SMTP_SENDER=&lt;send email address&gt; \\\n  --from-literal=SMTP_PASSWORD=&lt;sender app passowrd&gt; \\\n  --from-literal=SMTP_HOST=&lt;host&gt; \\\n  --from-literal=SMTP_PORT=&lt;OPTIONAL, SMTP SERVER PORT, DEFAULTS TO \"587\"&gt;\n  --type=addons.projectsveltos.io/cluster-profile \\\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack","Teams"]},{"location":"observability/example_addon_notification/#kubernetes-event","title":"Kubernetes event","text":"<pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: production\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: addons\n    type: Addons\n  notifications:\n  - name: kubernetes-events\n    type: KubernetesEvent\n</code></pre> <p>To list the events generated by Sveltos, use the below command.</p> <pre><code>$ kubectl get events -n default --field-selector reason=ClusterHealthCheck\nLAST SEEN   TYPE      REASON               OBJECT                  MESSAGE\n31s         Normal    ClusterHealthCheck   clusterhealthcheck/hc   cluster Capi:default/sveltos-management-workload...\n16s         Warning   ClusterHealthCheck   clusterhealthcheck/hc   cluster Capi:default/sveltos-management-workload...\n</code></pre> <p>Tip</p> <p>The Event type will be set to: <code>type: Normal</code> when the add-ons are deployed.</p> <p>The Event message contains the below information on the cluster:   1. Cluster <code>type: Capi or Sveltos</code>   1. Cluster <code>namespace</code>   1. Cluster <code>name</code></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack","Teams"]},{"location":"observability/example_cis_scan/","title":"Notifications - Projectsveltos","text":"<p>Running security scans on managed Kubernetes clusters is crucial to ensure compliance with best practices.</p> <p>Sveltos can leverage kube-bench to run a security scan on all managed clusters.</p> <p>Using the Sveltos event framework, we can centralized CIS Kubernetes Benchmark compliance results for a unified view.</p> <p></p> <p></p> <pre><code>sveltosctl show resources --kind=configmap\n+---------------+---------------------+------------+---------------------+--------------------------------+\n|    CLUSTER    |         GVK         | NAMESPACE  |        NAME         |            MESSAGE             |\n+---------------+---------------------+------------+---------------------+--------------------------------+\n| civo/cluster1 | /v1, Kind=ConfigMap | kube-bench | kube-bench-failures | [FAIL] 4.1.1 Ensure that       |\n|               |                     |            |                     | the kubelet service file       |\n|               |                     |            |                     | permissions are set to 600 or  |\n|               |                     |            |                     | more restrictive (Automated)   |\n|               |                     |            |                     | [FAIL] 4.1.5 Ensure that the   |\n|               |                     |            |                     | --kubeconfig kubelet.conf      |\n|               |                     |            |                     | file permissions are set       |\n|               |                     |            |                     | to 600 or more restrictive     |\n|               |                     |            |                     | (Automated) [FAIL] 4.1.6       |\n|               |                     |            |                     | Ensure that the --kubeconfig   |\n|               |                     |            |                     | kubelet.conf file ownership is |\n|               |                     |            |                     | set to root:root (Automated)   |\n|               |                     |            |                     | [FAIL] 4.1.9 If the kubelet    |\n|               |                     |            |                     | config.yaml configuration      |\n|               |                     |            |                     | file is being used validate    |\n|               |                     |            |                     | permissions set to 600 or      |\n|               |                     |            |                     | more restrictive (Automated)   |\n|               |                     |            |                     | [FAIL] 4.1.10 If the kubelet   |\n|               |                     |            |                     | config.yaml configuration      |\n|               |                     |            |                     | file is being used validate    |\n|               |                     |            |                     | file ownership is set          |\n|               |                     |            |                     | to root:root (Automated)       |\n|               |                     |            |                     | [FAIL] 4.2.1 Ensure that the   |\n|               |                     |            |                     | --anonymous-auth argument      |\n|               |                     |            |                     | is set to false (Automated)    |\n|               |                     |            |                     | [FAIL] 4.2.2 Ensure that       |\n|               |                     |            |                     | the --authorization-mode       |\n|               |                     |            |                     | argument is not set to         |\n|               |                     |            |                     | AlwaysAllow (Automated)        |\n|               |                     |            |                     | [FAIL] 4.2.3 Ensure that the   |\n|               |                     |            |                     | --client-ca-file argument is   |\n|               |                     |            |                     | set as appropriate (Automated) |\n|               |                     |            |                     | [FAIL] 4.2.6 Ensure that the   |\n|               |                     |            |                     | --make-iptables-util-chains    |\n|               |                     |            |                     | argument is set to             |\n|               |                     |            |                     | true (Automated) [FAIL]        |\n|               |                     |            |                     | 4.2.10 Ensure that the         |\n|               |                     |            |                     | --rotate-certificates          |\n|               |                     |            |                     | argument is not set to false   |\n|               |                     |            |                     | (Automated) [FAIL] 4.3.1       |\n|               |                     |            |                     | Ensure that the kube-proxy     |\n|               |                     |            |                     | metrics service is bound       |\n|               |                     |            |                     | to localhost (Automated)       |\n|               |                     |            |                     | [FAIL] 5.1.1 Ensure that       |\n|               |                     |            |                     | the cluster-admin role is      |\n|               |                     |            |                     | only used where required       |\n|               |                     |            |                     | (Automated) [FAIL] 5.1.2       |\n|               |                     |            |                     | Minimize access to secrets     |\n|               |                     |            |                     | (Automated) [FAIL] 5.1.3       |\n|               |                     |            |                     | Minimize wildcard use in Roles |\n|               |                     |            |                     | and ClusterRoles (Automated)   |\n|               |                     |            |                     | [FAIL] 5.1.4 Minimize access   |\n|               |                     |            |                     | to create pods (Automated)     |\n|               |                     |            |                     | [FAIL] 5.1.5 Ensure that       |\n|               |                     |            |                     | default service accounts are   |\n|               |                     |            |                     | not actively used. (Automated) |\n|               |                     |            |                     | [FAIL] 5.1.6 Ensure that       |\n|               |                     |            |                     | Service Account Tokens are     |\n|               |                     |            |                     | only mounted where necessary   |\n|               |                     |            |                     | (Automated)                    |\n| civo/cluster2 |                     | kube-bench | kube-bench-failures | [FAIL] 4.1.1 Ensure that       |\n|               |                     |            |                     | the kubelet service file       |\n|               |                     |            |                     | permissions are set to 600 or  |\n|               |                     |            |                     | more restrictive (Automated)   |\n|               |                     |            |                     | [FAIL] 4.1.5 Ensure that the   |\n|               |                     |            |                     | --kubeconfig kubelet.conf      |\n|               |                     |            |                     | file permissions are set       |\n|               |                     |            |                     | to 600 or more restrictive     |\n|               |                     |            |                     | (Automated) [FAIL] 4.1.6       |\n|               |                     |            |                     | Ensure that the --kubeconfig   |\n|               |                     |            |                     | kubelet.conf file ownership is |\n|               |                     |            |                     | set to root:root (Automated)   |\n|               |                     |            |                     | [FAIL] 4.1.9 If the kubelet    |\n|               |                     |            |                     | config.yaml configuration      |\n|               |                     |            |                     | file is being used validate    |\n|               |                     |            |                     | permissions set to 600 or      |\n|               |                     |            |                     | more restrictive (Automated)   |\n|               |                     |            |                     | [FAIL] 4.1.10 If the kubelet   |\n|               |                     |            |                     | config.yaml configuration      |\n|               |                     |            |                     | file is being used validate    |\n|               |                     |            |                     | file ownership is set          |\n|               |                     |            |                     | to root:root (Automated)       |\n|               |                     |            |                     | [FAIL] 4.2.1 Ensure that the   |\n|               |                     |            |                     | --anonymous-auth argument      |\n|               |                     |            |                     | is set to false (Automated)    |\n|               |                     |            |                     | [FAIL] 4.2.2 Ensure that       |\n|               |                     |            |                     | the --authorization-mode       |\n|               |                     |            |                     | argument is not set to         |\n|               |                     |            |                     | AlwaysAllow (Automated)        |\n|               |                     |            |                     | [FAIL] 4.2.3 Ensure that the   |\n|               |                     |            |                     | --client-ca-file argument is   |\n|               |                     |            |                     | set as appropriate (Automated) |\n|               |                     |            |                     | [FAIL] 4.2.6 Ensure that the   |\n|               |                     |            |                     | --make-iptables-util-chains    |\n|               |                     |            |                     | argument is set to             |\n|               |                     |            |                     | true (Automated) [FAIL]        |\n|               |                     |            |                     | 4.2.10 Ensure that the         |\n|               |                     |            |                     | --rotate-certificates          |\n|               |                     |            |                     | argument is not set to false   |\n|               |                     |            |                     | (Automated) [FAIL] 4.3.1       |\n|               |                     |            |                     | Ensure that the kube-proxy     |\n|               |                     |            |                     | metrics service is bound       |\n|               |                     |            |                     | to localhost (Automated)       |\n|               |                     |            |                     | [FAIL] 5.1.1 Ensure that       |\n|               |                     |            |                     | the cluster-admin role is      |\n|               |                     |            |                     | only used where required       |\n|               |                     |            |                     | (Automated) [FAIL] 5.1.2       |\n|               |                     |            |                     | Minimize access to secrets     |\n|               |                     |            |                     | (Automated) [FAIL] 5.1.3       |\n|               |                     |            |                     | Minimize wildcard use in Roles |\n|               |                     |            |                     | and ClusterRoles (Automated)   |\n|               |                     |            |                     | [FAIL] 5.1.4 Minimize access   |\n|               |                     |            |                     | to create pods (Automated)     |\n|               |                     |            |                     | [FAIL] 5.1.5 Ensure that       |\n|               |                     |            |                     | default service accounts are   |\n|               |                     |            |                     | not actively used. (Automated) |\n|               |                     |            |                     | [FAIL] 5.1.6 Ensure that       |\n|               |                     |            |                     | Service Account Tokens are     |\n|               |                     |            |                     | only mounted where necessary   |\n|               |                     |            |                     | (Automated)                    |\n| gke/cluster   |                     | kube-bench | kube-bench-failures | [FAIL] 3.2.1 Ensure that the   |\n|               |                     |            |                     | --anonymous-auth argument      |\n|               |                     |            |                     | is set to false (Automated)    |\n|               |                     |            |                     | [FAIL] 3.2.2 Ensure that       |\n|               |                     |            |                     | the --authorization-mode       |\n|               |                     |            |                     | argument is not set to         |\n|               |                     |            |                     | AlwaysAllow (Automated)        |\n|               |                     |            |                     | [FAIL] 3.2.3 Ensure that the   |\n|               |                     |            |                     | --client-ca-file argument is   |\n|               |                     |            |                     | set as appropriate (Automated) |\n|               |                     |            |                     | [FAIL] 3.2.6 Ensure that the   |\n|               |                     |            |                     | --protect-kernel-defaults      |\n|               |                     |            |                     | argument is set to true        |\n|               |                     |            |                     | (Manual) [FAIL] 3.2.9 Ensure   |\n|               |                     |            |                     | that the --event-qps argument  |\n|               |                     |            |                     | is set to 0 or a level which   |\n|               |                     |            |                     | ensures appropriate event      |\n|               |                     |            |                     | capture (Automated) [FAIL]     |\n|               |                     |            |                     | 3.2.12 Ensure that the         |\n|               |                     |            |                     | RotateKubeletServerCertificate |\n|               |                     |            |                     | argument is set to true        |\n|               |                     |            |                     | (Automated)                    |\n+---------------+---------------------+------------+---------------------+--------------------------------+\n</code></pre> <p>Tip</p> <p>The YAML defintions can be found here.</p>","tags":["Kubernetes","CIS Kubernetes Benchmark"]},{"location":"observability/example_crashloopbackoff_notification/","title":"Notifications - Projectsveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/example_crashloopbackoff_notification/#example-slack-notification-for-pods-in-crashloopbackoff","title":"Example: Slack Notification for Pods in Crashloopbackoff","text":"<p>The below <code>HealthCheck</code> and <code>ClusterhealthCheck</code> YAML definitions can be used to instruct Sveltos to:</p> <ol> <li>Detect pods in a crashloopbackoff state for every cluster that matched the labels __env=fv```</li> <li>Send a Slack notification when an event is detected</li> </ol> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\n  name: crashing-pod\nspec:\n  resourceSelectors:\n  - group: \"\"\n    version: v1\n    kind: Pod\n  evaluateHealth: |\n    function evaluate()\n      local statuses = {}\n\n      for _,resource in ipairs(resources) do\n        ignore = true\n        if resource.status.containerStatuses then\n          local containerStatuses = resource.status.containerStatuses\n          for _, containerStatus in ipairs(containerStatuses) do\n            if containerStatus.state.waiting and containerStatus.state.waiting.reason == \"CrashLoopBackOff\" then\n              ignore = false\n              status = \"Degraded\"\n              message = resource.metadata.namespace .. \"/\" .. resource.metadata.name .. \":\" .. containerStatus.state.waiting.message\n              if containerStatus.lastState.terminated and containerStatus.lastState.terminated.reason then\n                message = message .. \"\\nreason:\" .. containerStatus.lastState.terminated.reason\n              end\n            end\n          end\n          if not ignore then\n            table.insert(statuses, {resource=resource, status = status, message = message})\n          end\n        end\n      end\n\n      local hs = {}\n      if #statuses &gt; 0 then\n        hs.resources = statuses\n      end\n      return hs\n    end\n</code></pre> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: crashing-pod\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: crashing-pod\n    type: HealthCheck\n    livenessSourceRef:\n      kind: HealthCheck\n      apiVersion: lib.projectsveltos.io/v1beta1\n      name: crashing-pod\n  notifications:\n  - name: slack\n    type: Slack\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: slack\n      namespace: default\n</code></pre> <p>Tip</p> <p>The YAML defintions can be found here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/example_degrade_certificate_notification/","title":"Notifications - Projectsveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/example_degrade_certificate_notification/#example-degrade-certificates-notification","title":"Example: Degrade Certificates Notification","text":"<p>The below <code>HealthCheck</code> YAML definition will detect degrade Certificates.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\nname: failed-cert\nspec:\n  resourceSelectors:\n  - group: \"cert-manager.io\"\n    version: \"v1\"\n    kind: \"Certificate\"\n  evaluateHealth: |\n    function evaluate()\n      local statuses = {}\n\n      for _,resource in ipairs(resources) do\n        if resource.status ~= nil then\n          if resource.status.conditions ~= nil then\n            for i, condition in ipairs(resource.status.conditions) do\n              if condition.type == \"Ready\" and condition.status == \"False\" then\n                status = \"Degraded\"\n                message = condition.message\n                table.insert(statuses, {resource=resource, status = status, message = message})\n              end\n            end\n          end\n        end\n      end\n\n      local hs = {}\n      if #statuses &gt; 0 then\n        hs.resources = statuses\n      end\n      return hs\n    end\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/example_deployment_health_notification/","title":"Notifications - Projectsveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/example_deployment_health_notification/#example-deployment-health-notification","title":"Example: Deployment Health Notification","text":"<p>The below <code>HealthCheck</code> YAML definition considers all the cluster Deployments. It matches any <code>Deployment</code>:</p> <ol> <li>With the number of available replicas matching the number of requested replicas, it is marked as <code>Healthy</code>;</li> <li>With the number of available replicas different than the number of requested replicas, it is marked as <code>Progressing</code>;</li> <li>With the number of unavailable replicas set and different than zero, it is marked as <code>Degraded</code>.</li> </ol> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\n  name: deployment-replicas\nspec:\n  resourceSelectors:\n  - group: \"apps\"\n    version: v1\n    kind: Deployment\n  evaluateHealth: |\n    function evaluate()\n      local statuses = {}\n      status = \"Progressing\"\n      message = \"\"\n\n      for _,resource in ipairs(resources) do\n        if resource.status ~= nil then\n          if resource.status.availableReplicas ~= nil then\n            if resource.status.availableReplicas == resource.spec.replicas then\n              status = \"Healthy\"\n            else\n              status = \"Progressing\"\n              message = \"expected replicas: \" .. resource.spec.replicas .. \" available: \" .. resource.status.availableReplicas\n            end\n          end\n          if resource.status.unavailableReplicas ~= nil then\n            status = \"Degraded\"\n            message = \"deployments have unavailable replicas\"\n          end\n        end\n        table.insert(statuses, {resource=resource, status = status, message = message})\n      end\n\n      local hs = {}\n      if #statuses &gt; 0 then\n        hs.resources = statuses\n      end\n      return hs\n    end\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/","title":"Notifications - Projectsveltos","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/#introduction-to-notifications","title":"Introduction to Notifications","text":"<p>When a ClusterProfile is instantiated using Sveltos, it automatically watches for clusters that match the ClusterProfile clusterSelector field. When a match is found, Sveltos deploys all of the referenced add-ons, such as helm charts or Kubernetes resources.</p> <p>Once the necessary add-ons are deployed, there might be a need to perform other operations on the cluster, such as running a CI/CD pipeline. However, it is important to ensure that the cluster is healthy, i.e., all necessary add-ons are deployed, before proceeding. Sveltos can be configured to assess the cluster health and send notifications if any changes are detected.</p> <p>The notifications can be used by other tools to perform additional actions or trigger workflows. Sveltos will ensure the necessary Kubernetes add-ons are deployed and managed while ensuring the health and stability of the clusters.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/#clusterhealthcheck","title":"ClusterHealthCheck","text":"<p>ClusterHealthCheck is the CRD that can be used to:</p> <ol> <li>Define the cluster health checks;</li> <li>Instruct Sveltos when and how to send notifications</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/#cluster-selection","title":"Cluster Selection","text":"<p>The <code>clusterSelector</code> field is a Kubernetes label selector. Sveltos uses it to detect all the clusters to assess health and send out notifications.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/#livenesschecks","title":"LivenessChecks","text":"<p>The <code>livenessCheck</code> field is a list of cluster liveness checks to be evaluated.</p> <p>The supported types are:</p> <ol> <li>Addons: Addons type instructs Sveltos to evaluate state of add-ond deployment in such a cluster;</li> <li>HealthCheck: HealthCheck type allows to define a custom health check for any Kubernetes type.</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/#notifications","title":"Notifications","text":"<p>The notifications fields is a list of all notifications to be sent when the liveness check state changes.</p> <p>The supported types are:</p> <ol> <li> Slack</li> <li> Webex</li> <li> Teams</li> <li> Discord</li> <li> Telegram</li> <li> SMTP</li> <li> Kubernetes events (reason=ClusterHealthCheck)</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/#healthcheck-crd","title":"HealthCheck CRD","text":"<p>To define a custom health check, simply create a HealthCheck instance.</p> <p>The <code>HealthCheck</code> specification can can contain the below fields:</p> <ol> <li><code>`Spec.Group*/*Spec.Version*/*Spec.Kind</code> fields indicates which Kubernetes resources the HealthCheck is for. Sveltos will watch and evaluate these resources anytime a change occurs;</li> <li><code>Spec.Namespace</code> field can be used to filter resources by namespace;</li> <li><code>Spec.LabelFilters</code> field can be used to filter resources by labels;</li> <li><code>Spec.Script</code> can contain a Lua script, which define a custom health check.</li> </ol> <p>The Lua script must contain the function <code>evaluate()</code> that returns a table with a status field (Healthy/Progressing/Degraded/Suspended) and optional message field.</p> <p>When providing Sveltos with a Lua script, Sveltos expects following format:</p> <ol> <li>Must contain a function <code>function evaluate()</code>. The function is directly invoked and passed a Kubernetes resource (inside the function <code>obj</code> represents the passed in Kubernetes resource); 2.Must return a Lua table with following fields:</li> <li><code>status</code>: which can be set to either one of   Healthy/Progressing/Degraded/Suspended;</li> <li><code>ignore</code>: is a boolean field indicating whether Sveltos should ignore the resource. If hs.ignore is set to <code>true</code>, Sveltos will ignore the resource causing that result;</li> <li><code>message</code>: is a string that can be set and Sveltos will print a message if it is set</li> </ol> <p>Note</p> <p>Keep in mind the CEL language can be used as a way to express logic.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/#example-configmap-healthcheck","title":"Example: ConfigMap HealthCheck","text":"<p>In the follwoing example<sup>1</sup>, we are creating an HealthCheck that watches all the ConfigMap Kubernetes resources.</p> <p><code>hs</code> is the health status object we will return to Sveltos. It must contain a <code>status</code> attribute which indicates whether the resource is <code>Healthy</code>, <code>Progressing</code>, <code>Degraded</code> or <code>Suspended</code>. By default,the status is set to <code>Healthy</code> and the <code>hs.ignore</code> is set to <code>true</code>, as we do not want to mess with the status of other, non-OPA ConfigMaps. Optionally, the health status object may also contain a message.</p> <p>In this example, we want to identify if the ConfigMap is an OPA policy or another kind of ConfigMap. If it is a OPA policy, we retrieve the value of the openpolicyagent.org/policy-status annotation. The annotation is set to {\"status\":\"ok\"} if the policy loaded successfully. If errors occurred during loading (e.g., the policy contained a syntax error) the cause will be reported in the annotation. Depending on the value of the annotation, we set the status and message attributes appropriately.</p> <p>At the end, we return the <code>hs</code> object to Sveltos.</p> <p>Example - HealthCheck Definition</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: HealthCheck\nmetadata:\n  name: opa-configmaps\nspec:\n  resourceSelectors:\n  - group: \"\"\n    version: v1\n    kind: ConfigMap\n  evaluateHealth: |\n    function evaluate()\n      statuses = {}\n\n      status = \"Healthy\"\n      message = \"\"\n\n      local opa_annotation = \"openpolicyagent.org/policy-status\"\n\n      for _,resource in ipairs(resources) do\n        if resource.metadata.annotations ~= nil then\n          if resource.metadata.annotations[opa_annotation] ~= nil then\n            if obj.metadata.annotations[opa_annotation] == '{\"status\":\"ok\"}' then\n              status = \"Healthy\"\n              message = \"Policy loaded successfully\"\n            else\n              status = \"Degraded\"\n              message = obj.metadata.annotations[opa_annotation]\n            end\n            table.insert(statuses, {resource=resource, status = status, message = message})\n          end\n        end\n      end\n      local hs = {}\n      if #statuses &gt; 0 then\n        hs.resources = statuses\n      end\n      return hs\n    end\n</code></pre> <p>The below <code>ClusterHealthCheck</code> resources, will send a Webex message as notification if a ConfigMap with an incorrect OPA policy is detected.</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: ClusterHealthCheck\nmetadata:\n  name: hc\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  livenessChecks:\n  - name: deployment\n    type: HealthCheck\n    livenessSourceRef:\n      kind: HealthCheck\n      apiVersion: lib.projectsveltos.io/v1beta1\n      name: opa-configmaps\n  notifications:\n  - name: webex\n    type: Webex\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: webex\n      namespace: default\n</code></pre> <p>Tip</p> <p>If the Lua language is preferred to write the HealthCheck, it might be handy to validate the definition before use.</p> <p>This can be achieved by cloning the sveltos-agent repository. In the pkg/evaluation/healthchecks directory, create a directory for the deployed resources if it does not exist already. If a directory already exists, create a subdirectory instead.</p> <p>In the directory or the subdirectory, create the below points.</p> <ol> <li>The file named healthcheck.yaml containing the HealthCheck instance with Lua script;</li> <li>The file named healthy.yaml containing a Kubernetes resource supposed to be Healthy for the Lua script created in #1 (this is optional);</li> <li>The file named progressing.yaml containing a Kubernetes resource supposed to be Progressing for the Lua script created in #1 (this is optional);</li> <li>The file named degraded.yaml containing a Kubernetes resource supposed to be Degraded for the Lua script created in #1 (this is optional);</li> <li>The file named suspended.yaml containing a Kubernetes resource supposed to be Suspended for the Lua script created in #1 (this is optional);</li> <li>make test</li> </ol> <p>As mentioned above, one of the following statuses will get returned (<code>Healthy</code>, <code>Progressing</code>, <code>Degraded</code> or <code>Suspended</code>) once the resources are verified.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"observability/notifications/#notifications-and-multi-tenancy","title":"Notifications and multi-tenancy","text":"<p>If the below label is set on the HealthCheck instance created by the tenant admin</p> <pre><code>projectsveltos.io/admin-name: &lt;admin&gt;\n</code></pre> <p>Sveltos will ensure the tenant admin can define notifications only by looking at the resources it has been authorized to by platform admin.</p> <p>Sveltos suggests using the below Kyverno ClusterPolicy, which takes care of adding proper labels to each HealthCheck at creation time.</p> <pre><code>---\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: add-labels\n  annotations:\n    policies.kyverno.io/title: Add Labels\n    policies.kyverno.io/description: &gt;-\n      Adds projectsveltos.io/admin-name label on each HealthCheck\n      created by tenant admin. It assumes each tenant admin is\n      represented in the management cluster by a ServiceAccount.\nspec:\n  background: false\n  rules:\n  - exclude:\n      any:\n      - clusterRoles:\n        - cluster-admin\n    match:\n      all:\n      - resources:\n          kinds:\n          - HealthCheck\n    mutate:\n      patchStrategicMerge:\n        metadata:\n          labels:\n            +(projectsveltos.io/serviceaccount-name): '{{serviceAccountName}}'\n            +(projectsveltos.io/serviceaccount-namespace): '{{serviceAccountNamespace}}'\n    name: add-labels\n  validationFailureAction: enforce\n</code></pre> <ol> <li> <p>Credit for this example to https://blog.cubieserver.de/2022/argocd-health-checks-for-opa-rules/\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","Sveltos","Slack"]},{"location":"reference/resource-owners/","title":"Resource and Controller Mapping","text":"<p>The following table maps Sveltos resources to their managing controllers and shows where resources are located in different deployment modes (Local vs Centralized Agent Mode). Use this as a reference for operational design, troubleshooting, and understanding controller responsibilities in your Sveltos deployment.</p> Resource Managing Controller Creating by Resource Location (Local Agent Mode) Resource Location (Centralized Agent Mode) Annotation AccessRequests Access Manager User Management Cluster Management Cluster ClassifierReports Classifier Manager Sveltos Agent Managed Cluster Management Cluster Classifiers Classifier Manager User / Install script Management Cluster Management Cluster ClusterConfigurations Unknown Unknown Management Cluster Management Cluster ClusterHealthChecks HealthCheck Manager User Management Cluster Management Cluster ClusterProfiles / Profiles Add-on Controller Event Manager / User Management Cluster Management Cluster ClusterReports Unknown Unknown Management Cluster Management Cluster ClusterSets / Sets Add-on Controller User Management Cluster Management Cluster ClusterSummaries Add-on Controller Add-on Controller Management Cluster Management Cluster Useful when debugging ClusterProfiles ConfigMap/Secret for EventTriggers Event Manager User Management Cluster Management Cluster DebuggingConfigurations All Controllers User / Install script Management Cluster Management Cluster Useful for adjusting controller log level when filing bug reports EventReports Event Manager Sveltos Agent Managed Cluster Management Cluster Useful when debugging EventSources EventSources Event Manager User Managed Cluster Management Cluster Event Manager deploys EventSource instances to managed clusters EventTriggers Event Manager User Management Cluster Management Cluster HealthCheckReports HealthCheck Manager Sveltos Agent Managed Cluster Management Cluster HealthChecks HealthCheck Manager User Managed Cluster Management Cluster HealthCheck Manager deploys HealthCheck instances to managed clusters ReloaderReports Add-on Controller Sveltos Agent Managed Cluster Management Cluster Reloaders Add-on Controller User Management Cluster Management Cluster ResourceSummaries Add-on Controller Add-on Controller Management Cluster Management Cluster RoleRequests Access Manager User Management Cluster Management Cluster SveltosClusters SveltosCluster Controller User Management Cluster Management Cluster Periodically connects to cluster and runs readiness and liveness checks TechSupports TechSupport Controller TechSupport Controller Management Cluster Management Cluster Collects logs/events/resources from managed cluster and management cluster Sveltos Agent (Deployment) Not Applicable Classifier Manager Managed Cluster Management Cluster Ensure one agent per cluster. If no agent is present, EventReports are not generated; recreate the Classifier to redeploy the agent."},{"location":"reference/resource-owners/#notes","title":"Notes","text":"<ul> <li>Managing Controller: Controller that monitors the resource state and makes adjustments as needed</li> <li>Creating Controller: Entity (controller or user) that initially creates the resource</li> <li>Resource Location (Local Agent Mode): Where the resource is located when using Local Agent Mode (Mode 1) - agents deployed in managed clusters</li> <li>Resource Location (Centralized Agent Mode): Where the resource is located when using Centralized Agent Mode (Mode 2) - agents centralized in management cluster</li> <li>User: Resources created manually by users</li> <li>Unknown: Items where a clear relationship has not yet been established</li> <li>All Controllers: Resources managed collaboratively by all controllers</li> </ul>"},{"location":"reference/resource-owners/#controller-overview","title":"Controller Overview","text":"<p>Based on the Sveltos architecture, here are the main controllers and their responsibilities:</p> <p>Deployment Modes:</p> <ul> <li>Local Agent Mode (Mode 1): Sveltos agents (sveltos-agent and drift-detection-manager) are deployed in each managed cluster</li> <li>Centralized Agent Mode (Mode 2): Sveltos agents are created per managed cluster in the management cluster, leaving no footprint on managed clusters</li> </ul> <p>Controllers:</p> <ul> <li>SveltosCluster Controller: Manages SveltosCluster instances, periodically connects to clusters and runs readiness/liveness checks</li> <li>Add-on Controller: Manages ClusterProfile/Profile instances, creates ClusterSummary for matching clusters and deploys resources</li> <li>Classifier Manager: Deploys Classifier instances to managed clusters and sveltos-agent deployment, processes ClassifierReports to update cluster labels</li> <li>Event Manager: Manages EventTrigger instances, deploys EventSource instances to managed clusters and processes EventReports to create new ClusterProfiles</li> <li>HealthCheck Manager: Manages ClusterHealthChecks, deploys HealthCheck instances to managed clusters and processes HealthReports for notifications</li> <li>TechSupport Controller: Manages TechSupport instances, collects logs/events/resources from clusters and delivers tech support messages</li> <li>Shard Controller: Manages deployment and undeployment of Sveltos instances when clusters are marked as belonging to different shards</li> </ul>"},{"location":"reference/resource-owners/#related-information","title":"Related Information","text":"<p>For more details on Sveltos architecture and each controller, please refer to the following documents:</p> <ul> <li>Installing Sveltos</li> <li>Cluster Registration</li> <li>Add-on Distribution</li> </ul>"},{"location":"register/readiness_liveness_checks/","title":"Register Cluster Readines and Liveness Check","text":"<p>When a cluster is registered with Sveltos, Sveltos attempts to connect to its <code>API server</code>. A successful connection results in the cluster's status being set to a\u00a0<code>Ready</code> state. This status is a prerequisite for Sveltos to deploy add-ons and applications to clusters.</p> <p>Note</p> <p>Lua and CEL languages can be used as a way to express logic.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/readiness_liveness_checks/#sveltoscluster-readiness-checks","title":"SveltosCluster Readiness Checks","text":"<p>While basic API server connectivity is a good initial indicator, it is not always a comprehensive measure of a cluster's readiness condition. In many cases, a cluster might be reachable via its API server, but be in a transitional state. For example, the control plane might be running, but the worker nodes, responsible for running workloads, might still be joining the cluster. As a result, deploying applications to such a cluster could lead to failures. Sveltos needs a more nuanced approach to determine when a cluster is in a <code>stable</code> and <code>operational</code> state. To address these limitations, Sveltos provides a mechanism for defining custom <code>readiness checks</code> for each registered cluster. The <code>readiness checks</code> allow administrators to specify the same conditions that must be met before Sveltos considers a cluster <code>Ready</code>.</p> <p>The below YAML configuration defines a <code>SveltosCluster</code> resource that includes a readiness check. The check verifies the existence of at least one worker node in the cluster by iterating through all nodes and confirming that at least one node lacks the node-role.kubernetes.io/control-plane label, which is assumed to be absent on worker nodes in this particular setup.</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: SveltosCluster\nmetadata:\n  name: staging\n  namespace: default\nspec:\n  kubeconfigName: clusterapi-workload-sveltos-kubeconfig\n  readinessChecks:\n  - name: worker-node-check\n    condition: |-\n      function evaluate()\n        hs = {}\n        hs.pass = false\n\n        for _, resource in ipairs(resources) do\n          if  not (resource.metadata.labels and resource.metadata.labels[\"node-role.kubernetes.io/control-plane\"]) then\n            hs.pass = true\n          end\n        end\n\n        return hs\n      end\n\n    resourceSelectors:\n    - group: \"\"\n      kind: Node\n      version: v1\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/readiness_liveness_checks/#sveltoscluster-liveness-checks","title":"SveltosCluster Liveness Checks","text":"<p>In addition to <code>readiness checks</code>, Sveltos employs <code>liveness checks</code>.  While <code>readiness checks</code> determine if a cluster is initially ready for deployments, the <code>liveness checks</code> ensure the cluster remains healthy over time.  Once a cluster is marked as <code>Ready</code> (readiness checks), Sveltos periodically connects to its API server and evaluates the defined <code>liveness checks</code>. Only when all <code>liveness checks</code> pass, Sveltos consider the cluster as Healthy.</p> <p>The syntax for the <code>liveness checks</code> is identical to that of the <code>readiness checks</code>.  They are defined within the <code>SveltosCluster</code> specification using the <code>livenessChecks</code> field.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register-cluster/","title":"Register Cluster","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register-cluster/#sveltos-cluster-registration","title":"Sveltos Cluster Registration","text":"<p>Sveltos supports automatic discovery of clusters powered by ClusterAPI (CAPI). When Sveltos is deployed in a management cluster with CAPI, no further action is required for add-on management. It will watch for clusters.cluster.x-k8s.io instances and program those accordingly.</p> <p>For any other types of clusters, whether on-prem or in the cloud, Sveltos provides seamless management of Kubernetes add-ons. Check out the examples section.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register-cluster/#register-cluster","title":"Register Cluster","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register-cluster/#sveltosctl-registration","title":"sveltosctl Registration","text":"<p>It is recommended, but not required, to use the sveltosctl for cluster registration. Alternatively, to programmatically register clusters, consult the section.</p> <pre><code>$ sveltosctl register cluster \\\n    --namespace=monitoring \\\n    --cluster=prod-cluster \\\n    --kubeconfig=~/.kube/prod-cluster.config \\\n    --labels=environment=production,tier=backend\n</code></pre> Parameter Description <code>--namespace</code> The namespace in the management cluster where Sveltos stores information about the registered cluster. <code>--cluster</code> The name to identify the registered cluster within Sveltos. <code>--kubeconfig</code> The path to the kubeconfig file for the registered cluster. <code>--labels</code> (Optional) Comma-separated key-value pairs to define labels for the registered cluster. <p>Note</p> <p>If the <code>kubeconfig</code> has multiple contexes, and the default context points to the management cluster, use the --fleet-cluster-context option. This option sets the name of the context that points to the cluster to be registered. The below example will generate a kubeconfig file and register the cluster with Sveltos.</p> <pre><code>$ sveltosctl register cluster \\\n    --namespace=&lt;namespace&gt; \\\n    --cluster=&lt;cluster name&gt; \\\n    --fleet-cluster-context=&lt;context name&gt; \\\n    --labels=key1=value1,key2=value2\n</code></pre> Alternative Sveltos Cluster Registration <p>If a different <code>kubeconfig</code> is required, users can utilise the <code>sveltosctl generate kubeconfig</code> command. It allows Sveltos to create the required <code>ServiceAccount</code> alongside the <code>kubeconfig</code>. To proceed with the registration process, follow the steps listed below.</p> <ol> <li> <p>Generate the kubeconfig: Use the <code>sveltosctl generate kubeconfig</code> command while pointing it to the cluster you want Sveltos to manage. The command will create a ServiceAccount with <code>cluster-admin</code> permissions and generate the kubeconfig based on it. <sup>1</sup></p> </li> <li> <p>Register the Cluster: Use the <code>sveltosctl register cluster</code> pointing it to the Sveltos management cluster. Provide the following options:</p> <ul> <li><code>--namespace=&lt;namespace&gt;</code>: Namespace in the management cluster where Sveltos will store information about the registered cluster.</li> <li><code>--cluster=&lt;cluster name&gt;</code>: A chosen name to identify the registered cluster within Sveltos.</li> <li><code>--kubeconfig=&lt;path to file with Kubeconfig&gt;</code>: Path to the kubeconfig file generated in step 1.</li> <li><code>--labels=&lt;key1=value1,key2=value2&gt;</code> (Optional): Comma-separated key-value pairs to define labels for the registered cluster (e.g., --labels=environment=production,tier=backend).</li> </ul> </li> </ol> <p>Registration Example</p> <p>Pointing to the managed cluster (Generate kubeconfig with ServiceAccount creation):</p> <p><code>$ sveltosctl generate kubeconfig --create &gt; ~/.kube/prod-cluster.config</code></p> <p>Pointing to the management cluster (Register the cluster):</p> <pre><code>$ sveltosctl register cluster \\\n    --namespace=monitoring \\\n    --cluster=prod-cluster \\\n    --kubeconfig=~/.kube/prod-cluster.config \\\n    --labels=environment=production,tier=backend\n</code></pre> <p>The example will register a cluster (i.e, creates a SveltosCluster instance) named prod-cluster in the monitoring namespace with the labels set to \"environment=production\" and \"tier=backend\".</p> <p>If later on you want to change the labels assigned to the cluster, use the kubectl command below.</p> <p><code>$ kubectl edit sveltoscluster prod-cluster -n monitoring</code></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register-cluster/#registration-examples","title":"Registration Examples","text":"EKS <p>Once an EKS cluster is created, perform the below steps.</p> <ol> <li> <p>Retrieve the <code>kubeconfig</code> file with the AWS CLI.</p> <pre><code>$ aws eks update-kubeconfig --region &lt;the region the cluster created&gt; --name &lt;the name of the cluster&gt;\n</code></pre> </li> <li> <p>Generate Sveltos Relevant Kubeconfig     <pre><code>$ export KUBECONFIG=&lt;directory of the EKS kubeconfig file&gt;\n$ sveltosctl generate kubeconfig --create --expirationSeconds=86400 &gt; eks_kubeconfig.yaml\n</code></pre></p> </li> <li> <p>Register EKS with Sveltos</p> <pre><code>$ export KUBECONFIG=&lt;Sveltos management cluster&gt;\n$ sveltosctl register cluster --namespace=&lt;namespace&gt; --cluster=&lt;cluster name&gt; \\\n    --kubeconfig=&lt;path to Sveltos file with Kubeconfig&gt; \\\n    --labels=env=test\n</code></pre> </li> </ol> <p>Tip</p> <p>For Step #2, Sveltos will have cluster-admin privileges to the cluster.</p> GKE <ol> <li>Pointing to GKE cluster, run sveltosctl generate kubeconfig --create --expirationSeconds=86400</li> <li>Run sveltosctl register cluster command pointing it to the kubeconfig file generated by the step above.</li> </ol> <p>Tip</p> <p>Step #1 gives Sveltos cluster-admin privileges (that is done because we do not know in advance which add-ons we want Sveltos to deploy). We might choose to give Sveltos fewer privileges. Just keep in mind it needs enough privileges to deploy the add-ons you request to deploy.</p> Rancher RKE2 <p>If you use Rancher's next-generation Kubernetes distribution RKE2, you will only need to download the kubeconfig either from the Rancher UI under the Cluster Management section or via SSH into the RKE2 Cluster and under the /etc/rancher/rke2/rke2.yaml directory. Run the below command.</p> <pre><code>$ sveltosctl register cluster \\\n    --namespace=&lt;namespace&gt; \\\n    --cluster=&lt;cluster name&gt; \\\n    --kubeconfig=&lt;path to file with Kubeconfig&gt; \\\n    --labels=env=test\n</code></pre> Civo <p>If you use Civo Cloud, simply download the cluster Kubeconfig and perform the below.</p> <pre><code>$ sveltosctl register cluster \\\n    --namespace=&lt;namespace&gt; \\\n    --cluster=&lt;cluster name&gt; \\\n    --kubeconfig=&lt;path to file with Kubeconfig&gt; \\\n    --labels=env=test\n</code></pre> Kamaji <p>If you use the Hosted Control Plane solution Kamaji, follow steps below below to perform a tenant cluster registration with Sveltos.</p> <ol> <li>Point the kubeconfig to the Kamaji Management Cluster     <pre><code>$ export KUBECONFIG=~/demo/kamaji/kubeconfig/kamaji-admin.kubeconfig\n</code></pre></li> <li>Check the secrets in the namespace the tenant cluster was created     <pre><code>$ kubectl get secrets -n {your namespace}\n</code></pre></li> <li>Look for the secret with the following name format <code>&lt;tenant_name&gt;-admin-kubeconfig</code></li> <li>Get and decode the secret to a file of your preference     <pre><code>$ kubectl get secrets -n {your namespace} &lt;tenant_name&gt;-admin-kubeconfig -o json \\\n| jq -r '.data[\"admin.conf\"]' \\\n| base64 --decode \\\n&gt; &lt;path to file with kubeconfig&gt;/&lt;tenant_name&gt;-admin.kubeconfig\n</code></pre></li> <li>Perform a Sveltos registration     <pre><code>$ sveltosctl register cluster \\\n    --namespace=&lt;namespace&gt; \\\n    --cluster=&lt;cluster name&gt; \\\n    --kubeconfig=&lt;path to file with kubeconfig&gt; \\\n    --labels=key1=value1,key2=value2\n</code></pre>     Example     <pre><code>$ sveltosctl register cluster \\\n    --namespace=projectsveltos \\\n    --cluster=tenant-00 \\\n    --kubeconfig=~/demo/kamaji/kubeconfig/tenant-00-admin.kubeconfig \\\n    --labels=tcp=tenant-00\n</code></pre></li> </ol> vCluster <p>If you use vCluster with Helm for multi-tenancy, follow the steps below to perform a cluster registration with Sveltos.</p> <ol> <li>Point the kubeconfig to the parent Kubernetes cluster     <pre><code>$ export KUBECONFIG=~/demo/vcluster/multi-tenant/kubeconfig/demo01.yaml\n</code></pre></li> <li>Check the secrets in the namespace the virtual cluster was created     <pre><code>$ kubectl get secrets -n {your namespace}\n</code></pre></li> <li>Look for the secret with the following name format <code>vc-&lt;vcluster name&gt;</code></li> <li>Get and decode the secret to a file of your preference     <pre><code>$ kubectl get secret vc-vcluster-dev -n dev --template={{.data.config}} | base64 -d &gt; ~/demo/vcluster/multi-tenant/kubeconfig/vcluster-dev.yaml\n</code></pre></li> <li>Perform a Sveltos registration     <pre><code>$ sveltosctl register cluster \\\n    --namespace=&lt;namespace&gt; \\\n    --cluster=&lt;cluster name&gt; \\\n    --kubeconfig=&lt;path to file with Kubeconfig&gt; \\\n    --labels=key1=value1,key2=value2\n</code></pre>     Example     <pre><code>$ sveltosctl register cluster \\\n    --namespace=projectsveltos \\\n    --cluster=vcluster-dev \\\n    --kubeconfig=~/demo/vcluster/multi-tenant/kubeconfig/vcluster-dev.yaml \\\n    --labels=env=dev\n</code></pre></li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register-cluster/#programmatic-registration","title":"Programmatic Registration","text":"<p>To programmatically register clusters with Sveltos, create the below resources in the management cluster.</p> <ul> <li>Secret: Store the kubeconfig of the managed cluster in the data section under the key <code>kubeconfig</code>.</li> <li>SveltosCluster: Represent your cluster as an <code>SveltosCluster</code> instance.</li> </ul> <p>By default, Sveltos searches for a <code>Secret</code> named <code>&lt;cluster-name&gt;-sveltos-kubeconfig</code> in the same namespace as the SveltosCluster. To use a different Secret name, set the SveltosCluster.Spec.KubeconfigName field to the desired name.</p> Kubernetes Resources Example <p>Secret Resource <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: YOUR-CLUSTER-NAME-sveltos-kubeconfig\n  namespace: YOUR-CLUSTER-NAMESPACE\ndata:\n  kubeconfig: BASE64 ENCODED kubeconfig\ntype: Opaque\n</code></pre> SveltosCluster Resource <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: SveltosCluster\nmetadata:\n  name: YOUR-CLUSTER-NAME\n  namespace: YOUR-CLUSTER-NAMESPACE\n  labels:\n    foo: bar\n    sveltos-agent: present\n</code></pre></p> <p>Tip</p> <p>To get an idea on how an already registered cluster looks like, check out the Sveltos <code>mgmt</code> cluster using the command <code>kubectl get sveltoscluster mgmt -n mgmt -o yaml</code>.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register-cluster/#register-management-cluster","title":"Register Management Cluster","text":"<p>Sveltos manages add-ons not only on managed clusters but also on the management clusters. The management cluster is the Kubernetes cluster where Sveltos is deployed. To enable add-ons there, apply the labels of your choice. The Sveltos management cluster is registered in the <code>mgmt</code> namespace under the name <code>mgmt</code>.</p> <pre><code>$  kubectl get sveltoscluster -A --show-labels\nNAMESPACE    NAME         READY   VERSION        AGE   LABELS\nmgmt         mgmt         true    v1.32.6+k3s1   24h   projectsveltos.io/k8s-version=v1.32.6,sveltos-agent=present\n</code></pre> <pre><code>$ kubectl label sveltoscluster mgmt -n mgmt cluster=mgmt\n</code></pre> <p>For a Helm chart installation, have a look at the Helm chart values to include the labels of your choice.</p> <ol> <li> <p>As an alternative to generate kubeconfig have a look at the script: get-kubeconfig.sh. Read the script comments to get more clarity on the use and expected outcomes. This script was developed by Gravitational Teleport. We simply slightly modified to fit Sveltos use case.\u00a0\u21a9</p> </li> <li> <p>To manage add-ons and deployments on the management cluster, by default, Sveltos automatically registers the cluster as <code>mgmt</code> in the <code>mgmt</code> namespace. Follow the standard Sveltos label concept to mark it for deployments.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register_cluster_pull_mode/","title":"Register Cluster Pull Mode","text":"<p>Video</p> <p>To learn more about the Sveltos Pull Mode, check out the Youtube Video. If you find this valuable, we would be thrilled if you shared it! \ud83d\ude0a</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register_cluster_pull_mode/#sveltos-cluster-registration-pull-mode","title":"Sveltos Cluster Registration Pull Mode","text":"<p>Sveltos supports managed cluster registration in Pull Mode. In this model, managed clusters actively pull configuration and add-ons from a central source, rather than having the management cluster push them directly to the managed clusters.</p> <p>The Pull Mode is ideal for managed clusters behind a firewall, in air-gapped environments, edge deployments with limited bandwidth or highly regulated and secure setups.</p> <p>If Sveltos is not already installed, have a look at the installation details located here.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register_cluster_pull_mode/#how-does-it-work","title":"How does it work?","text":"<p>While the core Sveltos controllers run in a management cluster and orchestrate deployments, the concept of Pull Mode becomes relevant when managing clusters in challenging network environments such as firewalled, air-gapped, or edge deployments, where direct inbound access from the management cluster to the managed clusters is either not possible or not desired.</p> <p>This is how the Pull Mode flow works:</p> <ol> <li>Management Cluster: It defines the desired state. We define our <code>ClusterProfile</code>/<code>Profile</code> resources in the management cluster, specifying which add-ons and configurations should be applied to which managed clusters by utilising the Kubernetes labels selection concept.</li> <li>Managed Cluster: Rather than the management cluster initiating all deployments, a component on the managed cluster initiates a connection to the management cluster.</li> <li>Configuration Fetching: The managed cluster pulls the relevant configuration, manifest, or Helm chart from the management cluster. The management cluster prepares the relevant configuration bundle for the managed clusters in Pull Mode.</li> <li>Apply: The managed cluster's local agent applies the pulled configurations to the cluster.</li> </ol> <p></p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register_cluster_pull_mode/#advantages","title":"Advantages","text":"<p>The following items are some of the benefits of utilising Sveltos in Pull Mode.</p> <ul> <li>Firewalled and Air-Gapped Environments: This is the primary driver. When managed clusters are behind firewalls or in air-gapped networks, direct inbound connections from a central management cluster are not possible. Sveltos in Pull Mode allows the managed clusters to reach out to the management cluster or a Git repository to obtain deployment instructions.</li> <li>Edge Deployments: For edge locations with intermittent connectivity or limited network bandwidth, the Sveltos Pull Mode offers resilience.</li> <li>Security: Reducing the need for inbound ports to managed clusters enhances the security posture of an environment by minimising the attack surface.</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register_cluster_pull_mode/#register-cluster","title":"Register Cluster","text":"<p>To register a cluster in Pull mode, we use the sveltosctl.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register_cluster_pull_mode/#management-cluster","title":"Management Cluster","text":"<p>Pointing the KUBECONFIG to the management cluster where Sveltos is installed, perform the following <code>sveltosctl register</code> command.</p> <pre><code>$ export KUBECONFIG=&lt;/path/to/kubeconfig/management/cluster&gt;\n\n$ sveltosctl register cluster \\\n    --namespace=monitoring \\\n    --cluster=prod-cluster \\\n    --pullmode \\\n    --labels=environment=production,tier=backend \\\n    &gt; sveltoscluster_registration.yaml\n</code></pre> Parameter Description <code>--namespace</code> The namespace in the management cluster where Sveltos stores information about the registered cluster. <code>--cluster</code> The name of the cluster to identify the registered cluster within Sveltos. <code>--pullmode</code> Enables the Sveltos Pull Mode registration. <code>--labels</code> (Optional) Comma-separated key-value pairs to define labels for the registered cluster. <p>When the command is executed, looking at the <code>sveltosclusters</code> resource, we do see a new instance called <code>prod-cluster</code> in a not \"Ready\" state. This is the expected behaviour as we register the cluster in Pull Mode. Feel free to look at the generated file and identify what resources will be created for the managed cluster.</p> <pre><code>$ kubectl get sveltoscluster -n monitoring\nNAMESPACE   NAME           READY   VERSION   AGE\nmonitoring  prod-cluster                     1m7s\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register_cluster_pull_mode/#managed-cluster","title":"Managed Cluster","text":"<p>Apply the generated file to the managed cluster.</p> <pre><code>$ export KUBECONFIG=&lt;/path/to/kubeconfig/managed/cluster&gt;\n\n$ kubectl apply -f sveltoscluster_registration.yaml\nnamespace/projectsveltos created\nserviceaccount/sveltos-applier-manager created\nclusterrole.rbac.authorization.k8s.io/sveltos-applier-manager-role created\nclusterrolebinding.rbac.authorization.k8s.io/sveltos-applier-manager-rolebinding created\nservice/sveltos-applier-metrics-service created\ndeployment.apps/sveltos-applier-manager created\nsecret/pcluster01-sveltos-kubeconfig created\n</code></pre> <p>Note</p> <p>Test the Pull Mode with up to two managed clusters for free. Need more than two clusters? Contact us at <code>support@projectsveltos.io</code> to explore license options based on your needs!</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/register_cluster_pull_mode/#validation","title":"Validation","text":"<pre><code>$ export KUBECONFIG=&lt;/path/to/kubeconfig/management/cluster&gt;\n\n$ kubectl get sveltoscluster -n monitoring\nNAMESPACE   NAME           READY   VERSION    AGE\nmonitoring  prod-cluster   true    v1.30.5    9m15s\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/token-renewal/","title":"Automatically Token Renewal","text":"<p>To register a managed cluster (e.g., GKE, AKS, EKS) with Sveltos, a temporary Kubeconfig file is generated using sveltosctl. However, due to potential expiration limits imposed by cloud providers, this can disrupt Sveltos' management of the cluster.</p> <p>To prevent this, configure automatic renewal: edit the <code>SveltosCluster</code> resource. Add or modify the <code>tokenRequestRenewalOption</code> section to include:</p> <pre><code>tokenRequestRenewalOption:\n  renewTokenRequestInterval: 1h0m0s\n  tokenDuration: 5h\n  saName: projectsveltos\n  saNamespace: projectsveltos\n</code></pre> Supplementary Notes on Token Rotation  Note that: <ul> <li>The token rotation privilege is required by the token in the Secret (the Kubeconfig) itself, not by the sveltoscluster-manager\u2019s own ServiceAccount. Ensure that the token used in the Secret has the ability to create new tokens for the ServiceAccount. For example:</li> </ul> <pre><code>- apiGroups:\n  - \"\"\n  resources:\n  - serviceaccounts/token\n  verbs:\n    - create\n</code></pre> <ul> <li>The token is renewed based on the interval set in renewTokenRequestInterval.  The token total lifespan is determined  by tokenDuration. Ensure tokenDuration is longer than renewTokenRequestInterval to keep the token valid between renewals. </li> <li>If, for any reason, token rotation cannot happen before the current token expires, the sveltoscluster-manager can no longer update the token. Consequently, reconciliations for that cluster stop, and you must manually update the Secret for that cluster to restore functionality.</li> <li>The saName and saNamespace fields refer to a ServiceAccount in the remote (managed) cluster. This ServiceAccount must have the appropriate privileges to allow Sveltos to deploy add-ons and manage workloads in the cluster.</li> <li>If saName and saNamespace are not specified in the tokenRequestRenewalOption, Sveltos relies on whatever context is currently set in the Kubeconfig\u2019s (for example, the fields under contexts[0].context.user and contexts[0].context.namespace).</li> </ul>  Token Renewal Flow with sveltoscluster-manager:  <pre><code>%% sveltoscluster-manager uses the token from the Secret to request a new token from the remote cluster (via the ServiceAccount).\n%% It then updates the Secret with the newly generated token, and finally writes\n%% the last renewal timestamp to the SveltosCluster status (lastReconciledTokenRequestAt).\nflowchart LR\n    A((SveltosCluster CR)) --&gt; B[Check every 10 seconds if Renew Interval has passed]\n    B --&gt;|Needs Renewal| C[Read existing Token from Secret]\n    C --&gt; D[Use existing Token to request new Token from remote ServiceAccount]\n    D --&gt; E[Remote cluster issues new Token]\n    E --&gt; F[Update Secret with new Token in Kubeconfig]\n    F --&gt; G[Write last token renewal time to SveltosCluster status]</code></pre> <p>Below is an example showing how to configure token renewal for a GKE cluster.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/token-renewal/#example-gke","title":"Example: GKE","text":"<p>To connect a Google Kubernetes Engine (GKE) cluster to Sveltos, first use <code>sveltosctl</code> to create a temporary Kubeconfig file for the GKE cluster:</p> <pre><code>$ sveltosctl  generate kubeconfig --create --expirationSeconds=86400 &gt;  /tmp/GKE/kubeconfig\n</code></pre> <p>Remember that GKE's maximum expiration time for Kubeconfig files is 48 hours (172800 seconds).</p> <p>Next, point sveltosctl to your Sveltos management cluster and register the GKE cluster:</p> <pre><code>$ sveltosctl register cluster --namespace=gke --cluster=cluster --kubeconfig=/tmp/GKE/kubeconfig --labels=env=production\n</code></pre> <p>If we leave as it is, in 48 hours the Kubeconfig will expire. To prevent the Kubeconfig from expiring and disrupting Sveltos' management of the GKE cluster, you can configure Sveltos to automatically renew the Kubeconfig.</p> <p>Edit the SveltosCluster cluster in the gke namespace:</p> <pre><code>$ kubectl edit sveltoscluster -n gke cluster\n</code></pre> <p>Add or modify the <code>tokenRequestRenewalOption</code> section to include:</p> <pre><code>  tokenRequestRenewalOption:\n    renewTokenRequestInterval: 1h0m0s\n    saName: projectsveltos\n    saNamespace: projectsveltos\n</code></pre> <p>This assumes that the ServiceAccount projectsveltos exists in the projectsveltos namespace  on the GKE cluster and has the necessary permissions for Sveltos to deploy applications and add-ons to the cluster.</p> <p>With this configuration, Sveltos will generate a new token tied to the ServiceAccount and use it to create a new Kubeconfig every hour, ensuring continuous cluster management.</p> <p>The <code>SveltosCluster.Status</code> field provides information about the last time the token was renewed:</p> <pre><code> status:\n    connectionStatus: Healthy\n    lastReconciledTokenRequestAt: \"2024-10-08T07:36:42Z\"\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/features/cluster_maintenance_window/","title":"Per-Cluster Maintenance Windows","text":"<p>Sveltos offers precise control over updates with per-cluster maintenance windows. This means updates are deployed only during these designated periods, minimizing disruption to your workloads.</p> <p>For instance, you can configure <code>cluster1</code> to receive updates every Friday from 8 PM to Monday 7 AM:</p> <pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: SveltosCluster\nmetadata:\n  name: cluster1\n  namespace: civo\nspec:\n  activeWindow:\n    from: 0 20 * * 5  # Friday 8PM (0 hour, 20th minute, any day of month, any month, Friday)\n    to: 0 7 * * 1    # Monday 7AM (0 hour, 7th minute, any day of month, any month, Monday)\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"register/features/pausing/","title":"Pause Cluster","text":"<p>To temporarily halt all updates and deployments to a specific cluster, use the SveltosCluster.Spec.Paused field. When set to <code>true</code>, it instructs Sveltos to pause all operations for that cluster.</p> <p>What does this mean? No new add-ons, applications, or configurations will be deployed, and any existing ones managed by Sveltos will not be updated, even if their corresponding ClusterProfile or Profile is changed.</p> <p>This feature is particularly useful for:</p> <ul> <li> <p>Maintenance: Pausing a cluster while you perform manual updates or troubleshoot issues.</p> </li> <li> <p>Troubleshooting: Temporarily stopping Sveltos from applying changes to a cluster that is experiencing problems.</p> </li> <li> <p>Control: Gaining fine-grained control over when a cluster receives updates, which is important in environments where changes must be carefully coordinated.</p> </li> </ul> <p>To resume operations, set the SveltosCluster.Spec.Paused back to <code>false</code> or delete the field. Sveltos will resume its reconciliation loop and apply any pending changes.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy"]},{"location":"scalability/horizontal_scaling_sharding/","title":"Projectsveltos Sharding","text":"","tags":["Kubernetes","add-ons","horizontal scaling"]},{"location":"scalability/horizontal_scaling_sharding/#introduction-to-sharding","title":"Introduction to Sharding","text":"<p>When Sveltos is managing hundreds of clusters and thousands of applications, it is recommended to adopt a sharding strategy to distribute the load across multiple Sveltos controller instances.</p> <p>Sveltos has a controller running in the management cluster called the <code>shard controller</code>. It watches for cluster annotations. When it detects a new cluster shard, the <code>shard controller</code> automatically deploys a new set of Sveltos controllers to manage the shard.</p> <p>How does Sveltos distribute the load? This is done by adding the special annotation <code>sharding.projectsveltos.io/key</code> to the managed clusters of interest. By default, all clusters are managed by the same Sveltos controller. When no more managed clusters have a special annotation set, Sveltos automatically brings down the extra Sveltos controllers.</p> <p>For more information, have a look at the <code>.gif</code> below.</p> <p></p>","tags":["Kubernetes","add-ons","horizontal scaling"]},{"location":"scalability/horizontal_scaling_sharding/#sharding-benefits","title":"Sharding Benefits","text":"<p>The benefits of using a sharding strategy include:</p> <ol> <li>Improved performance: By distributing the load across multiple instances of Sveltos controllers, sharding can improve the performance of Sveltos.</li> <li>Increased scalability: Sharding allows Sveltos to manage a larger number of managed clusters and applications.</li> <li>Reduced risk: If one instance of a Sveltos controller fails, the other instances can continue to manage the applications in their respective cluster shards.</li> </ol>","tags":["Kubernetes","add-ons","horizontal scaling"]},{"location":"scalability/vertical_scaling/","title":"Projectsveltos horizontal scaling","text":"<p>The below arguments can be used to customize Sveltos add-on controller.</p> <ol> <li>concurrent-reconciles: By default the Sveltos manager reconcilers runs with a parallelism set to 10. This arg can be used to change level of parallelism for ClusterProfiles and ClusterSummary;</li> <li>worker-number: The number of workers performing long running task. By default it is set to 20. Increase it number if the managed clusters is above 100.</li> </ol> <p>More infromation can be found in the folllowing Medium post on how Sveltos handles long running task.</p>","tags":["Kubernetes","add-ons","vertical scaling"]},{"location":"template/additional_template_info/","title":"Additional Templates Information","text":"<p>Note</p> <p>Make sure to read the \"Introduction to Templates\" section before continuing. It provides important context for the information that follows.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/additional_template_info/#variables","title":"Variables","text":"<p>By default, the Sveltos Templates can access to the mentioned management cluster resources.</p> <ol> <li>CAPI Cluster instance: <code>Cluster</code></li> <li>CAPI Cluster infrastructure provider: <code>InfrastructureProvider</code></li> <li>CAPI Cluster kubeadm provider:<code>KubeadmControlPlane</code></li> <li>Sveltos registered clusters, the SveltosCluster instance: <code>Cluster</code></li> </ol> <p>Sveltos can retrieve any resource from the management cluster. To do this, include the <code>templateResourceRefs</code> in the <code>Spec</code> section of the ClusterProfile/Profile  resource.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/additional_template_info/#role-based-access-control-rbac","title":"Role Based Access Control (RBAC)","text":"<p>Sveltos adheres to the least privilege principle concept. That means, by default, Sveltos does not have all the necessary permissions to fetch resources from the management cluster. Therefore, when using <code>templateResourceRefs</code>, we need to provide Sveltos with the correct RBAC definition.</p> <p>Granting the necessary RBAC permissions to Sveltos is a simple process. The Sveltos <code>ServiceAccount</code> is tied to the addon-controller-role-extra ClusterRole. To grant Sveltos the necessary permissions, simply edit the role.</p> <p>If the <code>ClusterProfile</code> is created by a tenant administrator as part of a multi-tenant setup, Sveltos acts on behalf of (impersonate) the ServiceAccount that represents the tenant. This ensures the Kubernetes RBACs are enforced, which restricts the tenant's access to only authorised resources.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/additional_template_info/#templateresourcerefs-namespace-and-name","title":"templateResourceRefs: Namespace and Name","text":"<p>When using the <code>templateResourceRefs</code> field to locate resources in the management cluster, the <code>namespace</code> field is optional.</p> <ol> <li>If a namespace is provided (like default), Sveltos will look for the resource in the specified namespace</li> <li>If the namespace field is blank, Sveltos will search for the resource in the same namespace as the management cluster</li> </ol> <p>The <code>name</code> field in <code>templateResourceRefs</code> can be expressed as a template. It allows users to dynamically generate names based on the information available during the deployment.</p> <p>Available cluster information:</p> <ul> <li>cluster namespace: <code>.Cluster.metadata.namespace</code></li> <li>cluster name: <code>.Cluster.metadata.name</code></li> <li>cluster type: <code>.Cluster.kind</code></li> </ul> <p>For example, the below template creates a name by combining the cluster's <code>namespace</code> and <code>name</code>.</p> <pre><code>name: \"{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}\"\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/additional_template_info/#embedding-go-templates-in-sveltos","title":"Embedding Go Templates in Sveltos","text":"<p>When incorporating Go template logic into Sveltos templates, utilise the escape syntax <code>'{{`&lt;YOUR GO TEMPLATE&gt;`}}'</code>. This ensures that the code is treated as a Go template rather than a Sveltos template.</p> <p>Embedding Go Templates in Sveltos</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: meilisearch-proxy-secrets\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  secrets.yaml: |\n    {{ $cluster := .Cluster.metadata.name }}\n    {{- range $env := (list \"production\" \"staging\") }}\n    ---\n    apiVersion: external-secrets.io/v1beta1\n    kind: ExternalSecret\n    metadata:\n      name: meilisearch-proxy\n      namespace: {{ $env }}\n    spec:\n      refreshInterval: 1h\n      secretStoreRef:\n        kind: ClusterSecretStore\n        name: vault-backend\n      target:\n        name: meilisearch-proxy\n        template:\n          engineVersion: v2\n          data:\n            MEILISEARCH_HOST: https://meilisearch.{{ $cluster }}\n            MEILISEARCH_MASTER_KEY: '{{`{{ .master_key }}`}}'\n            PROXY_MASTER_KEY_OVERRIDE: \"false\"\n            CACHE_ENGINE: \"redis\"\n            CACHE_TTL: \"300\"\n            CACHE_URL: \"redis://meilisearch-proxy-redis:6379\"\n            PORT: \"80\"\n            LOG_LEVEL: \"info\"\n      data:\n        - secretKey: 'master_key'\n          remoteRef:\n            key: 'search'\n            property: '{{ $env }}.master_key'\n    {{- end }}\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/additional_template_info/#learn-more","title":"Learn More","text":"<ol> <li>Example - Helm Chart and Resources as Templates: Checkout the template examples here</li> <li>Helm Charts: See the \"Example: Express Helm Values as Templates\" section in here</li> <li>YAML &amp; JSON: Refer to the \"Example Template with Git Repository/Bucket Content\" section in here</li> <li>Kustomize: Substitution and templating are explained here</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/bring_your_own_controller/","title":"Extending Sveltos","text":"<p>In the example below, we will guide you through the process of extending Sveltos with your own controller. The example focuses on extending Sveltos by the use of a new controller that allocates a storage bucket from Google Cloud, followed by Sveltos deploying a pod in the managed cluster to upload a file to that bucket, all using a single YAML configuration.</p> <p>If you are new to Sveltos, please go through the deploy Kubernetes add-ons and the Sveltos events resources before proceeding.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"template/bring_your_own_controller/#deploy-sveltos-in-the-management-cluster","title":"Deploy Sveltos in the management cluster","text":"<p>Our management cluster is a Kind cluster running on a laptop. To deploy Sveltos on the management cluster, follow the steps described in the instructions documentation.</p> <pre><code>$ kubectl get pods -n projectsveltos\n\nNAME                                  READY   STATUS    RESTARTS   AGE\naccess-manager-69d7fd69fc-7mm2p       2/2     Running   0          3m58s\naddon-controller-6ccff8d976-99sdb     2/2     Running   0          3m58s\nclassifier-manager-6489f67447-q5gs2   2/2     Running   0          3m54s\nevent-manager-56ffd458f7-fchkv        2/2     Running   0          3m55s\nhc-manager-7b6d7c4968-52wj2           2/2     Running   0          3m55s\nsc-manager-cb6786669-ncdzg            2/2     Running   0          3m57s\nsveltosctl-0                          1/1     Running   0          3m41s\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"template/bring_your_own_controller/#google-cloud-storage-buckets","title":"Google Cloud Storage Buckets","text":"<p>When a new production cluster is created, we want to dynamically create a Google Bucket for it and grant a service account permissions to upload files to it. To automate the process, and demo Sveltos extensions, we developed a controller that can create and delete Google Storage Buckets as instructed. You can find this controller on Github.</p> <p>The controller is responsible for managing instances of CRD <code>bucket.v1alpha1.demo.projectsveltos.io</code>, creating a Bucket on Google Cloud Storage, and granting service accounts the <code>objectCreator</code> permission on it.</p> <p>To deploy the controller in the management cluster, please refer to the instructions in the README as you need to fetch some information from Google Cloud in order to give the controller permission to create buckets on Google Cloud Storage.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"template/bring_your_own_controller/#instruct-sveltos","title":"Instruct Sveltos","text":"<p>Before posting the necessary Sveltos ClusterProfile, we need to grant Sveltos permission to create bucket instance in the management clusters.</p> <pre><code>$ kubectl edit clusterrole addon-controller-role-extra\n</code></pre> <p>and edit rules by adding</p> <pre><code>rules:\n- apiGroups:\n  - storage.gcp.upbound.io\n  resources:\n  - buckets\n  verbs:\n  - \"*\"\n</code></pre> <p></p> <p>The following YAML instructions are used to deploy add-ons using Sveltos:</p> <ol> <li> <p>The content of the ConfigMap <code>bucket</code> in the <code>default</code> namespace is deployed by Sveltos on the management cluster (<code>deploymentType: Local</code>). The contents of this ConfigMap is an instance of the <code>Bucket</code> CRD, which is expressed as a template. Before deploying it to the management cluster, Sveltos instantiates the template by setting the namespace to be the same as the namespace of the managed cluster where Sveltos deploys add-ons defined in this ClusterProfile.</p> </li> <li> <p>The content of the ConfigMap <code>uploader</code> in the <code>default</code> namespace is deployed by Sveltos on the managed cluster (<code>deploymentType: Remote</code>). The contents of this ConfigMap is a Pod instance expressed as a template. Sveltos instantiates the template using information from the bucket instance created in the previous step. This Pod then uploads a file to the newly created bucket on Google Cloud Storage.</p> </li> </ol>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"template/bring_your_own_controller/#_1","title":"Extending Sveltos","text":"<p>Example</p> <pre><code>cat &gt; bucket.yaml &lt;&lt;EOF\n---\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: production\n  templateResourceRefs:\n  - resource:\n      apiVersion: demo.projectsveltos.io/v1alpha1\n      kind: Bucket\n      name: sveltos-demo-bucket\n    identifier: Bucket\n  policyRefs:\n  - deploymentType: Local\n    kind: ConfigMap\n    name: bucket\n    namespace: default\n  - deploymentType: Remote\n    kind: ConfigMap\n    name: uploader\n    namespace: default\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: bucket\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  bucket.yaml: |\n    apiVersion: demo.projectsveltos.io/v1alpha1\n    kind: Bucket\n    metadata:\n      name: sveltos-demo-bucket\n      namespace: {{ .Cluster.metadata.namespace }}\n    spec:\n      bucketName: \"sveltos-demo-{{ .Cluster.metadata.name }}\"\n      location: us-central1\n      serviceAccount: serviceAccount:uploader@sveltos.iam.gserviceaccount.com\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: uploader\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  secret.yaml: |\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: gcs-credentials\n      namespace: default\n      annotations:\n        bucket: {{ (getResource \"Bucket\").status.bucketURL }}\n    type: Opaque\n    data:\n      service-account.json: {{ (getResource \"Bucket\").status.serviceAccountCredentials }}\n  pod.yaml: |\n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: create-and-upload-to-gcs\n      namespace: default\n      annotations:\n        bucket: {{ (getResource \"Bucket\").status.bucketURL }}\n    spec:\n      containers:\n      - name: uploader\n        image: google/cloud-sdk:slim\n        command: [\"bash\"]\n        args:\n          - \"-c\"\n          - |\n            echo \"Hello world\" &gt; /tmp/hello.txt\n            gcloud auth activate-service-account --key-file=/var/run/secrets/cloud.google.com/service-account.json\n            gsutil cp /tmp/hello.txt gs://{{ (getResource \"Bucket\").spec.bucketName }}\n        volumeMounts:\n          - name: gcp-sa\n            mountPath: /var/run/secrets/cloud.google.com/\n            readOnly: true\n      volumes:\n        - name: gcp-sa\n          secret:\n            secretName: gcs-credentials\nEOF\n</code></pre> <p>After posting it a Bucket instance is created in the management cluster by Sveltos.</p> <pre><code>$ kubectl get bucket -A\n\nNAMESPACE   NAME                  BUCKET NAME                                BUCKET URL\ndefault     sveltos-demo-bucket   sveltos-demo-sveltos-management-workload   https://storage.googleapis.com/sveltos-demo-sveltos-management-workload\n</code></pre> <p>The gcs-storage-controller processes this instance and creates a bucket on Google Cloud Storage.</p> <p></p> <p>To verify the bucket has been created on Google Cloud Storage</p> <pre><code>$ gsutil iam get gs://&lt;BUCKET NAME&gt;\n</code></pre> <p>Sveltos has also deployed a Pod in the managed cluster and has passed this Pod with a secret containing the credentials to access the bucket (this information was provided to Sveltos by the gcs controller we developed). This pod has then uploaded a file to the Google Storage Bucket just created.</p> <p></p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"template/bring_your_own_controller/#summary","title":"Summary","text":"<p>The example focused on demostating how easy is to extend Sveltos. We focused on using a new controller that allocates a storage bucket from Google Cloud and deploy a pod in the managed cluster to upload a file to that bucket, all using a single YAML configuration.</p> <p>Whenever you need to generate extra information and pass it along to the managed cluster, using the scheme mentioned above you can extend Sveltos by writing your own controller.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"template/crossplane/","title":"Sveltos coordinating Crossplane","text":"<p>In this example, we will use Sveltos to coordinate with Crossplane to create a Google Cloud Storage Bucket for each managed cluster. We will then deploy an application in each managed cluster that uploads a file to the proper bucket.</p> <p>The following YAML code:</p> <ol> <li>Creates a ClusterProfile resource that instructs Sveltos to create a Bucket Custom Resource (CR) in the management cluster.</li> <li>Instructs Sveltos to fetch the Bucket CR instance, and use the Bucket status url and id fields to instantiate a Pod template.</li> <li>Deploys the Pod in the managed cluster.</li> </ol> <p>Once the Pod is deployed, it will upload a file to the <code>my-bucket</code> bucket.</p> <p>Example</p> <pre><code>cat &gt; crossplane_google_bucket.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  templateResourceRefs:\n  - resource:\n      apiVersion: storage.gcp.upbound.io/v1beta1\n      kind: Bucket\n      name: crossplane-bucket-{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}\n    identifier: CrossplaneBucket\n  - resource:\n      apiVersion: v1\n      kind: Secret\n      namespace: crossplane-system\n      name: gcp-secret\n    identifier: Credentials\n  policyRefs:\n  - deploymentType: Local\n    kind: ConfigMap\n    name: bucket\n    namespace: default\n  - deploymentType: Remote\n    kind: ConfigMap\n    name: uploader\n    namespace: default\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: bucket\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  bucket.yaml: |\n    apiVersion: storage.gcp.upbound.io/v1beta1\n    kind: Bucket\n    metadata:\n    name: crossplane-bucket-{{ .Cluster.metadata.namespace }}-{{ .Cluster.metadata.name }}\n    labels:\n      docs.crossplane.io/example: provider-gcp\n      clustername: {{ .Cluster.metadata.name }}\n      clusternamespace: {{ .Cluster.metadata.namespace }}\n    spec:\n      forProvider:\n        location: US\n      providerConfigRef:\n        name: default\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: uploader\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  secret.yaml: |\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: gcs-credentials\n      namespace: default\n      annotations:\n        bucket: \"{{ (getResource \"CrossplaneBucket\").status.atProvider.url }}\"\n    type: Opaque\n    data:\n      service-account.json: {{ $data:=(getResource \"Credentials\").data }} {{ (index $data \"creds\") }}\n  pod.yaml: |\n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: create-and-upload-to-gcs\n      namespace: default\n      annotations:\n        bucket: {{ (getResource \"CrossplaneBucket\").status.atProvider.url }}\n    spec:\n      containers:\n      - name: uploader\n        image: google/cloud-sdk:slim\n        command: [\"bash\"]\n        args:\n          - \"-c\"\n          - |\n            echo \"Hello world\" &gt; /tmp/hello.txt\n            gcloud auth activate-service-account --key-file=/var/run/secrets/cloud.google.com/service-account.json\n            gsutil cp /tmp/hello.txt gs://{{ (getResource \"CrossplaneBucket\").metadata.name }}\n        volumeMounts:\n          - name: gcp-sa\n            mountPath: /var/run/secrets/cloud.google.com/\n            readOnly: true\n      volumes:\n        - name: gcp-sa\n          secret:\n            secretName: gcs-credentials\nEOF\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","dry run"]},{"location":"template/example_multicluster_iteration_template/","title":"Templates","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#use-case-description","title":"Use Case Description","text":"<p>There are a number of cases where platform administrators or operators want to discover and iterate through multiple clusters to deploy specific Kubernetes resources dynamically. One example is the formation of a NATS supercluster that requires each NATS instance provisioned by Sveltos to be aware of other clusters.</p> <p>The use case can be easily achieved by Sveltos with the use of the templating functionality and Sveltos Event Franmework.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#identify-sveltos-managed-clusters","title":"Identify Sveltos Managed Clusters","text":"<p>Sveltos <code>Event Framework</code> will be used to dynamically detect all managed clusters (of type SveltosCluster) that are different from the management cluster. The management cluster has the Sveltos cluster label set to <code>type:mgmt</code>.</p> <p>Example - EventSource and EventTrigger Definition</p> <pre><code>cat &gt; eventsource_eventtrigger.yaml &lt;&lt;EOF\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventSource\nmetadata:\nname: detect-clusters\nspec:\ncollectResources: false\nresourceSelectors:\n- group: \"lib.projectsveltos.io\"\n  version: \"v1beta1\"\n  kind: \"SveltosCluster\"\n  labelFilters:\n  - key: sveltos-agent\n    operation: Equal\n    value: present\n  - key: type\n    operation: Different\n    value: mgmt\n---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: EventTrigger\nmetadata:\n  name: detect-cluster\nspec:\n  sourceClusterSelector:\n    matchLabels:\n      type: mgmt\n  eventSourceName: detect-clusters\n  oneForEvent: false\nEOF\n</code></pre> <p>Once the above YAML definition file is applied to the management cluster, an <code>EventReport</code> resource named <code>detect-clusters</code> will be created within the <code>projectsveltos</code> namespace.</p> <p>The report stores the information about all discovered managed clusters. Sveltos templating feature can reference the resource to retrieve a list of managed clusters. This allows a dynamic configuration based on the cluster environment.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#eventreport-output","title":"EventReport Output","text":"<pre><code>apiVersion: lib.projectsveltos.io/v1beta1\nkind: EventReport\n...\nspec:\n  eventSourceName: detect-clusters\n  matchingResources:\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster-1\n    namespace: civo\n  - apiVersion: lib.projectsveltos.io/v1beta1\n    kind: SveltosCluster\n    name: cluster-2\n    namespace: civo\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#automatic-service-creation","title":"Automatic Service Creation","text":"<p>Lets assume the clusters where the service should get deployed has the cluster label set to <code>type:nats</code>. The below YAML defintion will create a Sveltos <code>ClusterProfile</code> resource that points to the <code>EventReport</code> mentioned above and deploy a <code>ConfigMap</code> defined as a template.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#clusterprofile","title":"ClusterProfile","text":"<p>Example - ClusterProfile Definition</p> <pre><code>cat &gt; clusterprofile_nats.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      type: nats\n  templateResourceRefs:\n  - resource:\n      apiVersion: lib.projectsveltos.io/v1beta1\n      kind: EventReport\n      name: detect-clusters\n      namespace: projectsveltos\n    identifier: ClusterData\n  policyRefs:\n  - kind: ConfigMap\n    name: nats-services\n    namespace: default\nEOF\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#configmap","title":"ConfigMap","text":"<p>Example - ConfigMap Definition</p> <pre><code>cat &gt; cm_nats_services.yaml &lt;&lt;EOF\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nats-services\n  namespace: default\n  annotations:\n      projectsveltos.io/template: ok\ndata:\n  services.yaml: |\n    {{ range $cluster := (getResource \"ClusterData\").spec.matchingResources }}\n      apiVersion: v1\n      kind: Service\n      metadata:\n        labels:\n          app: nats\n          tailscale.com/proxy-class: default\n        annotations:\n          tailscale.com/tailnet-fqdn: nats-{{ $cluster.name }}\n        name: nats-{{ $cluster.name }}\n      spec:\n        externalName: placeholder\n        type: ExternalName\n    ---\n    {{ end }}\nEOF\n</code></pre> <p>Note</p> <p>The ConfigMap resource will create the <code>nats-{{ $cluster.name }}</code> services in the <code>default</code> namespace on every cluster with the cluster label set to <code>type:nats</code>.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#results","title":"Results","text":"<p>Once the <code>ClusterProfile</code> resource gets deployed on the management cluster, all the clusters with the label set to <code>type:nats</code> should get the Kubernetes service with the name <code>nats-{{ $cluster.name }}</code>.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#sveltos-clusters","title":"Sveltos Clusters","text":"<pre><code>$ kubectl get sveltoscluster -A --show-labels\n\nNAMESPACE  NAME    READY  VERSION    LABELS\ncivo    cluster-1  true  v1.29.2+k3s1  sveltos-agent=present\ncivo    cluster-2  true  v1.28.7+k3s1  sveltos-agent=present\nmgmt    mgmt       true  v1.30.0       sveltos-agent=present,type=mgmt\nnats    cluster-3  true  v1.28.7+k3s1  type=nats\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#kubernetes-services","title":"Kubernetes Services","text":"<pre><code>$ kubectl get service\nNAME       TYPE      CLUSTER-IP  EXTERNAL-IP  PORT(S)  AGE\n...\nnats-cluster-1  ExternalName  &lt;none&gt;    placeholder  &lt;none&gt;  68s\nnats-cluster-2  ExternalName  &lt;none&gt;    placeholder  &lt;none&gt;  68s\n</code></pre> <p>Note</p> <p>The output above is from the <code>cluster-3</code>.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/example_multicluster_iteration_template/#automatic-updates-advantages","title":"Automatic Updates Advantages","text":"<p>The beauty of Sveltos is its automatic reaction to changes. If we create or destroy managed clusters, Sveltos will automatically:</p> <ol> <li>Detect the change through the detect-clusters <code>EventSource</code></li> <li>Re-evaluate the nats-services template with updated data</li> <li>Add or remove Service instances in the mentioned clusters as needed</li> </ol> <p>This ensures the clusters remain dynamically updated based on the managed cluster configuration.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/examples/","title":"Template Examples with copy, setField, removeField and chain methods","text":"<p>This section is designed to help users get started with the Sveltos template feature. It provides simple to follow template examples based on defined resource manipulation functions.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/examples/#copy-example","title":"Copy Example","text":"<p>The example demonstrates how to copy a Secret from the management cluster to matching managed clusters using Sveltos.</p> <p>Create a Secret named imported-secret in the default namespace of your management cluster. The Secret should contain Docker registry credentials encoded in base64 format.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\ndata:\n  .dockerconfigjson: ewogICAgImF1dGhzIjogewogICAgICAgICJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOiB7CiAgICAgICAgICAgICJhdXRoIjogIkxXWWdjR0Z6YzNkdmNtUUsiCiAgICAgICAgfQogICAgfQp9Cg==\nkind: Secret\nmetadata:\n  name: imported-secret\n  namespace: default\ntype: kubernetes.io/dockerconfigjson\nEOF\n</code></pre> <p>The <code>ClusterProfile</code> to fetch the above resource looks like the below YAML definition.</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: production\n  templateResourceRefs:\n  - resource:\n      apiVersion: v1\n      kind: Secret\n      name: imported-secret\n      namespace: default\n    identifier: ExternalSecret\n  policyRefs:\n  - kind: ConfigMap\n    name: info\n    namespace: default\n</code></pre> <p>To simply copy the Secret grabbed from the management cluster to any matching managed cluster, use the YAML definition below.</p> <pre><code>    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: info\n      namespace: default\n      annotations:\n        projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\n    data:\n      secret.yaml: |\n        {{ copy \"ExternalSecret\" }}\n</code></pre> <p>Note</p> <p>We can define any resource contained in a referenced ConfigMap/Secret as a template by adding the <code>projectsveltos.io/template</code> annotation. This ensures that the template is instantiated at the deployment time, making the deployments faster and more efficient.</p> <p>This template simply references the ExternalSecret identifier (defined in the ClusterProfile) using the copy function. Consequently, the Secret from the management cluster will be copied to any matching managed clusters.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/examples/#setfieldremovefield-example","title":"SetField/RemoveField Example","text":"<p>The section demonstrates how to leverage Sveltos's setField and removeField functions within templates to manipulate deployments across managed clusters.</p> <p>Let's create a Deployment in the management cluster.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: default\n  labels:\n    apps: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 1\nEOF\n</code></pre> <p>The <code>ClusterProfile</code> to fetch the above resource looks like the below YAML definition.</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: production\n  templateResourceRefs:\n  - resource:\n      apiVersion: apps/v1\n      kind: Deployment\n      name: nginx-deployment\n      namespace: default\n    identifier: NginxDeployment\n  policyRefs:\n  - kind: ConfigMap\n    name: info\n    namespace: default\n</code></pre> <pre><code>    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: info\n      namespace: default\n      annotations:\n        projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\n    data:\n      secret.yaml: |\n        {{ setField \"NginxDeployment\" \"spec.replicas\" (int64 5) }}\n</code></pre> <p>The template will:</p> <ol> <li>Copy the NginxDeployment (referencing the original deployment).</li> <li>Update the spec.replicas field to 5.</li> <li>Deploy the modified deployment to matching managed clusters.</li> </ol> <p>Consequently, each managed cluster will have the deployment running with 5 replicas.</p> <pre><code>    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: info\n      namespace: default\n      annotations:\n        projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\n    data:\n      secret.yaml: |\n        {{ removeField \"NginxDeployment\" \"spec.replicas\" }}\n</code></pre> <p>The template above:</p> <ol> <li>Copies the NginxDeployment.</li> <li>Removes the spec.replicas field.</li> <li>Deploys the modified deployment.</li> </ol> <p>Since the replicas field is missing, Kubernetes will use the default value (typically 1) for replicas in each managed cluster.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/examples/#chainsetfieldchainremovefield-example","title":"ChainSetField/ChainRemoveField Example","text":"<p>Building upon the previous example, this section explores how to manipulate various fields within the copied Deployment using Sveltos's chainSetField function.</p> <pre><code>    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: info\n      namespace: default\n      annotations:\n        projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\n    data:\n      secret.yaml: |\n        {{ $depl := (getResource \"NginxDeployment\") }}\n        {{ $depl := (chainSetField $depl \"spec.replicas\" (int64 1) ) }}\n        {{ $depl := (chainSetField $depl \"metadata.namespace\" .Cluster.metadata.namespace ) }}\n        {{ $depl := (chainSetField $depl \"spec.template.spec.serviceAccountName\" \"default\" ) }}\n        {{ $depl := (chainSetField $depl \"spec.paused\" true ) }}\n        {{ toYaml $depl }}\n</code></pre> <ol> <li>Fetch Deployment: {{ $depl := (getResource \"NginxDeployment\") }} retrieves the deployment referenced by the identifier NginxDeployment (defined in the ClusterProfile).</li> <li>Chain Modifications: The chainSetField function is used repeatedly to modify specific fields:</li> <li>spec.replicas: Sets the number of replicas to 1.</li> <li>metadata.namespace: Sets the namespace to the current cluster's namespace (fetched using .Cluster.metadata.namespace).</li> <li>spec.template.spec.serviceAccountName: Sets the service account name to \"default\".</li> <li>spec.paused: Sets the deployment to be paused (not running).</li> <li>Convert to YAML: Finally, {{ toYaml $depl }} converts the modified deployment object back into YAML format for deployment on the managed cluster.</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/examples/#all-in-one-example","title":"All-in-One Example","text":"<p>The example demonstrates a more complex scenario where the template modifies the image tag within the copied deployment.</p> <pre><code>    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: info\n      namespace: default\n      annotations:\n        projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\n    data:\n      secret.yaml: |\n        {{ $currentContainers := (getField \"NginxDeployment\" \"spec.template.spec.containers\") }}\n        {{ $modifiedContainers := list }}\n        {{- range $element := $currentContainers }}\n          {{ $modifiedContainers = append $modifiedContainers (chainSetField $element \"image\" \"nginx:1.13\" ) }}\n        {{- end }}\n        {{ setField \"NginxDeployment\" \"spec.template.spec.containers\" $modifiedContainers }}\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/external_secret/","title":"External Secret Management - Sveltos integration","text":"","tags":["Kubernetes","Sveltos","add-ons","external secret operator","external secret management"]},{"location":"template/external_secret/#what-is-a-kubernetes-secret","title":"What is a Kubernetes Secret","text":"<p>A secret is any piece of information that you want to keep confidential, such as API keys, passwords, certificates, and SSH keys. Secret Manager systems store your secrets in a secure, encrypted format, and provides you with a simple, secure way to access them.</p> <p>Benefits of using a Secret Manager:</p> <ol> <li>Security: The Secret Manager uses strong encryption to protect your secrets. Your secrets are never stored in plaintext, and they are only accessible to authorized users only.</li> <li>Convenience: The Secret Manager makes it easy to manage the secrets. You can store, access, and rotate your secrets from anywhere.</li> <li>Auditability: The Secret Manager provides detailed audit logs that track who accessed your secrets and when. This helps you to track down security incidents and to comply with security regulations.</li> </ol>","tags":["Kubernetes","Sveltos","add-ons","external secret operator","external secret management"]},{"location":"template/external_secret/#external-secret-operator","title":"External Secret Operator","text":"<p>External Secrets Operator is an open source Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault, IBM Cloud Secrets Manager, and many more. The goal of External Secrets Operator is to synchronize secrets from external APIs into Kubernetes.  The operator reads information from external APIs and automatically injects the values into a Kubernetes Secret. If the secret from the external API changes, the controller will reconcile the state in the cluster and update the secrets accordingly.</p> <p></p>","tags":["Kubernetes","Sveltos","add-ons","external secret operator","external secret management"]},{"location":"template/external_secret/#distribute-secret-to-managed-clusters","title":"Distribute Secret to managed clusters","text":"<p>When managing a multitude of Kubernetes clusters, External Secrets Operator can be deployed in the management cluster. Sveltos can be used to distribute the secrets to the managed clusters.</p> <p></p> <ul> <li>The External Secret Operator fetches secrets from an external API and creates Kubernetes secrets;</li> <li>Sveltos distributes fetched secrets to the managed clusters;</li> <li>If the secret from the external API changes, the External Secret Operator will reconcile the state in the management cluster and update the secrets accordingly;</li> <li>Sveltos will reconcile the state in each managed cluster where secret was distributed.</li> </ul>","tags":["Kubernetes","Sveltos","add-ons","external secret operator","external secret management"]},{"location":"template/external_secret/#example-using-google-cloud-secret-manager","title":"Example using Google Cloud Secret Manager","text":"<p>To properly follow the example, please ensure you have installed the below tools:</p> <ul> <li>Sveltos management cluster</li> <li>external-secrets deployed in the management cluster</li> <li>gcloud cli installed</li> </ul> <p></p> <pre><code>$ yourproject=&lt;your-google-cloud-project-name-here&gt;\n\n$ gcloud config set project $yourproject\n\n$ gcloud services enable secretmanager.googleapis.com\n</code></pre> <p>Create a secret inside Google Cloud Secret Manager</p> <pre><code>$ echo -ne '{\"password\":\"yoursecret\"}' | gcloud secrets create yoursecret --data-file=-\n\n$ gcloud iam service-accounts create external-secrets\n\n$ gcloud secrets add-iam-policy-binding yoursecret --member \"serviceAccount:external-secrets@$yourproject.iam.gserviceaccount.com\" --role \"roles/secretmanager.secretAccessor\"\n</code></pre> <p>Create a key for the service account.</p> <pre><code>$ gcloud iam service-accounts keys create key.json --iam-account=external-secrets@$yourproject.iam.gserviceaccount.com\n\n$ kubectl create secret generic gcpsm-secret --from-file=secret-access-credentials=key.json\n</code></pre> <p>Now configure External Secret Operator to fetch secrets from Google Cloud Secret Manager and creates a secret in the Kubernetes management cluster.</p> <p>Example</p> <pre><code>cat &gt; secretstore.yaml &lt;&lt;EOF\n---\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: gcp-backend\nspec:\n  provider:\n      gcpsm:\n        auth:\n          secretRef:\n            secretAccessKeySecretRef:\n              name: gcpsm-secret\n              key: secret-access-credentials\n        projectID: $yourproject\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: gcp-external-secret\nspec:\n  secretStoreRef:\n    kind: SecretStore\n    name: gcp-backend\n  target:\n    name: imported-secret\n  data:\n  - secretKey: content\n    remoteRef:\n      key: yoursecret\nEOF\n</code></pre> <p>The secret default/imported-secret has been created by External Secret Operator in the management cluster.</p> <p>Now we can configure Sveltos to distribute such content to all managed clusters matching Kubernetes label selector env=fv</p> <p>Example</p> <pre><code>cat &gt; clusterprofile_extsecret.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  templateResourceRefs:\n  - resource:\n      apiVersion: v1\n      kind: Secret\n      name: imported-secret\n      namespace: default\n    identifier: ExternalSecret\n  policyRefs:\n  - kind: Secret\n    name: info\n    namespace: default\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: info\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\ntype: addons.projectsveltos.io/cluster-profile\nstringData:\n  secret.yaml: |\n    {{ $secret := (getResource \"ExternalSecret\") }}\n    {{ $secret := (chainSetField $secret \"metadata.ownerReferences\" list) }}\n    {{ toYaml $secret }}\nEOF\n</code></pre> <p>Note the ownerReferences field is reset. If a resource points to a non-existent owner, garbage collection might silently delete it. So resetting the field before asking Sveltos to copy it.</p> <p>Using sveltos CLI, it is possible to verify Sveltos has propagated the information to all managed clusters.</p> <pre><code>$ sveltosctl show addons\n+-----------------------------+---------------+-----------+------+---------+-------------------------------+------------------+\n|           CLUSTER           | RESOURCE TYPE | NAMESPACE | NAME | VERSION |             TIME              | CLUSTER PROFILES |\n+-----------------------------+---------------+-----------+------+---------+-------------------------------+------------------+\n| default/clusterapi-workload | :Secret       | default   | eso  | N/A     | 2023-07-24 05:18:19 -0700 PDT | deploy-resources |\n| gke/production              | :Secret       | default   | eso  | N/A     | 2023-07-24 05:18:29 -0700 PDT | deploy-resources |\n+-----------------------------+---------------+-----------+------+---------+-------------------------------+------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","external secret operator","external secret management"]},{"location":"template/generate_sveltos_configuration/","title":"Generating Sveltos configurations with Sveltos","text":"<p>Sveltos provides powerful templating capabilities that extend beyond simply deploying add-ons to clusters. This section dives into Sveltos' templating capabilities for generating Sveltos configurations themselves. This allows you to create reusable templates that adapt and generate child configurations based on defined variables.</p> <p></p> <ol> <li> <p>The <code>deploy-clusterprofiles</code> ClusterProfile acts as the trigger. It selects the management cluster (identified by the label type: mgmt) and references a ConfigMap named <code>test</code>.</p> </li> <li> <p>The referenced ConfigMap holds a template defined in the data section. This template utilizes a loop to dynamically generate two additional ClusterProfile resources: <code>keydb-services-production</code> and <code>keydb-services-staging</code>.</p> </li> <li> <p>Each generated ClusterProfile targets managed clusters (identified by a label change to purpose: edge) and includes specific environment configurations for production and staging deployments.</p> </li> </ol> <p>Note</p> <p>The management cluster has type: mgmt label</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-clusterprofiles\nspec:\n  clusterSelector:\n    matchLabels:\n      type: mgmt   # Select the management cluster\n  policyRefs:\n  - name: test\n    namespace: default\n    kind: ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: test\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  keydb_clusterprofile.yaml: |\n    {{ $cluster := print \"{{ .Cluster.metadata.name }}\" }}\n    {{- range $env := (list \"production\" \"staging\") }}\n    ---\n    apiVersion: config.projectsveltos.io/v1beta1\n    kind: ClusterProfile\n    metadata:\n      name: keydb-services-{{ $env }}\n    spec:\n      clusterSelector:\n        matchLabels:\n          purpose: edge # Select managed cluster\n      helmCharts:\n      - repositoryURL:    https://raw.githubusercontent.com/finkinfridom/charts/main/charts/\n        repositoryName:   finkinfridom\n        chartName:        finkinfridom/keydb\n        chartVersion:     \"0.48.3\"\n        releaseName:      keydb\n        releaseNamespace: {{ $env }}\n        helmChartAction:  Install\n        values: |\n          imageTag: \"x86_64_v6.3.2\"\n          multiMaster: \"yes\"\n          activeReplicas: \"yes\"\n          service:\n            annotations:\n              tailscale.com/expose: \"true\"\n              tailscale.com/hostname: {{ $cluster }}-keydb-{{ $env }}\n    {{- end }}\n</code></pre> <p>After applying this configuration, you'll end up with a total of three ClusterProfiles:</p> <pre><code>kubectl get clusterprofile\nNAME                        AGE\ndeploy-resources            18m\nkeydb-services-production   18m\nkeydb-services-staging      18m\n</code></pre> <p>Note this variable definition</p> <pre><code>{{ $cluster := print \"{{ .Cluster.metadata.name }}\" }}\n</code></pre> <p>It's important to capture it as a string to prevent further templating within the <code>deploy-clusterprofiles</code> ClusterProfile itself.</p> <p>The generated ClusterProfiles (<code>keydb-services-production</code> and <code>keydb-services-staging</code>) define Helm chart deployments tailored for their respective environments. For example, the production configuration utilizes the following Helm chart details:</p> <pre><code>  helmCharts:\n  - chartName: finkinfridom/keydb\n    chartVersion: 0.48.3\n    helmChartAction: Install\n    releaseName: keydb\n    releaseNamespace: production\n    repositoryName: finkinfridom\n    repositoryURL: https://raw.githubusercontent.com/finkinfridom/charts/main/charts/\n    values: |\n      imageTag: \"x86_64_v6.3.2\"\n      multiMaster: \"yes\"\n      activeReplicas: \"yes\"\n      service:\n        annotations:\n          tailscale.com/expose: \"true\"\n          tailscale.com/hostname: {{ .Cluster.metadata.name }}-keydb-production\n</code></pre> <p>Here, you can see how <code>{{ .Cluster.metadata.name }}</code> will be dynamically resolved with the actual managed cluster name when the Helm chart deploys on that cluster.</p>","tags":["Kubernetes","add-ons","helm","template"]},{"location":"template/intro_template/","title":"Templates","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/intro_template/#introduction-to-templates","title":"Introduction to Templates","text":"<p>Sveltos lets you define add-ons and applications using templates. Before deploying any resource down the managed clusters, Sveltos instantiates the templates using information gathered from the management cluster. Lua can also be used.</p> <p></p> <p>In this example, Sveltos retrieves the Secret imported-secret from the default namespace. This Secret is assigned the alias ExternalSecret. The template can subsequently refer to this Secret by employing the alias ExternalSecret. It can also be used with Helm Charts.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/intro_template/#template-functions","title":"Template Functions","text":"<p>Sveltos supports the template functions included from the Sprig open source project. The Sprig library provides over 70 template functions for Go\u2019s template language. Some of the functions are listed below. For the full list, have a look at the Spring Github page.</p> <ol> <li>String Functions: trim, wrap, randAlpha, plural, etc.</li> <li>String List Functions: splitList, sortAlpha, etc.</li> <li>Integer Math Functions: add, max, mul, etc.</li> <li>Integer Slice Functions: until, untilStep</li> <li>Float Math Functions: addf, maxf, mulf, etc.</li> <li>Date Functions: now, date, etc.</li> <li>Defaults Functions: default, empty, coalesce, fromJson, toJson, toPrettyJson, toRawJson, ternary</li> <li>Encoding Functions: b64enc, b64dec, etc.</li> <li>Lists and List Functions: list, first, uniq, etc.</li> <li>Dictionaries and Dict Functions: get, set, dict, hasKey, pluck, dig, deepCopy, etc.</li> <li>Type Conversion Functions: atoi, int64, toString, etc.</li> <li>Path and Filepath Functions: base, dir, ext, clean, isAbs, osBase, osDir, osExt, osClean, osIsAbs</li> <li>Flow Control Functions: fail</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/intro_template/#resource-manipulation-functions","title":"Resource Manipulation Functions","text":"<p>Sveltos provides a set of functions specifically designed for manipulating resources within your templates.</p> <ol> <li>getResource: Takes the identifier of a resource and returns a map[string]interface{} allowing to access any field of the resource.</li> <li>copy: Takes the identifier of a resource and returns a copy of that resource.</li> <li>setField: Takes the identifier of a resource, the field name, and a new value. Returns a modified copy of the resource with the specified field updated.</li> <li>removeField: Takes the identifier of a resource and the field name. Returns a modified copy of the resource with the specified field removed.</li> <li>getField: Takes the identifier of a resource and the field name. Returns the field value</li> <li>chainSetField: This function acts as an extension of setField. It allows for chaining multiple field updates.</li> <li>chainRemoveField: Similar to chainSetField, this function allows for chaining multiple field removals.</li> </ol> <p>Note</p> <p>These functions operate on copies of the original resource, ensuring the original data remains untouched.</p> <p>For practical examples, take a look here.</p> <p>Consider combining those methods with the post render patches approach.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/intro_template/#extra-template-functions","title":"Extra Template Functions","text":"<ol> <li>toToml: It takes an interface, marshals it to toml, and returns a string. It will always return a string, even on marshal error (empty string)</li> <li>toYaml: It takes an interface, marshals it to yaml, and returns a string. It will always return a string, even on marshal error (empty string)</li> <li>toJson: It takes an interface, marshals it to json, and returns a string. It will always return a string, even on marshal error (empty string)</li> <li>fromToml: It converts a TOML document into a map[string]interface{}</li> <li>fromYaml: It converts a YAML document into a map[string]interface{}</li> <li>fromYamlArray: It converts a YAML array into a []interface{}</li> <li>fromJson: It converts a YAML document into a map[string]interface{}</li> <li>fromJsonArray: It converts a JSON array into a []interface{}</li> </ol>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/intro_template/#next-steps","title":"Next Steps","text":"<ul> <li>Additional Templates Information</li> </ul>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/jsonnet_extension/","title":"Recources by Sveltos","text":"","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","jsonnet"]},{"location":"template/jsonnet_extension/#what-is-jsonnet","title":"What is JSONNET","text":"<p><code>jsonnet</code> is a data templating language and configuration file format designed to simplify the creation and management of complex JSON or YAML data structures. It provides a concise and flexible syntax for generating JSON-like data by defining reusable templates, composing objects and arrays, and applying transformations.</p> <p>If you want to use jsonnet in conjunction with Sveltos, you can install the jsonnet controller by performing the below:</p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/gianlucam76/jsonnet-controller/main/manifest/manifest.yaml\n</code></pre> <p>The above command will install the necessary components for <code>jsonnet controller</code>.</p> <p>The <code>jsonnet controller</code> offers the capability to process jsonnet files using different sources, such as Flux Sources (GitRepository/OCIRepository/Bucket), ConfigMap, or Secrets. It then programmatically invokes jsonnet go module and stores the output in the Status section making it available for Sveltos to consume.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","jsonnet"]},{"location":"template/jsonnet_extension/#option-1-gitrepository","title":"Option 1: GitRepository","text":"<p>We can leverage GitRepository as a source for the jsonnet controller<sup>1</sup>. For example, in the provided GitHub repository jsonnet-examples, we can find the jsonnet files that Flux will sync.</p> <p>To instruct the jsonnet controller to fetch files from this repository, create a JsonnetSource CRD instance with the below configuration:</p> <pre><code>---\napiVersion: extension.projectsveltos.io/v1beta1\nkind: JsonnetSource\nmetadata:\n  name: jsonnetsource-flux\nspec:\n  namespace: flux-system\n  name: flux-system\n  kind: GitRepository\n  path: ./variables/deployment.jsonnet\n  variables:\n    deploymentName: eng\n    namespace: staging\n    replicas: \"3\"\n</code></pre> <p>The <code>path</code> field specifies the location within the Git repository where the jsonnet file is stored. Once Flux detects changes in the repository and syncs it, the jsonnet-controller will automatically invoke the jsonnet module and store the output in the Status section of the JsonnetSource instance.</p> <p>At this point, you can use the Sveltos' template feature to deploy the output of the jsonnet (Kubernetes resources) to a managed cluster. The Kubernetes add-on controller will take care of deploying it<sup>2</sup>.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","jsonnet"]},{"location":"template/jsonnet_extension/#clusterprofile","title":"ClusterProfile","text":"<p>Example - ClusterProfile and Resources Definition</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  templateResourceRefs:\n  - resource:\n      apiVersion: extension.projectsveltos.io/v1beta1\n      kind: JsonnetSource\n      name: jsonnetsource-flux\n      namespace: default\n    identifier: JsonnetSource\n  policyRefs:\n  - kind: ConfigMap\n    name: jsonnet-resources\n    namespace: default\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: jsonnet-resources\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\ndata:\n  resource.yaml: |\n    {{ (getResource \"JsonnetSource\").status.resources }}\n</code></pre> <p>The above configuration instructs Sveltos to deploy the resources generated by jsonnet to the selected managed clusters.</p> <pre><code>$ sveltosctl show addons\n+-------------------------------------+-----------------+-----------+------+---------+-------------------------------+------------------+\n|               CLUSTER               |  RESOURCE TYPE  | NAMESPACE | NAME | VERSION |             TIME              | CLUSTER PROFILES |\n+-------------------------------------+-----------------+-----------+------+---------+-------------------------------+------------------+\n| default/sveltos-management-workload | apps:Deployment | staging   | eng  | N/A     | 2023-05-26 00:24:57 -0700 PDT | deploy-resources |\n+-------------------------------------+-----------------+-----------+------+---------+-------------------------------+------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","jsonnet"]},{"location":"template/jsonnet_extension/#option-2-configmapsecret","title":"Option 2: ConfigMap/Secret","text":"<p>Alternatively, you can use a ConfigMap/Secret as a source for the <code>jsonnet controller</code></p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","jsonnet"]},{"location":"template/jsonnet_extension/#step-1-create-a-tarball-containing-the-jsonnet-files","title":"Step 1: Create a tarball containing the jsonnet files","text":"<pre><code>$ tar -czf jsonnet.tar.gz -C ~mgianluc/go/src/github.com/gianlucam76/jsonnet-examples/multiple-files .\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","jsonnet"]},{"location":"template/jsonnet_extension/#step-2-create-a-configmap-with-the-tarball","title":"Step 2: Create a ConfigMap with the tarball","text":"<pre><code>$ kubectl create configmap jsonnet --from-file=jsonnet.tar.gz=jsonnet.tar.gz\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","jsonnet"]},{"location":"template/jsonnet_extension/#step-3-create-a-jsonnetsource-instance-that-references-this-configmap","title":"Step 3: Create a <code>JsonnetSource</code> instance that references this ConfigMap","text":"<pre><code>---\napiVersion: extension.projectsveltos.io/v1beta1\nkind: JsonnetSource\nmetadata:\n  name: jsonnetsource-configmap\nspec:\n  namespace: default\n  name: jsonnet\n  kind: ConfigMap\n  path: ./main.jsonnet\n  variables:\n    namespace: production\n</code></pre> <p>Outcome will be same as seen above with Flux GitRepository:</p> <pre><code>apiVersion: extension.projectsveltos.io/v1beta1\nkind: JsonnetSource\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"extension.projectsveltos.io/v1beta1\",\"kind\":\"JsonnetSource\",\"metadata\":{\"annotations\":{},\"name\":\"jsonnetsource-configmap\",\"namespace\":\"default\"},\"spec\":{\"kind\":\"ConfigMap\",\"name\":\"jsonnet\",\"namespace\":\"default\",\"path\":\"./main.jsonnet\",\"variables\":{\"namespace\":\"production\"}}}\n  creationTimestamp: \"2023-05-26T08:28:48Z\"\n  generation: 1\n  name: jsonnetsource-configmap\n  namespace: default\n  resourceVersion: \"121599\"\n  uid: eea93390-771d-4176-92fe-2b761b803764\nspec:\n  kind: ConfigMap\n  name: jsonnet\n  namespace: default\n  path: ./main.jsonnet\n  variables:\n    namespace: production\nstatus:\n  resources: |\n    ---\n    {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"name\":\"my-deployment\",\"namespace\":\"production\"},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"app\":\"my-app\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"my-app\"}},\"spec\":{\"containers\":[{\"image\":\"my-image:latest\",\"name\":\"my-container\",\"ports\":[{\"containerPort\":8080}]}]}}}}\n    ---\n    {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"name\":\"my-service\",\"namespace\":\"production\"},\"spec\":{\"ports\":[{\"port\":80,\"protocol\":\"TCP\",\"targetPort\":8080}],\"selector\":{\"app\":\"my-app\"},\"type\":\"LoadBalancer\"}}\n</code></pre> <pre><code>  ---\n  apiVersion: source.toolkit.fluxcd.io/v1\n  kind: GitRepository\n  metadata:\n    finalizers:\n    - finalizers.fluxcd.io\n    name: flux-system\n    namespace: flux-system\n  spec:\n    interval: 1m0s\n    ref:\n      branch: main\n    secretRef:\n      name: flux-system\n    timeout: 60s\n    url: ssh://git@github.com/gianlucam76/jsonnet-examples\n</code></pre> <ol> <li> <p>Flux is present in the management cluster and it is used to sync from GitHub repository. The GitRepository instance is the below.\u00a0\u21a9</p> </li> <li> <p>Instructing Sveltos involves the initial step of retrieving a resource from the management cluster, which is the JsonnetSource instance named <code>jsonnetsource-flux</code> in the <code>default</code> namespace. Sveltos is then responsible for deploying the resources found within the <code>jsonnet-resources</code> ConfigMap. However, this ConfigMap acts as a template, requiring instantiation before deployment. Within the Data section of the ConfigMap, there is a single entry called <code>resource.yaml</code>. After instantiation, this entry will contain the content that the jsonnet controller has stored in the JsonnetSource instance.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","jsonnet"]},{"location":"template/lua/","title":"Lua","text":"<p>Sveltos enables the execution of Lua code stored within ConfigMap or Secret Kubernetes resources. To allow Sveltos to identify and reference these resources within the Sveltos profiles, the annotation <code>projectsveltos.io/lua</code> annotation needs to be defined within the resources. Check out the example below. The Lua code gets executed, and the resulting output is used to deploy configurations to matching clusters.</p> <p>This will instruct Sveltos to fetch the Secret imported-secret in the default namespace and replicate it to any env: prod cluster:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  annotations:\n    projectsveltos.io/lua: ok\n  name: lua\n  namespace: default\ndata:\n  lua.yaml: |-\n    local json = require(\"json\")\n\n    function evaluate()\n      local secret = getResource(resources, \"ExternalSecret\")\n      print(secret.metadata.name)\n\n      local secretObj = {\n        apiVersion = \"v1\",\n        kind = \"Secret\",\n        metadata = {\n          name = secret.metadata.name,\n          namespace = secret.metadata.namespace,\n        },\n        data = secret.data,\n      }\n\n      local hs = {}\n      hs.resources = json.encode(secretObj)\n      return hs\n    end\n</code></pre> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  templateResourceRefs:\n  - resource:\n      apiVersion: v1\n      kind: Secret\n      name: imported-secret\n      namespace: default\n    identifier: ExternalSecret\n  policyRefs:\n  - kind: ConfigMap\n    name: lua\n    namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template","lua"]},{"location":"template/lua/#helper-functions","title":"Helper Functions:","text":"<p>Sveltos provides several helper functions to simplify resource manipulation within Lua scripts:</p> <ul> <li>getResource(resources, \"resource identifier\"):  Retrieves a specific resource by its identifier from the provided resource list. The resources represents all Kubernetes resources defined in the Spec.TemplateResourceRefs section of a Sveltos profile. The identifier must match an identifier previously defined in the same Spec.TemplateResourceRefs section.</li> <li>getLabel(resource, \"key\"): Returns the value of the label with the specified key for a given resource.</li> <li>getAnnotation(resource, \"key\"): Returns the value of the annotation with the specified key for a given resource.</li> <li>base64Encode and base64Decode: For base64 encoding and decoding.</li> <li>json.encode and json.decode: For JSON encoding and decoding.</li> <li>strings functions: Provides a range of string manipulation functions such as Compare, Contains, HasPrefix, HasSuffix, Join, Replace, Split, ToLower, and ToUpper. These functions are based on the this library. To use these methods, call them as strings.ToUpper(\"mystring\").</li> </ul> <p>Some examples:</p> <pre><code>  local strings = require(\"strings\")\n\n  function evaluate()\n    local secret = getResource(resources, \"ExternalSecret\")\n    print(strings.ToUpper(secret.metadata.name))\n    local splitTable = strings.Split(secret.metadata.name, \"-\") -- metadata.name in the example imported-secret\n    for i, v in ipairs(splitTable) do\n      print(\"Element\", i, \":\", v)\n    end\n    local encoded = base64Encode(secret.metadata.name)\n</code></pre> <pre><code>  local json = require(\"json\")\n\n  function evaluate()\n    local secret = getResource(resources, \"ExternalSecret\")\n    local encoded = json.encode(secret)\n    print(encoded)\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template","lua"]},{"location":"template/lua/#extending-lua-capabilities-with-custom-helper-functions","title":"Extending Lua Capabilities with Custom Helper Functions:","text":"<p>Sveltos allows users to extend its Lua scripting capabilities by defining custom helper functions. These functions are packaged as Lua code within a ConfigMap residing in the projectsveltos namespace.</p> <p>To load and utilize the custom functions, the<code>lua-methods</code> argument must be provided to the addon-controller  deployment. Once loaded, the custom methods become available whenever Lua code is executed by Sveltos.</p> <p>Note</p> <p>The below libraries are used. https://github.com/chai2010/glua-strings/ for string manipulation. https://github.com/layeh/gopher-json for JSON handling.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template","lua"]},{"location":"template/template_generic_examples/","title":"Templates Generic Examples","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_generic_examples/#template-generic-examples","title":"Template Generic Examples","text":"<p>This section is designed to help users get started with the Sveltos template feature. It provides simple, easy-to-follow examples. Let's dive in!</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_generic_examples/#example-calico-cni-deployment","title":"Example - Calico CNI Deployment","text":"<p>Imagine we want to set up Calico CNI on several CAPI powered clusters, automatically fetching Pod CIDRs from each cluster. Sveltos <code>ClusterProfile</code> definition lets you create a configuration with these details, and it will automatically deploy Calico to all matching clusters.</p> <p>In the example below, we use the Sveltos cluster label <code>env=fv</code> to identify all clusters that should use Calico as their CNI. Because the values are expressed as a template, Sveltos will dynamically replace the <code>{{ range ... }}</code> with the actual CIDRs from each target cluster. Calico will get configured with the correct Pod CIDRs for every cluster. No manual intervention is required.</p> <p>Example - ClusterProfile Calico Deployment</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-calico\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  helmCharts:\n  - repositoryURL:    https://projectcalico.docs.tigera.io/charts\n    repositoryName:   projectcalico\n    chartName:        projectcalico/tigera-operator\n    chartVersion:     v3.24.5\n    releaseName:      calico\n    releaseNamespace: tigera-operator\n    helmChartAction:  Install\n    values: |\n      installation:\n        calicoNetwork:\n          ipPools:\n          {{ range $cidr := .Cluster.spec.clusterNetwork.pods.cidrBlocks }}\n            - cidr: {{ $cidr }}\n              encapsulation: VXLAN\n          {{ end }}\n</code></pre> <p>The entire helmCharts section can be defined as a template</p> <p>Example - HelmCharts Section Defined as a Template</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  templateResourceRefs: # Instruct Sveltos to fetch the Cluster instance so it reacts to **any** Cluster change\n  - resource:\n      apiVersion: cluster.x-k8s.io/v1beta1\n      kind: Cluster\n      name: \"{{ .Cluster.metadata.name }}\"\n    identifier: Cluster\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     |-\n      {{$version := index .Cluster.metadata.labels \"k8s-version\" }}{{if eq $version \"v1.29.0\"}}v3.2.5\n      {{else}}v3.2.6\n      {{end}}\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_generic_examples/#example-deploy-kyverno-with-different-replicas","title":"Example - Deploy Kyverno with different replicas","text":"<p>We can define any resource contained in a referenced ConfigMap/Secret as a template by adding the <code>projectsveltos.io/template</code> annotation. This ensures that the template is instantiated at the deployment time, making the deployments faster and more efficient.</p> <p>For this example, we have two Civo clusters already registered with Sveltos. The clusters are the Sveltos managed clusters.</p> <pre><code>$ kubectl get sveltoscluster -n civo --show-labels\nNAME       READY   VERSION        LABELS\ncluster1   true    v1.29.2+k3s1   env=demo,projectsveltos.io/k8s-version=v1.29.2\ncluster2   true    v1.29.2+k3s1   env=demo,projectsveltos.io/k8s-version=v1.29.2\n</code></pre> <p>We also have two <code>ConfigMap</code> resources.</p> <pre><code>$ kubectl get configmap -n civo\nNAME               DATA   AGE\ncluster1           1      43m\ncluster2           1      43m\n</code></pre> <p>The content of the ConfigMap with the name <code>civo/cluster1</code> can be found below.</p> <p>Example - ConfigMap cluster1 Definition</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster1\n  namespace: civo\ndata:\n  values: |\n    admissionController:\n      replicas: 3\n    backgroundController:\n      replicas: 3\n    cleanupController:\n      replicas: 3\n    reportsController:\n      replicas: 3\n</code></pre> <p>The content of the ConfigMap with the <code>civo/cluster2</code> can be found below.</p> <p>Example - ConfigMap cluster2 Definition</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster2\n  namespace: civo\ndata:\n  values: |\n    admissionController:\n      replicas: 1\n    backgroundController:\n      replicas: 1\n    cleanupController:\n      replicas: 1\n    reportsController:\n      replicas: 1\n</code></pre> <p>Once we are happy with the configuration, we can proceed further with the Sveltos <code>ClusterProfile</code> resources. Have a look at the YAML definitions below.</p> <p>Example - ClusterProfile Kyverno</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: demo\n  templateResourceRefs:\n  - resource:\n      apiVersion: v1\n      kind: ConfigMap\n      name: \"{{ .Cluster.metadata.name }}\"\n    identifier: ConfigData\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    values: |\n      {{ (getResource \"ConfigData\").data.values }}\n</code></pre> <p>The <code>ClusterProfile</code> above will deploy Kyverno with 3 replicas on <code>cluster1</code>.</p> <pre><code>$ KUBECONFIG=civo-cluster1-kubeconfig kubectl get deployments -n kyverno\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\nkyverno-background-controller   3/3     3            3           15m\nkyverno-reports-controller      3/3     3            3           15m\nkyverno-cleanup-controller      3/3     3            3           15m\nkyverno-admission-controller    3/3     3            3           15m\n</code></pre> <p>The <code>ClusterProfile</code> for <code>cluster02</code> will deploy Kyverno with 1 replicas.</p> <pre><code>$ KUBECONFIG=civo-cluster2-kubeconfig kubectl get deployments -n kyverno\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\nkyverno-reports-controller      1/1     1            1           17m\nkyverno-background-controller   1/1     1            1           17m\nkyverno-cleanup-controller      1/1     1            1           17m\nkyverno-admission-controller    1/1     1            1           17m\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_generic_examples/#using-valuesfrom","title":"Using ValuesFrom","text":"<p>The same outcome can be achieved by leveraging Sveltos's <code>valuesFrom</code> feature.</p> <p>Example - ClusterProfile Kyverno</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: demo\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    valuesFrom:\n    - kind: ConfigMap\n      name: \"{{ .Cluster.metadata.name }}\"\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_generic_examples/#example-autoscaler-definition","title":"Example - Autoscaler Definition","text":"","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_generic_examples/#clusterprofile","title":"ClusterProfile","text":"<p>The below YAML definition instruct Sveltos to find a Secret named autoscaler in the default namespace. Sveltos makes the Secret available to the template using the keyword AutoscalerSecret.</p> <p>Example - ClusterProfile Resource Definition</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  templateResourceRefs:\n  - resource:\n      apiVersion: v1\n      kind: Secret\n      name: autoscaler\n      namespace: default\n    identifier: AutoscalerSecret\n  policyRefs:\n  - kind: ConfigMap\n    name: info\n    namespace: default\n</code></pre> <p>By adding the special annotation (<code>projectsveltos.io/template: ok</code>) to a ConfigMap named info (also in the default namespace), we can define a template within it. Find the example template below.</p> <p>Example - ConfigMap Definition</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: info\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\ndata:\n  secret.yaml: |\n    # AutoscalerSecret now references the Secret default/autoscaler\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: autoscaler\n      namespace: {{ (getResource \"AutoscalerSecret\").metadata.namespace }}\n    data:\n      token: {{ (getResource \"AutoscalerSecret\").data.token }}\n      ca.crt: {{ $data:=(getResource \"AutoscalerSecret\").data }} {{ (index $data \"ca.crt\") }}\n</code></pre> <p>Sveltos will use the content of the AutoscalerSecret to fill in the placeholders when deploying the resources to your managed clusters.</p> <p>Tip</p> <p>Sveltos stores information about fetched resources internally using a map data structure. For more technical details, feel free to get in touch via Slack.</p> <p>To use any resource that Sveltos has found based on the defintion, simply use the syntax below in the YAML template:</p> <pre><code>(getResource \"&lt;Identifier&gt;\")\n</code></pre> <p>Replace <code>&lt;Identifier&gt;</code> with the name you gave that resource in your ClusterProfile definition (like AutoscalerSecret).</p> <p>This works the same way for Helm charts. Inside the <code>values</code> section of the Helm chart, we can reference any data stored in the autoscaler Secret from the default namespace using the same identifier (AutoscalerSecret).</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_generic_examples/#example-replicate-secrets-with-sveltos","title":"Example - Replicate Secrets with Sveltos","text":"<p>In this scenario, imagine the management cluster has an External Secret Operator set up. The operator acts as a bridge, securely fetching secrets from a separate system. The secrets are stored safely within the management cluster.</p> <p>Suppose the following YAML code represents a Secret within the management cluster managed by External Secret Operator.</p> <p>Example - Secret Definition</p> <pre><code>---\napiVersion: v1\ndata:\n  key1: dmFsdWUx\n  key2: dmFsdWUy\nkind: Secret\nmetadata:\n  creationTimestamp: \"2024-05-27T13:51:00Z\"\n  name: external-secret-operator\n  namespace: default\n  resourceVersion: \"28731\"\n  uid: 99411506-8f5e-4846-9628-58f82b3d01be\ntype: Opaque\n</code></pre> <p>We want to replicate across all our <code>production</code> clusters. Sveltos can automate this process. Here's a step-by-step approach.</p> <p>Firstly, we create a <code>ConfigMap</code> named replicate-external-secret-operator-secret in the default namespace. The data section of this ConfigMap will act as a template for deploying the secret.</p> <p>Example - ConfigMap Definition</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: replicate-external-secret-operator-secret\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\ndata:\n  secret.yaml: |\n    {{ copy \"ESOSecret\" }}\n</code></pre> <ul> <li>The <code>projectsveltos.io/template: ok</code> annotation tells Sveltos this is a template</li> <li>The template references a placeholder named ESOSecret, which will be filled with the actual secret data later</li> </ul> <p>Next, we will define a <code>ClusterProfile</code> named replicate-external-secret-operator-secret. The profile instructs Sveltos on how to deploy the secrets:</p> <p>Example - ClusterProfile Resources Definition</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: replicate-external-secret-operator-secret\nspec:\n  clusterSelector:\n    matchLabels:\n      env: production\n  templateResourceRefs:\n  - resource:\n      apiVersion: v1\n      kind: Secret\n      name: external-secret-operator\n      namespace: default\n    identifier: ESOSecret\n  policyRefs:\n  - kind: ConfigMap\n    name: replicate-external-secret-operator-secret\n    namespace: default\n</code></pre> <ul> <li>The clusterSelector targets clusters with the label <code>env=production</code></li> <li>The templateResourceRefs section tells Sveltos to fetch the Secret named external-secret-operator from the default namespace. This secret managed by External Secret Operator that holds the actual data we want to replicate</li> <li>The identifier: ESOSecret connects this fetched secret to the placeholder in the template</li> <li>The policyRefs section references the ConfigMap we created earlier, which contains the template for deploying the secret</li> </ul> <p>By following the steps above, Sveltos will automatically deploy the secrets managed by the External Secret Operator to all your production clusters. This ensures consistent and secure access to these secrets across your production environment.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_generic_examples/#example-replicate-cloudconfig-with-sveltos","title":"Example - Replicate CloudConfig with Sveltos","text":"<p>In this scenario, the management cluster manages multiple clusters in a cloud provider. Let's assume the cloud provider for this specific example is Azure. In some setups, there would be a need to put a <code>cloud-config</code> Secret with dynamic content inside the managed clusters. We can leverage Sveltos templates for that.</p> <p>Firstly we need to ensure that ClusterProfile (or Profile) has additional elements in <code>templateResourceRefs</code>, and <code>policies</code> defined:</p> <p>Example - ClusterProfile Resource Definition</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  templateResourceRefs:\n  - resource:\n      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n      kind: AzureClusterIdentity\n      name: azure-cluster-identity\n    identifier: InfrastructureProviderIdentity\n  - resource:\n      apiVersion: v1\n      kind: Secret\n      name: azure-cluster-identity-secret\n    identifier: InfrastructureProviderIdentitySecret\n  policyRefs:\n  - kind: ConfigMap\n    name: azure-cloud-provider\n    namespace: default\n</code></pre> <p>We expose additional <code>InfrastructureProviderIdentity</code> and <code>InfrastructureProviderIdentitySecret</code> for templating purposes, and the <code>azure-cloud-provider</code> ConfigMap defined in policyRefs will be our template for pushing the <code>cloud-config</code> Secret to the managed clusters.</p> <p>Example - CloudConfig template</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: azure-cloud-provider\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    {{- $subnetName := \"\" -}}\n    {{- $securityGroupName := \"\" -}}\n    {{- $routeTableName := \"\" -}}\n    {{- range $cluster.spec.networkSpec.subnets -}}\n      {{- if eq .role \"node\" -}}\n        {{- $subnetName = .name -}}\n        {{- $securityGroupName = .securityGroup.name -}}\n        {{- $routeTableName = .routeTable.name -}}\n        {{- break -}}\n      {{- end -}}\n    {{- end -}}\n    {{- $cloudConfig := dict\n      \"aadClientId\" $identity.spec.clientID\n      \"aadClientSecret\" (index $secret.data \"clientSecret\" |b64dec)\n      \"cloud\" $cluster.spec.azureEnvironment\n      \"loadBalancerName\" \"\"\n      \"loadBalancerSku\" \"Standard\"\n      \"location\" $cluster.spec.location\n      \"maximumLoadBalancerRuleCount\" 250\n      \"resourceGroup\" $cluster.spec.resourceGroup\n      \"routeTableName\" $routeTableName\n      \"securityGroupName\" $securityGroupName\n      \"securityGroupResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n      \"subnetName\" $subnetName\n      \"subscriptionId\" $cluster.spec.subscriptionID\n      \"tenantId\" $identity.spec.tenantID\n      \"useInstanceMetadata\" true\n      \"useManagedIdentityExtension\" false\n      \"vmType\" \"vmss\"\n      \"vnetName\" $cluster.spec.networkSpec.vnet.name\n      \"vnetResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n    -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: azure-cloud-provider\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-config: {{ $cloudConfig | toJson |b64enc }}\n    ---\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: dump-cluster-sveltos-object\n      namespace: default\n    data:\n      object: |\n        {{ .Cluster | toYaml | nindent 4 }}\n</code></pre> <p>Note that we have 2 objects that are going to be created inside managed clusters via templating. <code>azure-cloud-provider</code> is the actual <code>cloud-config</code> that we need, and <code>dump-cluster-sveltos-object</code> is referenced here for illustrative purposes and can be used for debugging the template itself (seeing what values the reference object contains).</p> <p>It's worth mentioning also that template rendering status can be seen in the related <code>ClusterSummary</code> object status, this helps a lot in developing template script.</p>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/template_overlays_examples/","title":"Templates Generic Examples","text":"<p>This Sveltos configuration utilizes a powerful overlay pattern to dynamically deploy the Kyverno Helm chart. It targets all clusters with the label <code>zone: us-east-2</code> while customizing the deployment for each one. The core of this pattern is the runtime matching of cluster metadata.</p> <p>By using Go templates within the templateResourceRefs section, the ClusterProfile dynamically constructs the names of ConfigMaps:</p> <ul> <li>kyverno-{{.Cluster.metadata.labels.environment}}-version</li> <li>kyverno-{{.Cluster.metadata.labels.environment}}-values.</li> </ul> <p>At deployment time, Sveltos fetches the specific ConfigMap that matches the environment label of each target cluster. This allows you to maintain a single, reusable ClusterProfile that acts as a blueprint, while the individual ConfigMaps serve as overlays that provide the unique version and values for each specific environment (e.g., staging, production).</p> <p>This method effectively decouples the deployment logic from the configuration data, enabling a scalable and maintainable GitOps workflow for managing add-ons across diverse Kubernetes clusters.</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      zone: us-east-2\n  syncMode: Continuous\n  templateResourceRefs:\n  - resource:\n      apiVersion: cluster.x-k8s.io/v1beta2\n      kind: Cluster\n      name: \"{{ .Cluster.metadata.name }}\"\n    identifier: Cluster\n  - resource:\n      apiVersion: v1\n      kind: ConfigMap\n      name: \"kyverno-{{- if index .Cluster.metadata.labels `environment` -}}{{- index .Cluster.metadata.labels `environment` -}}{{- end -}}-version\"\n      namespace: default\n    identifier: Version\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     \"{{- index (getResource `Version`).data `chartVersion` -}}\"\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n    valuesFrom:\n    - kind: ConfigMap\n      name: kyverno-{{- if index .Cluster.metadata.labels `environment` -}}{{- index .Cluster.metadata.labels `environment` -}}{{- end -}}-values\n      namespace: default\n      optional: true\n</code></pre> <p>With this Sveltos configuration, the specific ConfigMaps used for the Kyverno deployment depend entirely on the environment label of the target cluster.</p> <p>When a cluster has the label <code>environment: production</code>, the ClusterProfile's Go templates resolve to the following:</p> <ul> <li>Version ConfigMap: kyverno-production-version</li> <li>Values ConfigMap: kyverno-production-values</li> </ul> <p>Sveltos will use these ConfigMaps to install the Kyverno Helm chart with 3 replicas for both the admissionController and backgroundController, as specified in the production ConfigMap. This is a classic example of a production-level, high-availability configuration.</p> <p>When a cluster has the label <code>environment: staging</code>, the ClusterProfile's Go templates resolve to the following:</p> <ul> <li>Version ConfigMap: kyverno-staging-version</li> <li>Values ConfigMap: kyverno-staging-values</li> </ul> <p>In this case, Sveltos will install Kyverno with all controllers set to 1 replica, which is typical for a staging or testing environment to conserve resources.</p> <p>This demonstrates how a single ClusterProfile can manage multiple environments by dynamically selecting the correct configuration \"overlay\" at runtime based on the cluster's labels.</p> <pre><code>apiVersion: v1\ndata:\n  chartVersion:     3.5.1\nkind: ConfigMap\nmetadata:\n  name: kyverno-production-version\n  namespace: default\n---\napiVersion: v1\ndata:\n  values: |2\n          admissionController:\n            replicas: 3\n          backgroundController:\n            replicas: 3\n          cleanupController:\n            replicas: 1\n          reportsController:\n            replicas: 1\nkind: ConfigMap\nmetadata:\n  name: kyverno-production-values\n  namespace: default\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  chartVersion:     3.5.1\nkind: ConfigMap\nmetadata:\n  name: kyverno-staging-version\n  namespace: default\n---\napiVersion: v1\ndata:\n  values: |2\n          admissionController:\n            replicas: 1\n          backgroundController:\n            replicas: 1\n          cleanupController:\n            replicas: 1\n          reportsController:\n            replicas: 1\nkind: ConfigMap\nmetadata:\n  name: kyverno-staging-values\n  namespace: default\n</code></pre>","tags":["Kubernetes","add-ons","helm","clusterapi","multi-tenancy","template"]},{"location":"template/ytt_extension/","title":"Recources by Sveltos","text":"","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","vmware carvel ytt"]},{"location":"template/ytt_extension/#what-is-ytt","title":"What is YTT","text":"<p><code>ytt</code> is a powerful tool used for templating and patching YAML files. If you want to use Carvel ytt in conjunction with Sveltos, you can install the ytt controller by executing the below command:</p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/gianlucam76/ytt-controller/main/manifest/manifest.yaml\n</code></pre> <p>The above will install the necessary components for <code>ytt controller</code>.</p> <p>The <code>ytt controller</code> offers the capability to process Carvel ytt files using different sources, such as Flux Sources (GitRepository/OCIRepository/Bucket), ConfigMap, or Secret. It then programmatically invokes Carvel ytt module and stores the output in its Status section making it available for Sveltos.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","vmware carvel ytt"]},{"location":"template/ytt_extension/#option-1-gitrepository","title":"Option 1: GitRepository","text":"<p>We can leverage the GitRepository as a source for the ytt controller<sup>1</sup>. For example, in the provided GitHub repository ytt-examples, we can find the ytt files that Flux will sync. To instruct the ytt controller to fetch files from this repository, create a YttSource CRD instance with the below configuration:</p> <pre><code>---\napiVersion: extension.projectsveltos.io/v1beta1\nkind: YttSource\nmetadata:\n  name: yttsource-flux\nspec:\n  namespace: flux-system\n  name: flux-system\n  kind: GitRepository\n  path: ./deployment/\n</code></pre> <p>The <code>path</code> field specifies the location within the Git repository where the ytt files are stored. Once Flux detects changes in the repository and syncs it, the ytt-controller will automatically invoke the ytt module and store the output in the Status section of the YttSource instance.</p> <p>At this point, you can use the Sveltos' template feature to deploy the output of ytt (Kubernetes resources) to a managed cluster. The Kubernetes add-on controller will take care of deploying it<sup>2</sup>.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","vmware carvel ytt"]},{"location":"template/ytt_extension/#clusterprofile","title":"ClusterProfile","text":"<p>Example - ClusterProfile and Resources Definition</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-ytt-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  templateResourceRefs:\n  - resource:\n      apiVersion: extension.projectsveltos.io/v1beta1\n      kind: YttSource\n      name: yttsource-flux\n      namespace: default\n    identifier: YttSource\n  policyRefs:\n  - kind: ConfigMap\n    name: ytt-resources\n    namespace: default\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ytt-resources\n  namespace: default\n  annotations:\n    projectsveltos.io/template: ok  # add annotation to indicate Sveltos content is a template\ndata:\n  resource.yaml: |\n    {{ (getResource \"YttSource\").status.resources }}\n</code></pre> <p>The above configuration instructs Sveltos to deploy the resources generated by ytt to the selected managed clusters.</p> <pre><code>$ sveltosctl show addons\n+-------------------------------------+-----------------+-----------+----------------------+---------+-------------------------------+------------------+\n|               CLUSTER               |  RESOURCE TYPE  | NAMESPACE |         NAME         | VERSION |             TIME              | CLUSTER PROFILES |\n+-------------------------------------+-----------------+-----------+----------------------+---------+-------------------------------+------------------+\n| default/sveltos-management-workload | :Service        | staging   | sample-app           | N/A     | 2023-05-22 08:00:28 -0700 PDT | deploy-resources |\n| default/sveltos-management-workload | apps:Deployment | staging   | sample-app           | N/A     | 2023-05-22 08:00:28 -0700 PDT | deploy-resources |\n| default/sveltos-management-workload | :Secret         | staging   | application-settings | N/A     | 2023-05-22 08:00:28 -0700 PDT | deploy-resources |\n+-------------------------------------+-----------------+-----------+----------------------+---------+-------------------------------+------------------+\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","vmware carvel ytt"]},{"location":"template/ytt_extension/#option-2-configmapsecret","title":"Option 2: ConfigMap/Secret","text":"<p>Alternatively, you can use ConfigMap/Secret as a source for <code>ytt controller</code>.</p>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","vmware carvel ytt"]},{"location":"template/ytt_extension/#step-1-create-a-tarball-containing-the-ytt-files","title":"Step 1: Create a tarball containing the ytt files","text":"<pre><code>$ tar -czf ytt.tar.gz -C ~mgianluc/go/src/github.com/gianlucam76/ytt-examples/deployment .\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","vmware carvel ytt"]},{"location":"template/ytt_extension/#step-2-create-a-configmap-with-the-tarball","title":"Step 2: Create a ConfigMap with the tarball","text":"<pre><code>$ kubectl create configmap ytt --from-file=ytt.tar.gz=ytt.tar.gz\n</code></pre>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","vmware carvel ytt"]},{"location":"template/ytt_extension/#step-3-create-a-yttsource-instance-that-references-this-configmap","title":"Step 3: Create a <code>YttSource</code> instance that references this ConfigMap","text":"<pre><code>---\napiVersion: extension.projectsveltos.io/v1beta1\nkind: YttSource\nmetadata:\n  name: yttsource-sample\nspec:\n  namespace: default\n  name: ytt\n  kind: ConfigMap\n  path: ./\n</code></pre> <p>Outcome will be same as seen above with Flux GitRepository:</p> <pre><code>---\napiVersion: extension.projectsveltos.io/v1beta1\nkind: YttSource\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"extension.projectsveltos.io/v1beta1\",\"kind\":\"YttSource\",\"metadata\":{\"annotations\":{},\"name\":\"yttsource-sample\",\"namespace\":\"default\"},\"spec\":{\"kind\":\"ConfigMap\",\"name\":\"ytt\",\"namespace\":\"default\",\"path\":\"./\"}}\n  creationTimestamp: \"2023-05-22T06:27:31Z\"\n  generation: 1\n  name: yttsource-sample\n  namespace: default\n  resourceVersion: \"94517\"\n  uid: 4b0b4efb-57b4-4ffd-ab32-dc56fee21a09\nspec:\n  kind: ConfigMap\n  name: ytt\n  namespace: default\n  path: ./\nstatus:\n  resources: |\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: sample-app\n      labels:\n        environment: staging\n    spec:\n      selector:\n        app: sample-app\n      ports:\n      - protocol: TCP\n        port: 80\n        targetPort: 8080\n    ---\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: sample-app\n      labels:\n        environment: staging\n    spec:\n      replicas: 1\n      selector:\n        matchLabels:\n          environment: staging\n      template:\n        metadata:\n          labels:\n            environment: staging\n        spec:\n          containers:\n          - name: sample-app\n            image: nginx:latest\n            imagePullPolicy: IfNotPresent\n            ports:\n            - containerPort: 8080\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: application-settings\n    stringData:\n      app_mode: staging\n      certificates: /etc/ssl/staging\n      db_user: staging-user\n      db_password: staging-password\n</code></pre> <pre><code>  ---\n  apiVersion: source.toolkit.fluxcd.io/v1\n  kind: GitRepository\n  metadata:\n    finalizers:\n    - finalizers.fluxcd.io\n    name: flux-system\n    namespace: flux-system\n  spec:\n    interval: 1m0s\n    ref:\n      branch: main\n    secretRef:\n      name: flux-system\n    timeout: 60s\n    url: ssh://git@github.com/gianlucam76/ytt-examples\n</code></pre> <ol> <li> <p>Flux is present in the management cluster and it is used to sync from GitHub repository. The GitRepository instance is the below.\u00a0\u21a9</p> </li> <li> <p>Instructing Sveltos involves the initial step of retrieving a resource from the management cluster, which is the YttSource instance named <code>yttsource-flux</code> in the <code>default</code> namespace. Sveltos is then responsible for deploying the resources found within the <code>ytt-resources</code> ConfigMap. However, this ConfigMap acts as a template, requiring instantiation before deployment. Within the Data section of the ConfigMap, there is a single entry called <code>resource.yaml</code>. After instantiation, this entry will contain the content that the ytt controller has stored in the YttSource instance.\u00a0\u21a9</p> </li> </ol>","tags":["Kubernetes","Sveltos","add-ons","helm","clusterapi","vmware carvel ytt"]},{"location":"use_cases/use_case_gitops/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease | GitOps | Flux Integration","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_gitops/#sveltos-and-gitops","title":"Sveltos and GitOps","text":"<p>Flux is a CNCF graduate project that offers users a set of continuous and progressive delivery solutions for Kubernetes which are open and extensible.</p> <p>By integrating Flux with Sveltos, we can automate the synchronisation of any desired Kubernetes add-ons, removing any manual steps and ensuring consistent deployment across different clusters.</p> <p></p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_gitops/#what-are-the-benefits","title":"What are the benefits?","text":"<ol> <li>Centralized Configuration: Store <code>YAML/JSON</code> manifests in a central Git repository or Bucket.</li> <li>Continuous Synchronisation: Flux in the management cluster ensures continuous synchronisation of configurations.</li> <li>Consistent Deployments: Use Sveltos <code>ClusterProfiles</code>, <code>Profiles</code> to reliably deploy Kubernetes add-ons in matching clusters.</li> </ol>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_gitops/#how-it-works","title":"How it works?","text":"<p>Store all required Kubernetes resources in a Git repository and let Flux handle continuous synchronisation. Below, we show how to use Flux and Sveltos to deploy a HelloWorld application across multiple managed clusters.</p> <p>\ud83d\udc49 Explore the Example Repository</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_gitops/#step-1-configure-flux-in-the-management-cluster","title":"Step 1: Configure Flux in the Management Cluster","text":"<p>Install and run Flux in the management cluster. Configure it to synchronise the Git repository which contains the <code>HelloWorld</code> manifests. Use a GitRepository resource similar to the below YAML definitions. More information about the Flux installation can be found here.</p> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: flux-system\n  namespace: flux-system\n  annotations:\n    projectsveltos.io/template: ok # (1)\nspec:\n  interval: 1m0s # (2)\n  ref:\n    branch: main\n  secretRef:\n    name: flux-system\n  timeout: 60s\n  url: https://github.com/gianlucam76/kustomize.git # (3)\n</code></pre> <ol> <li>Enable Sveltos templating functionality. More information have a look here.</li> <li>How often to sync with the reposiroty</li> <li>Reflects the repository we want to use</li> </ol> <p>The above definition will look for updates of the main branch of the specified repository every minute.</p> <p>Info</p> <p>If you use the Flux CLI to bootstrap a Git repo, the <code>GitRepository</code> Kubernetes resource will get created by Flux automatically.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_gitops/#step-2-create-a-sveltos-clusterprofile","title":"Step 2: Create a Sveltos ClusterProfile","text":"<p>Define a Sveltos ClusterProfile referencing to the <code>flux-system</code> <code>GitRepository</code> resource and define the HelloWorld directory as the deployment source. In the below YAML definition, an application will get deployed on the managed cluster with the label selector set to env=fv.</p> <pre><code>cat &gt; cluster_profile_flux.yaml &lt;&lt;EOF\n---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-helloworld-resources\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  policyRefs:\n  - kind: GitRepository\n    name: flux-system\n    namespace: flux-system\n    path: ./helloWorld/\n    targetNamespace: eng\nEOF\n</code></pre> <p>Whenever there is a change in the Git repository, Sveltos will leverage the Kustomize SDK to retrieve a list of resources to deploy to any cluster matching the label selector <code>env=fv</code> in the <code>eng</code> namespace.</p> <p>Note</p> <p>The GitRepository or Bucket content can also be a template. Sveltos will take the content of the files and instantiate them by the use of the data resources in the management cluster. For the templates deployment, we will have to ensure the <code>GitRepository</code> Kubernetes resource includes the <code>projectsveltos.io/template: ok</code> annotation.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_gitops/#more-resources","title":"More Resources","text":"<p>For more information about the Sveltos and Flux integration, have a look here.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_hybrid_multicloud/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease | Hybrid Multicloud","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_hybrid_multicloud/#what-is-sveltos","title":"What is Sveltos?","text":"<p>Sveltos is a set of Kubernetes controllers that run in a management cluster and manage add-ons and applications on a fleet of clusters. Sveltos automatically supports the discovery of ClusterAPI-powered clusters, but it does not stop there.</p> <p>Registration of any other cluster (on-prem, Cloud) is possible and seamless.</p> <p></p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_hybrid_multicloud/#platform-administrators-hybrid-multicloud-environments","title":"Platform Administrators - Hybrid Multicloud Environments","text":"<p>In today's fast-paced and ever-evolving IT landscape, where hybrid and multicloud concepts are becoming the norm, automating the creation of Kubernetes clusters while managing their lifecycle programmatically is essential for Kubernetes platform administrators.</p> <p>Sveltos offers a simple, unified interface to easily review existing configurations and perform necessary adjustments seamlessly. It helps break down organisation silos by allowing different administrators to collaborate on defining the required cluster types (test, staging, production). Each administrator can independently configure policies for their designated cluster type, ensuring global compliance and consistency across environments. This leads to optimised operations and supports overall organisational success!</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_hybrid_multicloud/#benefits-of-a-central-management-cluster","title":"Benefits of a Central Management Cluster","text":"<ul> <li> <p>Centralized Management: Manage multiple clusters from one place, ensuring consistency and reducing configuration errors.</p> </li> <li> <p>Consistency: Automate processes for uniform configurations and deployments, boosting reliability.</p> </li> <li> <p>Scalability: Easily scale infrastructure by simplifying the creation and management of clusters.</p> </li> <li> <p>Cost Optimisation: Centralized control maximizes resource use and lowers operational costs.</p> </li> <li> <p>Better Security: Security-related add-ons, such as network policies and secrets management, for secure cluster deployment.</p> </li> <li> <p>Increased Automation: Integrate with CI/CD pipelines to automate deployments and reduce management effort.</p> </li> </ul>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_hybrid_multicloud/#sveltos-add-on-management-solution","title":"Sveltos add-on Management Solution","text":"<p>Sveltos allows platform administrators to utilise the <code>ClusterProfile</code> Custom Resource Definition to perform Kubernetes add-on deployments. Within a Sveltos <code>ClusterProfile</code>, we define the below points.</p> <ol> <li>What Kubernetes add-ons to get deployed (Helm charts, Kustomize, YAML/JSON manifests)?</li> <li>Where should they get deployed?</li> <li>List the add-ons deployed</li> </ol> <p>Example - ClusterProfile</p> <pre><code>---\napiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-kyverno\nspec:\n  clusterSelector:\n    matchLabels:\n      env: prod\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    https://kyverno.github.io/kyverno/\n    repositoryName:   kyverno\n    chartName:        kyverno/kyverno\n    chartVersion:     v3.3.3\n    releaseName:      kyverno-latest\n    releaseNamespace: kyverno\n    helmChartAction:  Install\n  policyRefs:\n  - name: disallow-latest-tag # (1)\n    namespace: default\n    kind: ConfigMap\n</code></pre> <ol> <li>Reference a ConfigMap that contains a Kyverno ClusterPolicy</li> </ol>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_hybrid_multicloud/#more-resources","title":"More Resources","text":"<p>For more information about the Sveltos add-on deployment capabilities, have a look here.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_idp/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease | Build an IDP","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_idp/#what-is-an-internal-developer-platform","title":"What is an Internal Developer Platform?","text":"<p>There are many definitions around what an Internal Developer Platform or as commonly known IDP is. An easy-to-understand definition from Atlassian is the following:</p> <p>An internal developer platform (IDP) is a self-service interface between developers and the underlying infrastructure, tools, and processes required to build, deploy, and manage software applications.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_idp/#sveltos-event-framework-and-idp","title":"Sveltos Event Framework and IDP","text":"<p>On top of managing add-ons and applications, Sveltos can be used as a framework to build an Internal Developer Platform. Sveltos Event Framework allows platform teams to define events, and respond to events by deploying resources.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_idp/#how-it-works","title":"How it works?","text":"<p>Imagine an event like the creation of a new service or a namespace in a specified cluster. Sveltos can automatically respond to such <code>events</code> by deploying new resources either within the same cluster or across different clusters.</p> <p>This cross-cluster feature makes it easy to automate tasks across different clusters. For example, when a namespace with a specific label is created in a cluster, Sveltos can automatically set up a new database in a dedicated cluster reserved for databases.</p> <p>For easy to follow deployment examples, check out the Event Framework Section.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_idp/#sveltos-and-nats-integration","title":"Sveltos and NATS Integration","text":"<p>NATS is a lightweight, high-performance messaging system optimized for speed and scalability. It excels at publish/subscribe communication. JetStream enhances NATS with robust streaming and data management features, including message persistence, flow control, and ordered delivery, creating a powerful platform for modern distributed systems.</p> <p>With this integration in place, Sveltos can connect to a NATS server and listen for CloudEvents published on NATS subjects. That means Sveltos is not limited to events related to Kubernetes resources!</p> <ul> <li>Check out: Explore Sveltos and NATS Integration Example</li> </ul>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_idp/#sveltos-in-action-real-world-scenarios","title":"Sveltos in Action: Real-World Scenarios","text":"<p>\ud83d\udc49 How to Create Event-Driven Resources in Kubernetes by Colin Lacy</p> <p>\ud83d\udc49 Automate vCluster Management in EKS with Sveltos and Helm by Colin Lacy</p> <p>\ud83d\udc49 Building Your Own Event-Driven Internal Developer Platform with GitOps and Sveltos by Artem Lajko</p> <p>\ud83d\udc49 Click-to-Cluster: GitOps EKS Provisioning by Gianluca Mardente</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_multi-tenancy/","title":"Sveltos - Kubernetes Add-on Controller | Manage Kubernetes Add-ons with Ease | Multi-tenancy","text":"","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_multi-tenancy/#what-is-multi-tenancy","title":"What is Multi-tenancy?","text":"<p>Multi-tenancy in cloud computing is the concept of multiple clients sharing the same computing resources. Multi-tenancy in Kubernetes can appear in two forms, either share a cluster between multiple tenants within an organisation or have more than one clusters reserved by an organisation.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_multi-tenancy/#common-challenges","title":"Common Challenges","text":"<ol> <li>How can platform admins grant permissions to tenant admins programmatically?</li> <li>How to control tenant admins' deployment actions based on their permissions?</li> </ol> <p>We mentioned two terms in the challenges above: <code>platform admins</code> and <code>tenant admins</code>.</p> <ul> <li> <p>Platform admin: Manages the infrastructure of Kubernetes clusters, including tasks like creating clusters, managing nodes, and ensuring cluster health.</p> </li> <li> <p>Tenant admin: Has admin access to clusters or namespaces where applications run, with permissions assigned by the platform admin.</p> </li> </ul>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_multi-tenancy/#sveltos-multi-tenancy-full-isolation","title":"Sveltos Multi-tenancy: Full Isolation","text":"<p>In a multi-tenant setup, each tenant is assigned a dedicated namespace within the management cluster. Tenant admins can create and manage clusters in their namespace, using <code>Profile</code> instances to define the add-ons and apps to deploy. Like <code>ClusterProfiles</code>, <code>Profiles</code> use a cluster selector and a list of add-ons and apps, but they operate within a specific namespace, matching only clusters created in that namespace.</p> <p></p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_multi-tenancy/#sveltos-multi-tenancy-cluster-sharing-between-tenants","title":"Sveltos Multi-tenancy: Cluster Sharing Between Tenants","text":"<p>Sveltos allows platform admins to utilise the Custom Resource Definition <code>RoleRequest</code> that will grant permissions to a number of tenant admins. More information can be found here.</p> <p>Example - RoleRequest</p> <pre><code>---\napiVersion: lib.projectsveltos.io/v1beta1\nkind: RoleRequest\nmetadata:\n  name: full-access\nspec:\n  serviceAccountName: \"eng\"\n  serviceAccountNamespace: \"default\"\n  clusterSelector:\n    matchLabels:\n      env: prod\n  roleRefs:\n  - name: full-access\n    namespace: default\n    kind: ConfigMap\n</code></pre> <p>Based on the YAML definition above, the following fields are defined:</p> <ul> <li><code>serviceAccountName</code>: The service account to which the permission will be applied.</li> <li><code>serviceAccountNamespace</code>: The namespace where the service account is deployed in the management cluster.</li> <li><code>clusterSelector</code>: A Kubernetes <code>label selector</code> used by Sveltos to identify clusters where permissions should be granted.</li> <li><code>roleRefs</code>: References to ConfigMaps/Secrets, each containing Kubernetes ClusterRoles or Roles that define the permissions to be granted.</li> </ul> <p>The <code>configMap</code> in the example above might look like the YAML definition below.</p> <p>Example - ConfigMap</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: full-access\n  namespace: default\ndata:\n  role.yaml: |\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRole\n    metadata:\n      name: eng-full-access\n    rules:\n    - apiGroups: [\"*\"]\n      resources: [\"*\"]\n      verbs: [\"*\"]\n</code></pre> <p>Based on the YAML definitions above, here is what happens from a Sveltos perspective:</p> <p>By referencing the ConfigMap <code>default/full-access</code>, the <code>RoleRequest</code> named <code>full-access</code> will reserve a cluster matching the <code>clusterSelector</code> env=prod to the service account <code>eng</code>.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]},{"location":"use_cases/use_case_multi-tenancy/#more-resources","title":"More Resources","text":"<p>For more information about the Sveltos multi-tenancy capabilities, have a look here.</p>","tags":["Kubernetes","add-ons","helm","kustomize","clusterapi","multi-tenancy","Sveltos"]}]}